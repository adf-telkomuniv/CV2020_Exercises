{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CV2020 - 06 - Augmentation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"f_ChrQTZyAZU"},"source":["![title](https://i.ibb.co/f2W87Fg/logo2020.png)\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"oUkeuLWzoHoN"},"source":["<table  class=\"tfo-notebook-buttons\" align=\"left\"><tr><td>\n","    \n","<a href=\"https://colab.research.google.com/github/adf-telkomuniv/CV2020_Exercises/blob/master/CV2020 - 06 - Augmentation.ipynb\" source=\"blank\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n","</td><td>\n","<a href=\"https://github.com/adf-telkomuniv/CV2020_Exercises/blob/master/CV2020 - 06 - Augmentation.ipynb\" source=\"blank\" ><img src=\"https://i.ibb.co/6NxqGSF/pinpng-com-github-logo-png-small.png\"></a>\n","    \n","</td></tr></table>"]},{"cell_type":"markdown","metadata":{"id":"sjlcHwermNUS"},"source":["\n","# Task 6 - Data Augmentation\n","\n","In the previous exercise, you have successfully built a classification model using the Tensorflow library.\n","This time we will learn how to use the tensorflow library to augment data and help improve the training performance.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QGarRj9tWulZ"},"source":["---\n","## GPU Runtime\n","Since we're going to use TensorFlow, we can utilize the GPU to accelerate the process\n","\n","For that, make sure that this Colaboratory file is set to use GPU\n","\n","* select **Runtime** in taskbar\n","* select **Change Runtime Type**\n","* choose Hardware accelerator **GPU**\n","\n","<center>\n","  \n","![gpu](https://i.ibb.co/QX3Brf0/gpu.png)\n"]},{"cell_type":"markdown","metadata":{"id":"6VAE_sE-Bgk1"},"source":["---\n","---\n","#[Part 0] Import Libraries and Load Data"]},{"cell_type":"markdown","metadata":{"id":"M8molz1J9Dl-"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"CkzQo8MY9DmB"},"source":["## --- start your code here ----\n","\n","NIM = ??\n","Nama = ??\n","\n","## --- end your code here ----"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"12SMnaunBw96"},"source":["---\n","## 1 - Import Libraries\n","\n","Import required libraries"]},{"cell_type":"code","metadata":{"id":"GrnacNg8In1K"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","np.set_printoptions(precision=7)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTMPdtW1mNVv"},"source":["---\n","## 2 - Load CIFAR-10"]},{"cell_type":"code","metadata":{"id":"e9lcWKk9In1U"},"source":["import tensorflow as tf\n","\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","print('X_train.shape =',X_train.shape)\n","print('y_train.shape =',y_train.shape)\n","print('X_test.shape  =',X_test.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9rhC32AWIn1Z"},"source":["like always, let's visualize it first"]},{"cell_type":"code","metadata":{"id":"Zk4QVV87In1b"},"source":["fig, ax = plt.subplots(2,10,figsize=(15,4.5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for j in range(0,2):\n","    for i in range(0, 10):\n","        ax[j,i].imshow(X_train[i+j*10])\n","        ax[j,i].set_title(classes[y_train[i+j*10,0]])\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c57WBdtqmNVz"},"source":["---\n","## 3 - Split Validation Data"]},{"cell_type":"code","metadata":{"id":"oCSjAqPzIn1i"},"source":["X_val = X_train[-10000:,:]\n","y_val = y_train[-10000:]\n","\n","X_train = X_train[:-10000, :]\n","y_train = y_train[:-10000]\n","\n","print('X_val.shape   =',X_val.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('X_train.shape =',X_train.shape)\n","print('y_train.shape =',y_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJZisKPImNV4"},"source":["---\n","## 4 - Normalize and Reshape Data"]},{"cell_type":"code","metadata":{"id":"yfEl2gZJIn1p"},"source":["\n","mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n","std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n","\n","X_train = (X_train - mean_pixel) / std_pixel\n","X_val   = (X_val - mean_pixel) / std_pixel\n","X_test  = (X_test - mean_pixel) / std_pixel\n","\n","X_train = X_train.astype('float32')\n","X_val   = X_val.astype('float32')\n","X_test  = X_test.astype('float32')\n","\n","print('X_train.shape =',X_train.shape)\n","print('X_val.shape   =',X_val.shape)\n","print('X_test.shape  =',X_test.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-dDw-6VS4KIM"},"source":["Somehow the Augmentation API is performing better when the label is in one-hot categorical. So let's just do that."]},{"cell_type":"code","metadata":{"id":"neQU8KqDIn1_"},"source":["from tensorflow.keras.utils import to_categorical\n","\n","y_train_hot = to_categorical(y_train.ravel(), 10)\n","y_val_hot = to_categorical(y_val.ravel(), 10)\n","y_test_hot = to_categorical(y_test.ravel(), 10)\n","\n","print('y_train_hot.shape =',y_train_hot.shape)\n","print('y_val_hot.shape   =',y_val_hot.shape)\n","print('y_test_hot.shape  =',y_test_hot.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MZwtehYZxrhn"},"source":["---\n","## 5 - Helper Functions\n"]},{"cell_type":"markdown","metadata":{"id":"77k034bYk4oE"},"source":["### a. Plot History \n","Below is a function to plot the training and validation accuracy, as well the training and validation loss during the training process\n"]},{"cell_type":"code","metadata":{"id":"4BjwCJmntAEg"},"source":["def plot_history(history):\n","    plt.rcParams['figure.figsize'] = [14, 3.5]\n","    plt.subplots_adjust(wspace=0.2)\n","\n","    plt.subplot(121)\n","    # Plot training & validation accuracy values\n","    plt.plot(history.history['accuracy'])\n","    plt.plot(history.history['val_accuracy'])\n","    plt.title('Model accuracy')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Test'])\n","\n","    plt.subplot(122)\n","    # Plot training & validation loss values\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Test'])\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jJa3vIazk8mD"},"source":["### b. Plot Augmentation Result\n","Below is a function to show augmentation results from a data generator\n"]},{"cell_type":"code","metadata":{"id":"GcHVd3iJIkQG"},"source":["def plot_augmentation(data_generator, nrows=1, ncols=4, figsize=(20,6)):\n","\n","    data_generator.fit(images)\n","    image_iterator = data_generator.flow(images)\n","    \n","    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n","    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","    if len(ax.shape) == 1:\n","        ax = np.expand_dims(ax,0)\n","    for i in range(nrows):\n","        for j in range(ncols):\n","            image = image_iterator.next()[0].astype('int')\n","            ax[i,j].imshow(image)\n","            ax[i,j].axis('off')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ld_C6JC4olXG"},"source":["---\n","---\n","\n","# [Part 1] Three-Layer ConvNet\n","In this exercise, to easily compare the result between model trained with and without Augmentation, let's build a fairly simple network"]},{"cell_type":"code","metadata":{"id":"mYdhz2T2olXS"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import MaxPool2D\n","\n","num_data    = X_train.shape[0]\n","X_dim       = X_train.shape[1]\n","num_classes = y_train_hot.shape[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8EAjJVOV62wH"},"source":["---\n","## 1 - Define Model\n","\n","Define a five-layer convolutional network with the following architecture:\n","\n","<pre>\n","1. <font color='red'>16 @3x3 Conv2D</font>, activation=relu, padding=same, \n","   input_shape=(32,32,3),\n","2. <font color='red'>32 @3x3 Conv2D</font>, activation=relu, padding=same,\n","3. <font color='blue'>MaxPool2D</font>,\n","4. <font color='red'>64 @3x3 Conv2D</font>, activation=relu, padding=same,\n","5. <font color='blue'>MaxPool2D</font>,\n","6. Flatten,\n","7. <font color='red'>Dense 128</font>, activation=relu, \n","8. <font color='red'>Dense num_classes</font>, activation=softmax, \n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"UxN9swef0e27"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","complete the function\n"]},{"cell_type":"code","metadata":{"id":"n7uDuN9BolXc"},"source":["# create model compact sequential\n","def create_model(model_name = 'myModel'):\n","    model = Sequential([\n","        ??\n","    ], name=model_name)\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d9xtgVaMUrfK"},"source":["Show the model summary"]},{"cell_type":"code","metadata":{"id":"8e9ozXZDLeMi"},"source":["model = create_model()\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L7A5vwNGLeM1"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_? (Conv2D)            (None, 32, 32, 16)        448       \n","_________________________________________________________________\n","conv2d_? (Conv2D)            (None, 32, 32, 32)        4640      \n","_________________________________________________________________\n","max_pooling2d_? (MaxPooling2 (None, 16, 16, 32)        0         \n","_________________________________________________________________\n","conv2d_? (Conv2D)            (None, 16, 16, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_? (MaxPooling2 (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 4096)              0         \n","_________________________________________________________________\n","dense_? (Dense)              (None, 128)               524416    \n","_________________________________________________________________\n","dense_? (Dense)              (None, 10)                1290      \n","=================================================================\n","Total params: 549,290\n","Trainable params: 549,290\n","Non-trainable params: 0\n","_________________________________________________________________"]},{"cell_type":"markdown","metadata":{"id":"rigkXhbUqhKg"},"source":["let's also plot the model design"]},{"cell_type":"code","metadata":{"id":"VF4cEoDbHDqC"},"source":["from tensorflow.keras.utils import plot_model\n","\n","plot_model(model, \n","           to_file=model.name+'.png', \n","           show_shapes=True, \n","           show_layer_names=False,\n","           rankdir='LR',\n","           dpi=70\n","          )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aegx3nGCNjTq"},"source":["**EXPECTED OUTPUT**:\n","\n","<img src=\"https://i.ibb.co/7RLjxJ8/5layerconv.png\">"]},{"cell_type":"markdown","metadata":{"id":"DNJmngIeolXm"},"source":["---\n","---\n","# [Part 2] Training without Augmentation"]},{"cell_type":"markdown","metadata":{"id":"ATlROUvI76rq"},"source":["---\n","## 1 - Build Model"]},{"cell_type":"markdown","metadata":{"id":"h4rGAHDqNtH6"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","Build the model by calling `create_model()` function. \n"]},{"cell_type":"code","metadata":{"id":"0bJ-IuhDyhbQ"},"source":["model_no_aug = ??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ua6VR12iyP8j"},"source":["---\n","## 2 - Compile Model"]},{"cell_type":"markdown","metadata":{"id":"jY1YqgH8yP8m"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","compile your model using `'categorical_crossentropy'` loss, `'adam'` optimizer, and `['accuracy']` metrics\n"]},{"cell_type":"code","metadata":{"id":"j7-hIlWEolXo"},"source":["model_no_aug.??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1vlfw39Ry0kz"},"source":["---\n","## 3 - Train Model"]},{"cell_type":"markdown","metadata":{"id":"wFLbeuBTy0k1"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","Train the model for `20` epochs with batch size of `256`.<br>\n","Don't forget to put the validation data.<br>\n","Use the one-hot label.\n"]},{"cell_type":"code","metadata":{"id":"IRi2PNBsyzkZ"},"source":["num_epochs = ??\n","batch_size = ??\n","\n","history_no_aug = ??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o_lvvWE7L-ej"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>The loss should start from ~1.5 and end around 0.05 with training accuracy around 98%, \n","but with validation accuracy stuck at 70%"]},{"cell_type":"markdown","metadata":{"id":"0BUZEHLMolXr"},"source":["---\n","## 4 - Evaluate Model\n"]},{"cell_type":"markdown","metadata":{"id":"jB7mbSquzWEZ"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","Evaluate model using data test.<br>\n","Use the one-hot label.\n"]},{"cell_type":"code","metadata":{"id":"lAmucgajolXt"},"source":["scores_no_aug = ??\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores_no_aug[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coc1tJzANl6e"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","you should get around 70% of testing accuracy"]},{"cell_type":"markdown","metadata":{"id":"oMMVmvCu76r3"},"source":["---\n","## 5 - Visualize Training History\n","\n","Visualize the train-validation accuracy"]},{"cell_type":"code","metadata":{"id":"X94mNmsa76r4"},"source":["plot_history(history_no_aug)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mMDIXfzDsbAa"},"source":["The Training Accuracy is close to 100%, and the validation accuracy is in the 70% range.\n","\n","This is a great example of overfitting -- which in short means that it can do very well with images it has seen before, but not so well with images it hasn't. \n","\n","At this point, we cannot continue training to improve the performance."]},{"cell_type":"markdown","metadata":{"id":"VjJE25yOQD8e"},"source":["---\n","# [Part 3] Image Augmentation\n","\n","Let's see if we can do better to avoid overfitting -- and one simple method is to augment the images a bit. If you think about it, most pictures of a cat are very similar -- the ears are at the top, then the eyes, then the mouth etc. Things like the distance between the eyes and ears will always be quite similar too. \n","\n","What if we tweak with the images to change this up a bit -- rotate the image, squash it, etc.  That's what image augementation is all about. And there's an API that makes it easy...\n","\n","Now take a look at the ImageGenerator. There are properties on it that you can use to augment the image"]},{"cell_type":"code","metadata":{"id":"Zyh2PhCdBe9i"},"source":["!wget -q -O 'image.jpg' 'https://www.humanesociety.org/sites/default/files/styles/1240x698/public/2018/08/kitten-440379.jpg'\n","\n","image = plt.imread(\"image.jpg\")\n","images = np.expand_dims(image, 0)\n","\n","plt.imshow(images[0])\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xNpUPpHYkEcr"},"source":["These are just a few of the options available (for more, see the Keras documentation. Let's quickly go over what we just wrote:\n","\n","* `rotation_range` is a value in degrees (0–180), a range within which to randomly rotate pictures.\n","* `width_shift` and `height_shift` are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\n","* `shear_range` is for randomly applying shearing transformations.\n","* `zoom_range` is for randomly zooming inside pictures.\n","* `horizontal_flip` is for randomly flipping half of the images horizontally. This is relevant when there are no assumptions of horizontal assymmetry (e.g. real-world pictures).\n","* `fill_mode` is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\n","\n","\n","Here's some code where we've added Image Augmentation. Run it to see the impact."]},{"cell_type":"code","metadata":{"id":"rLT6p0YFCeJ9"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hChKKndoCoXR"},"source":["---\n","## 1 - Rotation\n","By specifying the `rotation_range` , the data generated is randomly rotated by an angle in the range of `+rotation_range` to `-rotation_range` (in degrees)."]},{"cell_type":"code","metadata":{"id":"OhKsGWXuCW5s"},"source":["data_generator = ImageDataGenerator(rotation_range=90)\n","plot_augmentation(data_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xJMRkx4MCx7s"},"source":["---\n","## 2 - Width Shifting\n","The `width_shift_range` is a floating point number between 0.0 and 1.0 which specifies the upper bound of the fraction of the total width by which the image is to be randomly shifted, either towards the left or right."]},{"cell_type":"code","metadata":{"id":"dZUfB8qKCW3L"},"source":["data_generator = ImageDataGenerator(width_shift_range=0.3)\n","plot_augmentation(data_generator)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"murMcnPWC33B"},"source":["---\n","## 3 - Height  Shifting\n","Exactly like width shifting, except that the image is shifted vertically instead of horizontally."]},{"cell_type":"code","metadata":{"id":"IsUFXvwoCW0V"},"source":["data_generator = ImageDataGenerator(height_shift_range=0.3)\n","plot_augmentation(data_generator)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1xJZEcChC9a-"},"source":["---\n","## 4 - Brightness\n","The `brightness_range` specifies the range for randomly picking a brightness shift value from. A brightness of 0.0 corresponds to absolutely no brightness, and 1.0 corresponds to maximum brightness."]},{"cell_type":"code","metadata":{"id":"1WpkRR1BCWxR"},"source":["data_generator = ImageDataGenerator(brightness_range=(0.1, 0.9))\n","plot_augmentation(data_generator)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yIulRFkvDFrg"},"source":["---\n","## 5 - Shear Intensity\n","Shear transformation slants the shape of the image. This is different from rotation in the sense that in shear transformation, we fix one axis and stretch the image at a certain angle known as the shear angle. This creates a sort of ‘stretch’ in the image, which is not seen in rotation. `shear_range` specifies the angle of the slant in degrees."]},{"cell_type":"code","metadata":{"id":"vJbxKmvQDFrj"},"source":["data_generator = ImageDataGenerator(shear_range=45.0)\n","plot_augmentation(data_generator)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cMQ5eFBSDN3I"},"source":["---\n","## 6 - Zoom\n","A random zoom is obtained by the `zoom_range` argument. A zoom less than 1.0 magnifies the image, while a zoom greater than 1.0 zooms out of the image."]},{"cell_type":"code","metadata":{"id":"7x9NLgcoDN3M"},"source":["data_generator = ImageDataGenerator(zoom_range=[0.5, 1.5])\n","plot_augmentation(data_generator)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjStZX1sDXUd"},"source":["---\n","## 7 - Channel Shift\n","Channel shift randomly shifts the channel values by a random value chosen from the range specified by channel_shift_range."]},{"cell_type":"code","metadata":{"id":"0r7wMpZhDXUg"},"source":["data_generator = ImageDataGenerator(channel_shift_range=150.0)\n","plot_augmentation(data_generator)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KHNCXWKsDiPM"},"source":["---\n","## 8 - Horizontal Flip\n","The generator will generate images, which on a random basis, will be horizontally flipped."]},{"cell_type":"code","metadata":{"id":"mYuS4w8YDiPQ"},"source":["data_generator = ImageDataGenerator(horizontal_flip=True)\n","plot_augmentation(data_generator)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MBR7MAPhDqwu"},"source":["---\n","## 9 - Vertical Flip\n","nstead of flipping horizontally, we can also apply a vertical flip."]},{"cell_type":"code","metadata":{"id":"SHs4iq8kDquK"},"source":["data_generator = ImageDataGenerator(vertical_flip=True)\n","plot_augmentation(data_generator)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCqAbWS9EJRJ"},"source":["---\n","## 10 - Nearest Pixel Fill\n","Notice that when we augment the image, TensorFlow fill the empty spaces with the last neighboring pixels from the image. This is called as nearest fill mode. This is the default option where the closest pixel value is chosen and repeated for all the empty values"]},{"cell_type":"code","metadata":{"id":"NGcUm_3aEJRN"},"source":["data_generator = ImageDataGenerator(rotation_range=180, fill_mode='nearest')\n","plot_augmentation(data_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yZzIQgFVE5dd"},"source":["---\n","## 11 - Replect Pixel Fill\n","Another mode is the reflex fill mode. This mode creates a ‘reflection’ and fills the empty values in a reverse order of the known values."]},{"cell_type":"code","metadata":{"id":"4inAPWQoE5dg"},"source":["data_generator = ImageDataGenerator(rotation_range=180, fill_mode='reflect')\n","plot_augmentation(data_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0IhtUbDIFPNh"},"source":["---\n","## 12 - Wrap Pixel Fill\n","The third one is called Wrap. Instead of a reflect effect, we can also create a ‘wrap’ effect by copying the values of the known points into the unknown points, keeping the order unchanged."]},{"cell_type":"code","metadata":{"id":"6MUh9lkSFPNk"},"source":["data_generator = ImageDataGenerator(rotation_range=180, fill_mode='wrap')\n","plot_augmentation(data_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F0Tar0H4Fj53"},"source":["---\n","## 13 - Constant Pixel Fill\n","The last one is if we want to fill all the points lying outside the boundaries of the input by a constant value, this mode helps us achieve exactly that. The constant value is specified by the `cval` argument."]},{"cell_type":"code","metadata":{"id":"-UO7JwgZFj58"},"source":["data_generator = ImageDataGenerator(rotation_range=180, fill_mode='constant', cval=190)\n","plot_augmentation(data_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YE4f-_WFGQBJ"},"source":["---\n","## 14 - Combine it all\n","Now, let's try and see how all the augmentation options combine"]},{"cell_type":"code","metadata":{"id":"2vq4Fh73GQBN"},"source":["data_generator = ImageDataGenerator(\n","    rotation_range=90,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    brightness_range=(0.1, 0.9),\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    channel_shift_range=150.0,\n","    fill_mode='nearest',\n","    horizontal_flip=True,\n","    vertical_flip=True\n","    )\n","\n","plot_augmentation(data_generator, nrows=3, ncols=4, figsize=(20,8))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RYLHiJc2Y_LJ"},"source":["---\n","---\n","# [Part 4] Training with Augmentation\n","\n","After you master the Augmentation options, now let's train a new model, with the same architecture, but using Augmentation process"]},{"cell_type":"markdown","metadata":{"id":"wN20QjECZKtK"},"source":["---\n","## 1 - Build Model"]},{"cell_type":"markdown","metadata":{"id":"eb52XBMPtJWF"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","Build a new model by calling `create_model()` function. \n"]},{"cell_type":"code","metadata":{"id":"0daZgiQHZKtS"},"source":["model_aug = ??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jl-mmeEqZKth"},"source":["---\n","## 2 - Compile Model"]},{"cell_type":"markdown","metadata":{"id":"wtA6wW_ttOHg"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","compile your model using `'categorical_crossentropy'` loss, `'adam'` optimizer, and `['accuracy']` metrics\n"]},{"cell_type":"code","metadata":{"id":"IccOY5wqZKtj"},"source":["model_aug.??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uc8HwsvKbWNx"},"source":["---\n","## 3 - Train Data Generator\n","Now let's define our Data Generator to generate batches of tensor image data with real-time data augmentation. \n","\n","For starter, let's not get too wild on the Augmentation, and only use 3 options: width shift, height shift, and horizontal flip"]},{"cell_type":"markdown","metadata":{"id":"S4_cDVkutSF9"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","create Image Data Generator with:\n","* width_shift_range=0.1,\n","* height_shift_range=0.1,\n","* horizontal_flip=True,\n"]},{"cell_type":"code","metadata":{"id":"D05R_2nBP57z"},"source":["train_datagen = ImageDataGenerator(\n","    ??\n","    )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wZLNhM16biPd"},"source":["Now call `.fit()` function to Fits the data generator to some sample data."]},{"cell_type":"code","metadata":{"id":"QREERhakbiut"},"source":["train_datagen.fit(X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kZzh8tdRxwNA"},"source":["To use the generator, we can use `.flow()` function to continuously read a batch of data, augment it in real time, and feed it into training loop.\n","\n","You can call the `.flow()` function directly inside the training `.fit()` function, but it's also cleaner if we define as a named function outside like this.\n","\n"]},{"cell_type":"code","metadata":{"id":"GoWUUbGgRmzu"},"source":["batch_size = 256\n","\n","train_generator = train_datagen.flow(\n","        X_train, y_train_hot, batch_size=batch_size\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v3RIUusVZKtt"},"source":["---\n","## 4 - Train Model"]},{"cell_type":"markdown","metadata":{"id":"WFx5hrEazA3S"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","Train the model for `20` epochs.<br>\n","Put `train_generator` in place for training data.<br>\n","You don't need to add batch number to the function.<br>\n","Don't forget to put the validation data.<br>\n","Use the one-hot label.\n"]},{"cell_type":"code","metadata":{"id":"3z4JtJSQZKtu"},"source":["num_epochs = 20\n","\n","history_aug = model_aug.fit(??, \n","                            validation_data=??,\n","                            epochs=??)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"opW9BmjQZKt0"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>The loss should start from ~1.5 and end around 0.6 \n","the training accuracy only reach 78%, but the validation accuracy should stay close to it"]},{"cell_type":"markdown","metadata":{"id":"8-O8jTJjZKt0"},"source":["---\n","## 5 - Evaluate Model\n"]},{"cell_type":"markdown","metadata":{"id":"KKAa_6X0ZKt2"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","Evaluate model using data test. <br>\n","Use the one-hot label.\n"]},{"cell_type":"code","metadata":{"id":"VU7YCQ8ZZKt2"},"source":["scores_aug = ??\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores_aug[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-nCMx9umZKt6"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","you should get around 75% of testing accuracy"]},{"cell_type":"markdown","metadata":{"id":"_P9pNfmQb964"},"source":["---\n","## 6 - Visualize Training History\n","\n","Visualize the train-validation accuracy"]},{"cell_type":"code","metadata":{"id":"Iwf1UJLSb97F"},"source":["plot_history(history_aug)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"23j8gJzt1dJG"},"source":["You should see that the training and validation accuracy, as well the loss is still close. Indicates that we can continue the training a bit longet."]},{"cell_type":"markdown","metadata":{"id":"NnkdHf-gsn3R"},"source":["---\n","---\n","# [Part 5] Comparison\n","\n","Now let's compare the two networks in terms of accuracy and loss"]},{"cell_type":"markdown","metadata":{"id":"AAdNRfYCuJVB"},"source":["---\n","## 1 - Training Loss and Accuracy"]},{"cell_type":"code","metadata":{"id":"j6M2Nzr3uJ1S"},"source":["fig, ax = plt.subplots(1,3,figsize=(18,3))\n","\n","ax[0].plot(history_no_aug.history['loss'])\n","ax[0].plot(history_aug.history['loss'])\n","ax[0].set_title('Train Loss')\n","ax[0].set_ylabel('Loss')\n","ax[0].set_xlabel('Epoch')\n","ax[0].legend(['w/o Aug', 'w/ Aug'], loc='upper right')\n","\n","ax[1].plot(history_no_aug.history['accuracy'])\n","ax[1].plot(history_aug.history['accuracy'])\n","ax[1].set_title('Train Accuracy')\n","ax[1].set_ylabel('Accuracy')\n","ax[1].set_xlabel('Epoch')\n","ax[1].legend(['w/o Aug', 'w/ Aug'], loc='lower right')\n","\n","ax[2].plot(history_no_aug.history['val_accuracy'])\n","ax[2].plot(history_aug.history['val_accuracy'])\n","ax[2].set_title('Validation Accuracy')\n","ax[2].set_ylabel('Accuracy')\n","ax[2].set_xlabel('Epoch')\n","ax[2].legend(['w/o Aug', 'w/ Aug'], loc='lower right')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tf4ELAef2Hj5"},"source":["You should see that the model trained without Augmentation has higher accuracy (and lower loss) compared to the model trained with Augmentation. This happens because the Augmented model has to see much more data per epoch, which makes it harder to generalize. \n","\n","However, if we see the validation accuracy, the model trained with augmentation has higher accuracy."]},{"cell_type":"markdown","metadata":{"id":"NfOgnKWBzs1L"},"source":["---\n","## 2 - Overfitting Comparison\n","Now if we compare the model overfitting"]},{"cell_type":"code","metadata":{"id":"_B364ZEqzNw2"},"source":["fig, ax = plt.subplots(1,2,figsize=(15,4))\n","\n","ax[0].plot(history_no_aug.history['accuracy'])\n","ax[0].plot(history_no_aug.history['val_accuracy'])\n","ax[0].set_title('Train Accuracy without Augmentation')\n","ax[0].set_ylabel('Accuracy')\n","ax[0].set_xlabel('Epoch')\n","ax[0].set_ylim(bottom=.2)\n","ax[0].legend(['train', 'val'], loc='lower right')\n","\n","ax[1].plot(history_aug.history['accuracy'])\n","ax[1].plot(history_aug.history['val_accuracy'])\n","ax[1].set_title('Train Accuracy with Augmentation')\n","ax[1].set_ylabel('Accuracy')\n","ax[1].set_xlabel('Epoch')\n","ax[1].set_ylim(bottom=.2)\n","ax[1].legend(['train', 'val'], loc='lower right')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VI0QLlzQ291a"},"source":["You can see that augmentation may prevent the overfitting. \n","\n","If the gap of loss and accuracy between training and validation is still small, it means that we can train the network longer, or even increase the capacity (e.g. add more layers) to improve its performance"]},{"cell_type":"markdown","metadata":{"id":"UiN4exlAtfly"},"source":["---\n","## 3 - Testing Accuracy\n","\n","Lastly, we should see that the testing accuracy is also higher"]},{"cell_type":"code","metadata":{"id":"RAbAilYSt0pD"},"source":["print(\"Test Accuracy without Augmentation : %.2f%%\" % (scores_no_aug[1]*100))\n","print(\"Test Accuracy with Augmentation    : %.2f%%\" % (scores_aug[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5JECsUGsN94_"},"source":["---\n","---\n","# [Part 6] CIFAR-10 Open-ended Challenge\n","\n","Similar to the previous exercise, in this section you can experiment with whatever ConvNet architecture you'd like on CIFAR-10. You should experiment with **architectures**, **hyperparameters**, **loss functions**, **regularization**, or anything else you can think of to train a model \n","\n","but now, add <font color='blue' size='4' ><b>Augmentation</b></font> to the mix\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qbYFGiXWOPX4"},"source":["---\n","## Some things you can try:\n","- **Filter size**\n","- **Number of filters**\n","- **Pooling**\n","- **Normalization**\n","- **Network architecture**\n","- **Global average pooling**\n","- **Regularization**\n","- **Optimization**\n"]},{"cell_type":"markdown","metadata":{"id":"0eYZkoAQO-S2"},"source":["---\n","## Tips for training\n","For each network architecture that you try, you should tune the learning rate and other hyperparameters. \n","\n","When doing this there are a couple important things to keep in mind:\n","\n","- If the parameters are working well, you should see improvement within a few hundred iterations\n","\n","- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n","\n","- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n","\n","- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set."]},{"cell_type":"markdown","metadata":{"id":"CM0a9PvRPhqa"},"source":["<center>\n","<h2><font color='blue'>--- Go Wild, Have Fun, and Happy Training!  --- </font></h2>"]},{"cell_type":"markdown","metadata":{"id":"AW4Ees_zVLBJ"},"source":["---\n","## 1 - Define Model"]},{"cell_type":"markdown","metadata":{"id":"rq_8xfRAQIKO"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Design your Convolutional Neural Network Architecture\n","\n","    "]},{"cell_type":"code","metadata":{"id":"euoKYCFcQNOf"},"source":["from tensorflow.keras.layers import *\n","\n","myModel = ??\n","\n","\n","myModel.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jJ9XUD5iVP26"},"source":["---\n","## 2 - Augment Data"]},{"cell_type":"markdown","metadata":{"id":"OwRct_FtVP3A"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Define Data Augmentation generator\n","    "]},{"cell_type":"code","metadata":{"id":"ZcszYDKV76In"},"source":["train_datagen = ??\n","\n","train_datagen.??"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KBPnquMC74aj"},"source":["batch_size = ??\n","\n","train_generator = ??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sjF3pXWk7plX"},"source":["---\n","## 2 - Train Model"]},{"cell_type":"markdown","metadata":{"id":"lqQFvqVK7pla"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Compile the model\n","    Train the model\n","    "]},{"cell_type":"code","metadata":{"id":"lQCmkzyXSAeg"},"source":["# Compile model\n","myModel.compile(??)\n","\n","num_epochs = ??\n","\n","history = myModel.fit(??)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"czp7YYjSsGt0"},"source":["# Save model if needed\n","myModel.save(??)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y8bwtzimVUfs"},"source":["---\n","## 3 - Evaluate Model"]},{"cell_type":"markdown","metadata":{"id":"SOzUKWj5Q_La"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    evaluate your model on test set\n","    "]},{"cell_type":"code","metadata":{"id":"QZMNJP7hQ-G5"},"source":["myModel = load_model(??)\n","\n","train_scores = myModel.evaluate(X_train, y_train_hot, verbose=1)\n","val_scores   = myModel.evaluate(X_val, y_val_hot, verbose=1)\n","test_scores  = myModel.evaluate(X_test, y_test_hot, verbose=1)\n","\n","print(\"Training Accuracy  : %.2f%%\" % (train_scores[1]*100))\n","print(\"Validation Accuracy: %.2f%%\" % (val_scores[1]*100))\n","print(\"Testing Accuracy  :  %.2f%%\" % (test_scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Q2Lgw7gSwqD"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","try to get above 80% of accuracy for train, val, and test set"]},{"cell_type":"markdown","metadata":{"id":"V7SIMIWzYIq9"},"source":["---\n","## 4 - Test Model on New Image\n","\n","Test your model on new image"]},{"cell_type":"markdown","metadata":{"id":"fkTntuHaYIrI"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    define five image urls\n","    one image has been given for an example, you can change it"]},{"cell_type":"code","metadata":{"id":"bcYaoCy0YIrP"},"source":["!wget -q -O 'data_test_0.jpg' 'https://ichef.bbci.co.uk/news/912/cpsprodpb/160B4/production/_103229209_horsea.png'\n","!wget -q -O 'data_test_1.jpg' '??'\n","!wget -q -O 'data_test_2.jpg' '??'\n","!wget -q -O 'data_test_3.jpg' '??'\n","!wget -q -O 'data_test_4.jpg' '??'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DJrPxToeYIrb"},"source":["Run and Recognize the images"]},{"cell_type":"code","metadata":{"id":"0pYhcYJeYIrd"},"source":["import cv2 as cv\n","from PIL import Image\n","class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","\n","for i in range(5):\n","    # load and open the image\n","    new_img = Image.open('data_test_'+str(i)+'.jpg')\n","    new_img = np.array(new_img)\n","\n","    # resize into (32,32,3)\n","    new_img2 = cv.resize(new_img, (32,32), interpolation=cv.INTER_AREA)\n","    plt.imshow(new_img2)\n","    plt.axis('off')\n","    plt.show()\n","\n","    # preprocess the image using X_train mean and std\n","    new_img2 = (new_img2 - mean_pixel) / std_pixel\n","\n","    # predict the class\n","    pred = myModel.predict(new_img2)\n","    class_id = np.argmax(pred)\n","    print('predicted id   :',class_id)\n","    print('predicted class:', class_names[class_id])\n","    print('--------------------------------\\n\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aQqQ_gcO92ub"},"source":["\n","---\n","\n","# Congratulation, You've Completed Exercise 6\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2020 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"id":"7_Hkr9kg9Do-"},"source":["![footer](https://i.ibb.co/yX0jfMS/footer2020.png)"]}]}