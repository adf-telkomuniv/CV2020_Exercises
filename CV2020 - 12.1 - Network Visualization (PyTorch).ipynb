{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"CV2020 - 12.1 - Network Visualization (PyTorch).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"f_ChrQTZyAZU"},"source":["![title](https://i.ibb.co/f2W87Fg/logo2020.png)\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"MQjL2XtX7uKJ"},"source":["<table  class=\"tfo-notebook-buttons\" align=\"left\"><tr><td>\n","    \n","<a href=\"https://colab.research.google.com/github/adf-telkomuniv/CV2020_Exercises/blob/main/CV2020 - 12.1 - Network Visualization (PyTorch).ipynb\" source=\"blank\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n","</td><td>\n","<a href=\"https://github.com/adf-telkomuniv/CV2020_Exercises/blob/main/CV2020 - 12.1 - Network Visualization (PyTorch).ipynb\" source=\"blank\" ><img src=\"https://i.ibb.co/6NxqGSF/pinpng-com-github-logo-png-small.png\"></a>\n","    \n","</td></tr></table>"]},{"cell_type":"markdown","metadata":{"id":"1JBASSKXL_y3"},"source":["\n","# Task 12 part 1 - Network Visualization (PyTorch)\n","\n","In this notebook we will explore the use of *image gradients* for generating new images.\n","\n","When training a model, we define a loss function which measures our current unhappiness with the model's performance; we then use backpropagation to compute the gradient of the loss with respect to the model parameters, and perform gradient descent on the model parameters to minimize the loss.\n","\n","Here we will do something slightly different. We will start from a convolutional neural network model which has been pretrained to perform image classification on the ImageNet dataset. We will use this model to define a loss function which quantifies our current unhappiness with our image, then use backpropagation to compute the gradient of this loss with respect to the pixels of the image. We will then keep the model fixed, and perform gradient descent *on the image* to synthesize a new image which minimizes the loss.\n","\n","In this notebook we will explore three techniques for image generation:\n","\n","1. **Saliency Maps**: Saliency maps are a quick way to tell which part of the image influenced the classification decision made by the network.\n","2. **Fooling Images**: We can perturb an input image so that it appears the same to humans, but will be misclassified by the pretrained network.\n","3. **Class Visualization**: We can synthesize an image to maximize the classification score of a particular class; this can give us some sense of what the network is looking for when it classifies images of that class.\n"]},{"cell_type":"markdown","metadata":{"id":"SF71bN55cZzi"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"S5yg44U8cZzk"},"source":["## --- start your code here ----\n","\n","NIM  = ??\n","Nama = ??\n","\n","## --- end your code here ----"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6VAE_sE-Bgk1"},"source":["---\n","---\n","#[Part 0] Import Libraries and Load Data"]},{"cell_type":"code","metadata":{"id":"F2s9tmfkhhTu"},"source":["import torch\n","import torchvision\n","import torchvision.transforms as T\n","import random\n","import numpy as np\n","from scipy.ndimage.filters import gaussian_filter1d\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","SQUEEZENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","SQUEEZENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WmMOQpzthhT3"},"source":["---\n","## 1 - Helper Functions\n","\n","Our pretrained model was trained on images that had been preprocessed by subtracting the per-color mean and dividing by the per-color standard deviation. \n","\n","We define a few helper functions for performing and undoing this preprocessing. \n","\n","You don't need to do anything in these cells."]},{"cell_type":"markdown","metadata":{"id":"okLO72JrwBCR"},"source":["---\n","### a. Preprocess Image\n","\n","preprocess image by normalizeing (zero centering) according to `squeezenet` model\n","\n","you need to change the function if you use another pretrained model"]},{"cell_type":"code","metadata":{"id":"5t1E3E9MwZO-"},"source":["def preprocess(img, size=224):\n","    transform = T.Compose([\n","        T.Resize(size),\n","        T.ToTensor(),\n","        T.Normalize(mean=SQUEEZENET_MEAN.tolist(),\n","                    std=SQUEEZENET_STD.tolist()),\n","        T.Lambda(lambda x: x[None]),\n","    ])\n","    return transform(img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VsYs1KO9wD7Y"},"source":["---\n","### b. De-Process Image\n","Denormalize back image for viewing purpose\n","\n","again, you need to change the function if you use another pretrained model"]},{"cell_type":"code","metadata":{"id":"90RVUlwswvkv"},"source":["def rescale(x):\n","    low, high = x.min(), x.max()\n","    x_rescaled = (x - low) / (high - low)\n","    return x_rescaled\n","\n","def deprocess(img, should_rescale=True):\n","    transform = T.Compose([\n","        T.Lambda(lambda x: x[0]),\n","        T.Normalize(mean=[0, 0, 0], std=(1.0 / SQUEEZENET_STD).tolist()),\n","        T.Normalize(mean=(-SQUEEZENET_MEAN).tolist(), std=[1, 1, 1]),\n","        T.Lambda(rescale) if should_rescale else T.Lambda(lambda x: x),\n","        T.ToPILImage(),\n","    ])\n","    return transform(img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCnbsppvwD-Q"},"source":["---\n","### c. Blur Image\n","\n","Image jittering by blurring the image"]},{"cell_type":"code","metadata":{"id":"ueEXSBg3hhT4"},"source":["def blur_image(X, sigma=1):\n","    X_np = X.cpu().clone().numpy()\n","    X_np = gaussian_filter1d(X_np, sigma, axis=2)\n","    X_np = gaussian_filter1d(X_np, sigma, axis=3)\n","    X.copy_(torch.Tensor(X_np).type_as(X))\n","    return X"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DVKcTRxihhUQ"},"source":["---\n","## 2 - ImageNet Validation\n","We have provided a few example images from the validation set of the ImageNet ILSVRC 2012 Classification dataset. \n","\n","Since they come from the validation set, our pretrained model did not see these images during training.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f808mtv1SgM7"},"source":["---\n","### a. Load Imagenet Validation"]},{"cell_type":"code","metadata":{"id":"FbkK2G9zkM2J"},"source":["!wget 'https://raw.githubusercontent.com/CNN-ADF/Task2020/master/resources/imagenet_val_25.npz' -q"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A4NM6OwQhhUR"},"source":["def load_imagenet_val(num=None):\n","    f = np.load('imagenet_val_25.npz', allow_pickle=True)\n","    X = f['X']\n","    y = f['y']\n","    class_names = f['label_map'].item()\n","    idx = np.arange(25)\n","    np.random.shuffle(idx)\n","    if num is not None:\n","        idx = idx[:num]\n","        X   = X[idx]\n","        y   = y[idx]\n","    return X, y, class_names"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AcjClhlhhhUW"},"source":["Now load randomly some image for this exercise\n","\n","you can change the `num=5` to get more image from `imagenet validation set`. \n","\n","The maximum is `25` or `None`"]},{"cell_type":"code","metadata":{"id":"uuEUYtoZSyIb"},"source":["X, y, class_names = load_imagenet_val(num=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DDZ0FpEsSpxU"},"source":["---\n","### b. View Images\n","Run the following cell to visualize some of these images, along with their ground-truth labels."]},{"cell_type":"code","metadata":{"id":"Ambe-9oZhhUY"},"source":["plt.figure(figsize=(12, 6))\n","for i in range(5):\n","    plt.subplot(1, 5, i + 1)\n","    plt.imshow(X[i])\n","    plt.title(class_names[y[i]])\n","    plt.axis('off')\n","plt.gcf().tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4qjR4RqnhhUJ"},"source":["---\n","---\n","# [Part 1] SqueezeNet Pretrained Model\n","\n","For all of our image generation experiments, we will start with a convolutional neural network which was pretrained to perform image classification on ImageNet. We can use any model here, but for the purposes of this assignment we will use SqueezeNet [1], which achieves accuracies comparable to AlexNet but with a significantly reduced parameter count and computational complexity.\n","\n","Using SqueezeNet rather than AlexNet or VGG or ResNet means that we can easily perform all image generation experiments on CPU.\n","\n","But you can try any other model from PyTorch. See the list [here](https://pytorch.org/docs/stable/torchvision/models.html)\n","\n","[1] Iandola et al, \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5MB model size\", arXiv 2016"]},{"cell_type":"markdown","metadata":{"id":"baK1VzEjNGGJ"},"source":["---\n","## 1 - Load SqueezeNet\n","\n","Download and load the pretrained SqueezeNet model."]},{"cell_type":"code","metadata":{"id":"Vw568akRhhUL"},"source":["model = torchvision.models.squeezenet1_1(pretrained=True)\n","print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5zGYQYoxNNeH"},"source":["---\n","## 2 - Freeze the Model\n","We don't want to train the model, so tell PyTorch not to compute gradients\n","with respect to model parameters.\n","\n","you may see warning regarding initialization deprecated, that's fine, please continue to next steps\n"]},{"cell_type":"code","metadata":{"id":"wF7oWrJpNVED"},"source":["for param in model.parameters():\n","    param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MVsnrja-hhUd"},"source":["---\n","---\n","# [Part 2] Saliency Maps\n","Using this pretrained model, we will compute class saliency maps as described in Section 3.1 of paper [[2]](https://arxiv.org/abs/1312.6034).\n","\n","A **saliency map** tells us the degree to which each pixel in the image affects the classification score for that image. \n","\n","[2] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. \"Deep Inside Convolutional Networks: Visualising\n","Image Classification Models and Saliency Maps\", ICLR Workshop 2014."]},{"cell_type":"markdown","metadata":{"id":"_JrDS4SMhhUf"},"source":["---\n","## 1 - PyTorch `gather` method\n","Recall in previous exercises you needed to select one element from each row of a matrix; \n","* if `s` is an numpy array of shape `(N, C)` and `y` is a numpy array of shape `(N,)` containing integers `0 <= y[i] < C`, \n","* then `s[np.arange(N), y]` is a numpy array of shape `(N,)` which selects one element from each element in `s` using the indices in `y`.\n","\n","In PyTorch you can perform the same operation using the `gather()` method.\n","\n","---\n","run the following cell to see an example.\n","\n","You can also read the documentation for [the gather method](http://pytorch.org/docs/torch.html#torch.gather)\n","and [the squeeze method](http://pytorch.org/docs/torch.html#torch.squeeze)."]},{"cell_type":"code","metadata":{"id":"aHIDSU9xhhUg"},"source":["# Example of using gather to select one entry from each row in PyTorch\n","def gather_example():\n","    N, C = 4, 5\n","    s = torch.randn(N, C)\n","    y = torch.LongTensor([1, 2, 1, 3])\n","    print('s =\\n', s)\n","    print('y        = ',y)\n","    print('s.gather = ',s.gather(1, y.view(-1, 1)).squeeze())\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J2R5Qi1ChhUl"},"source":["gather_example()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5-wtdo1ZhhUo"},"source":["**Expected Output**:\n","<pre>\n","s = \n","tensor([[ ??,   <b>A</b>,  ??,  ??,  ??],\n","        [ ??,  ??,   <b>B</b>,  ??,  ??],\n","        [ ??,   <b>C</b>,  ??,  ??,  ??],\n","        [ ??,  ??,  ??,   <b>D</b>,  ??]])\n","<i>*example of random values</i>\n","\n","y        =  tensor([1, 2, 1, 3])\n","s.gather =  tensor([<b>A</b>, <b>B</b>, <b>C</b>, <b>D</b>])\n","<i>*tensor with number gathered from respective column from tensor s</i>"]},{"cell_type":"markdown","metadata":{"id":"TI3XonVyTxn-"},"source":["---\n","## 2 - Saliency Map Function\n","\n","A **saliency map** tells us the degree to which each pixel in the image affects the classification score for that image. \n","\n","To compute it, we compute the gradient of the unnormalized score corresponding to the correct class (which is a scalar) with respect to the pixels of the image. \n","* If the image has shape $(3\\times \\text{H}\\times \\text{W})$ then this gradient will also have shape $(3\\times \\text{H}\\times \\text{W})$; for each pixel in the image, this gradient tells us the amount by which the classification score will change if the pixel changes by a small amount. \n","* To compute the saliency map, we take the absolute value of this gradient, then take the maximum value over the 3 input channels; the final saliency map thus has shape $(\\text{H}\\times \\text{W})$ and all entries are nonnegative."]},{"cell_type":"markdown","metadata":{"id":"xc2RyhhXUYMU"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement this function. \n","* Perform a forward and backward pass through the model to compute the gradient of the correct class score with respect to each input image. \n","* You first want to compute the loss over the correct scores (we'll combine losses across a batch by summing), \n","* and then compute the gradients with a backward pass. \n","\n"]},{"cell_type":"code","metadata":{"id":"I9y0PoRLhhUq"},"source":["def compute_saliency_maps(X, y, model):\n","    \"\"\"\n","    Compute a class saliency map using the model for images X and labels y.\n","\n","    Input:\n","    - X: Input images; Tensor of shape (N, 3, H, W)\n","    - y: Labels for X; LongTensor of shape (N,)\n","    - model: A pretrained CNN that will be used to compute the saliency map.\n","\n","    Returns:\n","    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n","    images.\n","    \"\"\"\n","    # Make sure the model is in \"test\" mode\n","    model.eval()\n","    \n","    # Make input tensor require gradient\n","    X.requires_grad_()\n","    \n","    # 1. Forward pass\n","    # calculate forward pass from model with input X\n","    scores = model(X)\n","    \n","    # 2. Get correct class scores\n","    # call gather() function from scores tensor, \n","    # with input 1 and y.view(-1, 1)\n","    # then squeeze it by calling .squeeze() function\n","    # (see the gather method example)\n","    scores = scores.??.??\n","    \n","    # 3. Backward pass\n","    # Note: scores is a tensor here, need to supply initial gradients of same tensor shape as scores.\n","    # get scores size\n","    scores_size = scores.shape\n","\n","    # create ones tensor matrix with size of scores\n","    # call torch.ones() function with inpput scores_size\n","    ones_tensor = ??\n","    \n","    \n","    # call .backward() function from scores tensor with input ones_tensor\n","    scores.??\n","\n","    # 4. retrieve the gradient as saliency map\n","    saliency = X.grad\n","    \n","    # 5. Convert 3d to 1d\n","    saliency = saliency.abs()\n","    saliency, _= torch.max(saliency, dim=1)\n","    \n","    return saliency"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Xg5uILPhhUu"},"source":["---\n","## 3 - Show Saliency Map\n","Once you have completed the implementation in the cell above, run the following to visualize some class saliency maps on our example images from the ImageNet validation set:"]},{"cell_type":"code","metadata":{"id":"UWQezGkYhhUw"},"source":["# Convert X and y from numpy arrays to Torch Tensors\n","X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n","y_tensor = torch.LongTensor(y)\n","\n","# Compute saliency maps for images in X\n","saliency = compute_saliency_maps(X_tensor, y_tensor, model)\n","\n","# Convert the saliency map from Torch Tensor to numpy array and show images\n","# and saliency maps together.\n","saliency = saliency.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N--9y5OsqMhY"},"source":["visualize the saliency"]},{"cell_type":"code","metadata":{"id":"BTQIR8j4o5sL"},"source":["N = X.shape[0]\n","for i in range(N):\n","    plt.subplot(2, N, i + 1)\n","    plt.imshow(X[i])\n","    plt.axis('off')\n","    plt.title(class_names[y[i]])\n","    plt.subplot(2, N, N + i + 1)\n","    plt.imshow(saliency[i], cmap=plt.cm.hot)\n","    plt.axis('off')\n","    plt.gcf().set_size_inches(12, 5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ynj6ewSep--V"},"source":["You can try for another image by loading another random images, then re-run the 2 cells above"]},{"cell_type":"code","metadata":{"id":"_7xeuJmSqCtB"},"source":["X, y, class_names = load_imagenet_val(num=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OzoUuIYBhhUz"},"source":["---\n","---\n","# [Part 3] Class Visualization\n","By starting with a random noise image and performing gradient ascent on a target class, we can generate an image that the network will recognize as the target class. \n","\n","This idea was first presented in [[2]](https://arxiv.org/abs/1312.6034); then\n","[[3]](https://arxiv.org/abs/1506.06579) extended this idea by suggesting several regularization techniques that can improve the quality of the generated image.\n","\n","[2] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. \"Deep Inside Convolutional Networks: Visualising\n","Image Classification Models and Saliency Maps\", ICLR Workshop 2014.\n","\n","[3] Yosinski et al, \"Understanding Neural Networks Through Deep Visualization\", ICML 2015 Deep Learning Workshop"]},{"cell_type":"markdown","metadata":{"id":"AzQw5myGWO-C"},"source":["---\n","Concretely,\n","* Let $I$ be an image and let $y$ be a target class.\n","* Let $s_y(I)$ be the score that a convolutional network assigns to the image $I$ for class $y$;\n","   * note that these are raw unnormalized scores, not class probabilities. \n","* We wish to generate an image $I^*$ that achieves a high score for the class $y$ by solving the problem\n","$$\n","I^* = \\arg\\max_I (s_y(I) - R(I))\n","$$\n","where $R$ is a (possibly implicit) regularizer (note the sign of $R(I)$ in the argmax: we want to minimize this regularization term). \n","* We can solve this optimization problem using gradient ascent, computing gradients with respect to the generated image. \n","\n","* We will use (explicit) L2 regularization of the form\n","$$\n","R(I) = \\lambda \\|I\\|_2^2\n","$$\n","* **and** implicit regularization as suggested by [3] by periodically blurring the generated image. \n","\n","We can solve this problem using gradient ascent on the generated image."]},{"cell_type":"markdown","metadata":{"id":"4IZYwSLjWqCh"},"source":["***\n","## 1 - Jittering Function\n","Helper function to randomly jitter and blur an image.\n"]},{"cell_type":"code","metadata":{"id":"_bYYXiHUhhU0"},"source":["def jitter(X, ox, oy):\n","    \"\"\"    \n","    Inputs\n","    - X: PyTorch Tensor of shape (N, C, H, W)\n","    - ox, oy: Integers giving number of pixels to jitter along W and H axes\n","    \n","    Returns: A new PyTorch Tensor of shape (N, C, H, W)\n","    \"\"\"\n","    if ox != 0:\n","        left  = X[:, :, :, :-ox]\n","        right = X[:, :, :, -ox:]\n","        X = torch.cat([right, left], dim=3)\n","    if oy != 0:\n","        top    = X[:, :, :-oy]\n","        bottom = X[:, :, -oy:]\n","        X = torch.cat([bottom, top], dim=2)\n","    return X"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UuPi0PL2W5pi"},"source":["---\n","## 2 - Class Visualization Function\n","\n","Implement function to Generate an image to maximize the score of target_y under a pretrained model."]},{"cell_type":"markdown","metadata":{"id":"4aZwXofRXGfp"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","complete the implementation of the `create_class_visualization` function.\n","\n","* Use the model to compute the gradient of the score for the  class `target_y` with respect to the pixels of the image, \n","* then make a gradient step on the image using the learning rate. \n","* Don't forget the L2 regularization term!\n","* Be very careful about the signs of elements in your code.            \n","\n","<br>\n","\n"," **NOTE**:\n","\n"," if `x` is a tensor, the value of `x` can be retrieved by calling `x.data`\n","\n"]},{"cell_type":"code","metadata":{"id":"mKo65gcBhhU5"},"source":["def create_class_visualization(target_y, model, dtype, **kwargs):\n","    \"\"\"\n","    Inputs:\n","    - target_y: Integer in the range [0, 1000) giving the index of the class\n","    - model: A pretrained CNN that will be used to generate the image\n","    - dtype: Torch datatype to use for computations\n","    \n","    Keyword arguments:\n","    - l2_reg: Strength of L2 regularization on the image\n","    - learning_rate: How big of a step to take\n","    - num_iterations: How many iterations to use\n","    - blur_every: How often to blur the image as an implicit regularizer\n","    - max_jitter: How much to gjitter the image as an implicit regularizer\n","    - show_every: How often to show the intermediate result\n","    \"\"\"\n","\n","    # prepare the hyperparameters\n","    model.type(dtype)\n","    l2_reg = kwargs.pop('l2_reg', 1e-3)\n","    learning_rate = kwargs.pop('learning_rate', 25)\n","    num_iterations = kwargs.pop('num_iterations', 100)\n","    blur_every = kwargs.pop('blur_every', 10)\n","    max_jitter = kwargs.pop('max_jitter', 16)\n","    show_every = kwargs.pop('show_every', 25)\n","\n","    # Randomly initialize the image as a PyTorch Tensor, and make it requires gradient.\n","    img = torch.randn(1, 3, 224, 224).mul_(1.0).type(dtype).requires_grad_()\n","\n","    for t in range(num_iterations):\n","        # Randomly jitter the image a bit; this gives slightly nicer results\n","        ox, oy = random.randint(0, max_jitter), random.randint(0, max_jitter)\n","        img.data.copy_(jitter(img.data, ox, oy))\n","\n","        # 1. Forward pass\n","        #    calculate forward pass from model() with input img        \n","        #    (see the implementation from compute_saliency_maps function above)\n","        scores = ??\n","        \n","        #    get the target score\n","        target_score = scores[0, target_y]\n","        \n","        #    perform backward pass from target_score by calling .backward() function\n","        target_score.??\n","        \n","        # 2. perform L2 regularization\n","        #    get the image gradient value by calling img.grad.data\n","        grad = img.??.??\n","        \n","        #    new grad is old grad subtracted by 2 * l2_reg * img.data\n","        grad -= 2 * l2_reg * img.data\n","        \n","        # 3. update image using gradient ascent\n","        #    new img.data is the old img.data added by learning rate * (grad / grad.norm()) \n","        img.data = ??\n","        \n","        #    reset img.grad by calling .zero_() function\n","        img.grad.??\n","                \n","        # 3. Undo the random jitter\n","        img.data.copy_(jitter(img.data, -ox, -oy))\n","\n","        # As regularizer, clamp and periodically blur the image\n","        for c in range(3):\n","            lo = float(-SQUEEZENET_MEAN[c] / SQUEEZENET_STD[c])\n","            hi = float((1.0 - SQUEEZENET_MEAN[c]) / SQUEEZENET_STD[c])\n","            img.data[:, c].clamp_(min=lo, max=hi)\n","        if t % blur_every == 0:\n","            blur_image(img.data, sigma=0.5)\n","        \n","        # Periodically show the image result\n","        if t == 0 or (t + 1) % show_every == 0 or t == num_iterations - 1:\n","            plt.imshow(deprocess(img.data.clone().cpu()))\n","            class_name = class_names[target_y]\n","            plt.title('%s\\nIteration %d / %d' % (class_name, t + 1, num_iterations))\n","            plt.gcf().set_size_inches(4, 4)\n","            plt.axis('off')\n","            plt.show()\n","\n","    return deprocess(img.data.cpu())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gJ1hZP0DqYQ8"},"source":["---\n","## 3 - Show Class Visualization\n","\n","Once you have completed the implementation in the cell above, run the following cell to generate an image of a Tarantula:"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"XD_yg1lLhhU-"},"source":["dtype = torch.cuda.FloatTensor\n","model.type(dtype)\n","\n","# uncomment target that you want to visualize\n","target_y = 76 # Tarantula\n","# target_y = 78 # Tick\n","# target_y = 187 # Yorkshire Terrier\n","# target_y = 683 # Oboe\n","# target_y = 366 # Gorilla\n","# target_y = 604 # Hourglass\n","\n","out = create_class_visualization(target_y, model, dtype)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pXnxAsqmhhVB"},"source":["Try out your class visualization on other classes! You should also feel free to play with various hyperparameters to try and improve the quality of the generated image, but this is not required."]},{"cell_type":"code","metadata":{"id":"KQdwonUYhhVB"},"source":["# target_y = 76 # Tarantula\n","# target_y = 78 # Tick\n","# target_y = 187 # Yorkshire Terrier\n","# target_y = 683 # Oboe\n","target_y = 366 # Gorilla\n","# target_y = 604 # Hourglass\n","\n","out = create_class_visualization(target_y, model, dtype)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nLsndIxthhVG"},"source":["---\n","---\n","# [Part 4] Fooling Images\n","We can also use image gradients to generate \"fooling images\" as discussed in [[4]](https://arxiv.org/abs/1312.6199). \n","\n","Given an image and a target class, we can perform **gradient ascent** over the image to maximize the target class, stopping when the network classifies the image as the target class. \n","\n","\n","[4] Szegedy et al, \"Intriguing properties of neural networks\", ICLR 2014"]},{"cell_type":"markdown","metadata":{"id":"cNIeEPJtk_GA"},"source":["---\n","## 1 - Fooling Image Function\n","\n","                                                                          \n","For most examples, you should be able to generate a fooling image in fewer than 100 iterations of gradient ascent.  "]},{"cell_type":"markdown","metadata":{"id":"3WyjMw6PjIw8"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement the following function to generate fooling images.\n","\n","* Generate a fooling image `X_fooling` that the model will classify as the class `target_y`. \n","* You should perform **gradient ascent** on the score of the target class, stopping when the model is fooled.     \n","* When computing an update step, first normalize the gradient:              \n","  $dX = learning\\_rate * \\dfrac{g}{||g||}$\n","                        \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"lLsi-_FjhhVH"},"source":["def make_fooling_image(X, target_y, model):\n","    \"\"\"\n","    Generate a fooling image that is close to X, but that the model classifies\n","    as target_y.\n","\n","    Inputs:\n","    - X: Input image; Tensor of shape (1, 3, 224, 224)\n","    - target_y: An integer in the range [0, 1000)\n","    - model: A pretrained CNN\n","\n","    Returns:\n","    - X_fooling: An image that is close to X, but that is classifed as target_y\n","    by the model.\n","    \"\"\"\n","    # Initialize our fooling image to the input image, and make it require gradient\n","    X_fooling = X.clone()\n","    X_fooling = X_fooling.requires_grad_()\n","    \n","    learning_rate = 1\n","\n","    \n","    for i in range(100):\n","                \n","        # Forward pass\n","        # calculate forward pass from model with input X_fooling\n","        # (see the implementation from compute_saliency_maps function above)\n","        scores = ??\n","        \n","        # get the current maximum scores index\n","        # use torch.argmax() function with input scores and dim=1\n","        index = ??\n","        \n","        # if the maximum score (index[0]) is equal to target_y, \n","        # break the process\n","        if index[0] == target_y:\n","            break\n","            \n","        # get the target score\n","        target_score = scores[0, target_y]\n","        \n","        # perform backward pass from target_score by calling backward() function\n","        target_score.??\n","        \n","        # get the X_fooling image gradient value \n","        # by calling X_fooling.grad.data\n","        grad = X_fooling.??.??\n","        \n","        # Update image using graident ascent\n","        # new X_fooling.data is the old X_fooling.data added by learning rate * (grad / grad.norm()) \n","        X_fooling.data = ??\n","        \n","        # reset X_fooling.grad by calling .zero_() function\n","        X_fooling.??\n","    \n","    return X_fooling"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IwX_bpyahhVP"},"source":["---\n","## 2 - Fool a Class\n","\n","Now to create a fooling image, first we decide what class we want to turn the image into\n","\n","\n","you can see the list of label in `class_names` dictionary"]},{"cell_type":"code","metadata":{"id":"gP2Sd2aGpcm3"},"source":["## uncoment to print all class names\n","# class_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPcyx1xhhhVQ"},"source":["# print only 20 first class name\n","dict(list(class_names.items())[0: 20])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H4bX98vphhVK"},"source":["\n","\n","Run the following cell to generate a fooling image.\n","\n","You should ideally see at first glance no major difference between the original and fooling images, and the network should now make an incorrect prediction on the fooling one.\n","\n","However you should see a bit of random noise if you look at the 10x magnified difference between the original and fooling images. \n","\n","Feel free to change the `idx` variable to explore other images."]},{"cell_type":"code","metadata":{"id":"J7Z6yyyXhhVL"},"source":["# change idx to any image id\n","idx = 1\n","\n","# change target_y to any class\n","target_y = 16\n","\n","\n","X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0).cuda()\n","X_fooling = make_fooling_image(X_tensor[idx:idx+1], target_y, model)\n","\n","scores = model(X_fooling)\n","assert target_y == scores.data.max(1)[1][0].item(), 'The model is not fooled!'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-TxTUIAhhVT"},"source":["After generating a fooling image, run the following cell to visualize the original image, the fooling image, as well as the difference between them."]},{"cell_type":"code","metadata":{"id":"najFxH6ZhhVT"},"source":["X_fooling_np = deprocess(X_fooling.cpu().clone())\n","X_fooling_np = np.asarray(X_fooling_np).astype(np.uint8)\n","\n","plt.subplot(1, 4, 1)\n","plt.imshow(X[idx])\n","plt.title('actual:\\n'+class_names[y[idx]])\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 2)\n","plt.imshow(X_fooling_np)\n","plt.title('fooled into:\\n'+class_names[target_y])\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 3)\n","X_pre = preprocess(Image.fromarray(X[idx])).cuda()\n","X_diff = (X_fooling - X_pre).cpu()\n","diff = np.asarray(deprocess(X_diff, should_rescale=False))\n","plt.imshow(diff)\n","plt.title('Difference')\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 4)\n","diff = np.asarray(deprocess(10 * (X_diff), should_rescale=False))\n","plt.imshow(diff)\n","plt.title('Magnified difference (10x)')\n","plt.axis('off')\n","\n","plt.gcf().set_size_inches(12, 5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xuujlTQYhhVW"},"source":["---\n","---\n","\n","# Congratulation, You've Completed Exercise 12 part 1\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2020 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"id":"7_Hkr9kg9Do-"},"source":["![footer](https://i.ibb.co/yX0jfMS/footer2020.png)"]}]}