{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"CV2020 - 17 - Recurrent Neural Net.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"f_ChrQTZyAZU"},"source":["![title](https://i.ibb.co/f2W87Fg/logo2020.png)\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"MQjL2XtX7uKJ"},"source":["<table  class=\"tfo-notebook-buttons\" align=\"left\"><tr><td>\n","    \n","<a href=\"https://colab.research.google.com/github/adf-telkomuniv/CV2020_Exercises/blob/main/CV2020 - 17 - Recurrent Neural Net.ipynb\" source=\"blank\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n","</td><td>\n","<a href=\"https://github.com/adf-telkomuniv/CV2020_Exercises/blob/main/CV2020 - 17 - Recurrent Neural Net.ipynb\" source=\"blank\" ><img src=\"https://i.ibb.co/6NxqGSF/pinpng-com-github-logo-png-small.png\"></a>\n","    \n","</td></tr></table>"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"y3ltBKv-ogtS"},"source":["# Task 17 - Recurrent Neural Net\n","\n","In this exercise you will implement a vanilla recurrent neural networks from scratch and use them it to train a model that can generate novel captions for images.\n","\n","The goals of this assignment are as follows:\n","\n","* Understand the architecture of recurrent neural networks (RNNs) and how they operate on sequences by sharing weights over time\n","* Understand and implement Vanilla RNNs \n","* Understand how to sample from an RNN language model at test-time\n","* Understand how to combine convolutional neural nets and recurrent nets to implement an image captioning system"]},{"cell_type":"markdown","metadata":{"id":"SF71bN55cZzi"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"S5yg44U8cZzk"},"source":["## --- start your code here ----\n","\n","NIM  = ??\n","Nama = ??\n","\n","## --- end your code here ----"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BkN9g_WeogtU"},"source":["---\n","---\n","#[Part 0] Import Libraries \n","As usual, a bit of setup"]},{"cell_type":"code","metadata":{"id":"UY9wLMmgogtW"},"source":["import time, os, json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import urllib.request, urllib.error, urllib.parse, os, tempfile\n","from imageio import imread\n","from PIL import Image\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PvMOoCw7ogtd"},"source":["---\n","## 1 - Helper Functions\n","\n","Below are helper functions to check your implementation"]},{"cell_type":"markdown","metadata":{"id":"Bl8Ql2-7o70u"},"source":["---\n","### a. Numerical Gradient Check\n","\n","Functions to check the gradients from your implementation\n","\n","a naive implementation of numerical gradient of $f$ at $x$\n"]},{"cell_type":"code","metadata":{"id":"DCqtT7Ttogtf"},"source":["from random import randrange\n","\n","def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n","\n","    # evaluate function value at original point\n","    fx = f(x) \n","    grad = np.zeros_like(x)\n","\n","    # iterate over all indexes in x\n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","    \n","    while not it.finished:\n","\n","        # evaluate function at x+h\n","        ix     = it.multi_index\n","        oldval = x[ix]\n","        x[ix]  = oldval + h # increment by h\n","        fxph   = f(x)       # evalute f(x + h)\n","        x[ix]  = oldval - h\n","        fxmh   = f(x)       # evaluate f(x - h)\n","        x[ix]  = oldval     # restore\n","\n","        # compute the partial derivative with centered formula\n","        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n","\n","        if verbose:\n","            print(ix, grad[ix])\n","        it.iternext() # step to next dimension\n","\n","    return grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5P7el209pB5m"},"source":["---\n","### b. Array Gradient Check\n","\n","Functions to check the array of gradients from your implementation\n","\n","Evaluate a numeric gradient for a function that accepts a numpy    array and returns a numpy array.\n"]},{"cell_type":"code","metadata":{"id":"S_DNe1giogtk"},"source":["def eval_numerical_gradient_array(f, x, df, h=1e-5):\n","\n","    grad = np.zeros_like(x)\n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","\n","    while not it.finished:\n","\n","        ix = it.multi_index\n","        oldval = x[ix]\n","        x[ix]  = oldval + h\n","        pos    = f(x).copy()\n","        x[ix]  = oldval - h\n","        neg    = f(x).copy()\n","        x[ix]  = oldval\n","\n","        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n","        it.iternext()\n","        \n","    return grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCnbsppvwD-Q"},"source":["---\n","### c. Relative Error Function\n","\n","Function to calculate difference between your matrix and our expected results"]},{"cell_type":"code","metadata":{"id":"hBF5HAFBogtp"},"source":["def rel_error(x, y):\n","  \n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MYMv3bjxJJ3K"},"source":["---\n","### d. Download Image Funtion\n","\n","Read an image from a URL. Returns a numpy array with the pixel data."]},{"cell_type":"code","metadata":{"id":"FM1gc4QAoguI"},"source":["def image_from_url(url):\n","    try:\n","        f = urllib.request.urlopen(url)\n","        _, fname = tempfile.mkstemp()\n","        with open(fname, 'wb') as ff:\n","            ff.write(f.read())\n","\n","        img = imread(fname)\n","        # os.remove(fname)\n","        \n","        return img\n","\n","    except urllib.error.URLError as e:\n","        print('URL Error: ', e.reason, url)\n","        return Image.new('RGB', (64,64), color='LightGray')\n","        \n","    except urllib.error.HTTPError as e:\n","        print('HTTP Error: ', e.code, url) \n","        return Image.new('RGB', (64,64), color='LightGray')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"amwox6dGogtu"},"source":["---\n","## 2 - Install h5py\n","The COCO dataset we will be using is stored in HDF5 format. To load HDF5 files, we will need to install the `h5py` Python package. \n","\n","So install it if it's not already installed"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"NCzppM-Dogtv"},"source":["!pip install h5py\n","\n","import h5py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ze9A7cpBogtz"},"source":["---\n","---\n","# [Part 1] Microsoft COCO Dataset\n","For this exercise we will use the $2014$ release of the [Microsoft COCO dataset](http://cocodataset.org/) which has become the standard testbed for image captioning.\n","\n","The dataset consists of $80,000$ training images and $40,000$ validation images,\n","\n","Each annotated with $5$ captions written by workers on **Amazon Mechanical Turk**."]},{"cell_type":"markdown","metadata":{"id":"TF-OMq63ogt2"},"source":["---\n","## 1 - Download Preprocessed Data\n","\n","We have preprocessed the data and extracted features for you already. For all images we have extracted features from the **fc7** layer of the **VGG-16** network pretrained on ImageNet; these features are stored in the files `train2014_vgg16_fc7.h5` and `val2014_vgg16_fc7.h5` respectively. \n","\n","To cut down on processing time and memory requirements, we have reduced the dimensionality of the features from $4096$ to $512$; these features can be found in the files `train2014_vgg16_fc7_pca.h5` and `val2014_vgg16_fc7_pca.h5`.\n","\n","The raw images take up a lot of space (nearly **38GB**) so we have not included them in the download. However all images are taken from Flickr, and URLs of the training and validation images are stored in the files `train2014_urls.txt` and `val2014_urls.txt` respectively. \n","\n","This allows you to download images on the fly for visualization."]},{"cell_type":"markdown","metadata":{"id":"8eD-4LQLHvse"},"source":["We've provided you with the implementation to download the dataset"]},{"cell_type":"code","metadata":{"id":"y8jqWkbzwUUF"},"source":["!wget -O 'coco_downloader.py' 'https://github.com/adf-telkomuniv/CV2020_Exercises/raw/main/resources/coco_downloader.py' -q"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7VZDy_SFb2M7"},"source":["Then call the `download_coco()` function below to download the Preprocessed Microsoft COCO Dataset"]},{"cell_type":"code","metadata":{"id":"CUG7aCqOwdLd"},"source":["from coco_downloader import *\n","\n","download_coco()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cjv_ztgYcBI9"},"source":["unzip the datasets, and delete the compressed one"]},{"cell_type":"code","metadata":{"id":"Dpj5IlpwsVln"},"source":["!unzip -o coco_captioning.zip\n","\n","!rm coco_captioning.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lVcujoQHogt4"},"source":["---\n","## 2 - Text Processing\n","\n","Dealing with strings is inefficient, so we will work with an encoded version of the captions.\n","\n","Each word is assigned an integer ID, allowing us to represent a caption by a sequence of integers. The mapping between integer IDs and words is in the file `coco2014_vocab.json`, and you can use the function `decode_captions` to convert numpy arrays of integer IDs back into strings."]},{"cell_type":"code","metadata":{"id":"sSRNI_Juogt6"},"source":["def decode_captions(captions, idx_to_word):\n","  \n","    singleton = False\n","    if captions.ndim == 1:\n","        singleton = True\n","        captions = captions[None]\n","\n","    decoded = []\n","    N, T = captions.shape\n","    for i in range(N):\n","        words = []\n","        for t in range(T):\n","            word = idx_to_word[captions[i, t]]\n","            if word != '<NULL>':\n","                words.append(word)\n","            if word == '<END>':\n","                break\n","        decoded.append(' '.join(words))\n","    if singleton:\n","        decoded = decoded[0]\n","\n","    return decoded"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3NbCgWUuogt9"},"source":["---\n","## 3 - Load Coco Dataset\n","\n","There are a couple special tokens that we add to the vocabulary.\n","\n","We prepend a special <font color='red'>**`<START>`**</font> token and append an <font color='red'>**`<END>`**</font>  token to the beginning and end of each caption respectively. Rare words are replaced with a special <font color='red'>**`<UNK>`**</font>  token (for \"unknown\"). \n","\n","In addition, since we want to train with minibatches containing captions of different lengths, we pad short captions with a special <font color='red'>**`<NULL>`**</font>  token after the `<END>` token and don't compute loss or gradient for `<NULL>` tokens. \n","\n","Since they are a bit of a pain, we have taken care of all implementation details around special tokens for you.\n","\n","You can load all of the MS-COCO data (captions, features, URLs, and vocabulary) using the `load_coco_data` function below. Run the following cell to do so:"]},{"cell_type":"code","metadata":{"id":"Kh5q32zeogt_"},"source":["def load_coco_data(base_dir='coco_captioning', max_train=None, pca_features=True):\n","\n","    data = {}\n","    \n","    # read caption encoding\n","    caption_file = os.path.join(base_dir, 'coco2014_captions.h5')\n","    with h5py.File(caption_file, 'r') as f:\n","        for k, v in f.items():\n","            data[k] = np.asarray(v)\n","\n","    # use original training features or pca-reduced features\n","    if pca_features:\n","        train_feat_file = os.path.join(base_dir, 'train2014_vgg16_fc7_pca.h5')\n","    else:\n","        train_feat_file = os.path.join(base_dir, 'train2014_vgg16_fc7.h5')\n","    with h5py.File(train_feat_file, 'r') as f:\n","        data['train_features'] = np.asarray(f['features'])\n","\n","    # use original validation features or pca-reduced features\n","    if pca_features:\n","        val_feat_file = os.path.join(base_dir, 'val2014_vgg16_fc7_pca.h5')\n","    else:\n","        val_feat_file = os.path.join(base_dir, 'val2014_vgg16_fc7.h5')\n","    with h5py.File(val_feat_file, 'r') as f:\n","        data['val_features'] = np.asarray(f['features'])\n","\n","    # read vocabulary token\n","    dict_file = os.path.join(base_dir, 'coco2014_vocab.json')\n","    with open(dict_file, 'r') as f:\n","        dict_data = json.load(f)\n","        for k, v in dict_data.items():\n","            data[k] = v\n","\n","    # read training image urls\n","    train_url_file = os.path.join(base_dir, 'train2014_urls.txt')\n","    with open(train_url_file, 'r') as f:\n","        train_urls = np.asarray([line.strip() for line in f])\n","    data['train_urls'] = train_urls\n","\n","    # read validation image urls\n","    val_url_file = os.path.join(base_dir, 'val2014_urls.txt')\n","    with open(val_url_file, 'r') as f:\n","        val_urls = np.asarray([line.strip() for line in f])\n","    data['val_urls'] = val_urls\n","\n","    # Maybe subsample the training data\n","    if max_train is not None:\n","        num_train = data['train_captions'].shape[0]\n","        mask = np.random.randint(num_train, size=max_train)\n","\n","        data['train_captions']   = data['train_captions'][mask]\n","        data['train_image_idxs'] = data['train_image_idxs'][mask]\n","\n","    return data\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zLfrsgZQInai"},"source":["Load COCO data from disk; this returns a dictionary. We'll work with dimensionality-reduced features for this notebook, but feel free to experiment with the original features by changing the **`pca_features`** flag below."]},{"cell_type":"code","metadata":{"id":"v9wwijpKoguD"},"source":["data = load_coco_data(pca_features=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DD24CW8EI3qp"},"source":["Print out all the keys and values from the data dictionary\n"]},{"cell_type":"code","metadata":{"id":"zK-pu5CtI5F-"},"source":["p = ['  ', '', '    ','  ','  ','    ','     ','     ','      ','        ']\n","i=0\n","for k, v in data.items():\n","    if type(v) == np.ndarray:\n","        print(k+p[i], type(v), v.shape, '\\t', v.dtype)\n","    else:\n","        print(k+p[i], type(v), '\\t', len(v))\n","    i += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"galG568toguH"},"source":["---\n","## 4 - Look at the data\n","It is always a good idea to look at examples from the dataset before working with it.\n","\n","You can use the `sample_coco_minibatch()` function below to sample minibatches of data from the data structure returned from `load_coco_data()`. "]},{"cell_type":"code","metadata":{"id":"-oNya34eoguL"},"source":["def sample_coco_minibatch(data, batch_size=100, split='train'):\n","    split_size = data['%s_captions' % split].shape[0]\n","    mask       = np.random.choice(split_size, batch_size)\n","\n","    captions       = data['%s_captions' % split][mask]\n","    image_idxs     = data['%s_image_idxs' % split][mask]\n","    image_features = data['%s_features' % split][image_idxs]\n","    \n","    urls = data['%s_urls' % split][image_idxs]\n","    \n","    return captions, image_features, urls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yt_v-pT4oguP"},"source":["Run the following to sample a small minibatch of training data and show the images and their captions. \n","\n","Running it multiple times and looking at the results helps you to get a sense of the dataset.\n","\n","Note that we decode the captions using the `decode_captions` function and that we download the images on-the-fly using their Flickr URL"]},{"cell_type":"code","metadata":{"id":"_OUo-_8noguQ"},"source":["# Sample a minibatch and show the images and captions\n","batch_size = 3\n","\n","captions, features, urls = sample_coco_minibatch(data, batch_size=batch_size)\n","for i, (caption, url) in enumerate(zip(captions, urls)):\n","    caption_str = decode_captions(caption, data['idx_to_word'])\n","\n","    fig = plt.figure(figsize=(8,6))\n","    fig.suptitle(caption_str, x=0, y=0.96, ha='left', size=15)\n","\n","    plt.imshow(image_from_url(url))\n","    plt.axis('off')\n","    plt.show()\n","    \n","    print('\\n-----------------------------------------------------\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XFT0HzEerE0o"},"source":["Run the cell again to view another set of images\n","\n","<br>\n","\n","<font color='red' size=4> NOTE: </font>\n","\n","You might see that some of the images are already **removed** or **lost** from the Internet. So they cannot be downloaded.\n","\n","But fear not since we have already retrieved all the feature for this training"]},{"cell_type":"markdown","metadata":{"id":"HC7wD-JNoguV"},"source":["---\n","---\n","# [Part 2] Recurrent Neural Networks\n","As discussed in lecture, we will use Vanilla Recurrent Neural Network (RNN) language models for image captioning. \n","\n","<center> <img src='http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png' width=600></center>\n","\n","You should implement the basic layer types that are needed for Recurrent Neural Network \n","\n"]},{"cell_type":"markdown","metadata":{"id":"v_doiba_oguW"},"source":["---\n","## 1 - Single Forward Step\n","\n","\n","First implement the `rnn_step_forward()` function which implements the forward pass for a **single timestep** of a vanilla recurrent neural network that uses a tanh\n","    activation function. \n","\n","Store the next hidden state ($h_t$) and any values you need for the backward pass in the `next_h` and `cache` variables respectively.         \n","<br>\n","\n","$$\n","\\begin{align*}\n","xh &= x_t\\cdot W_{x}\\\\\\\\\n","hh &= h_{t-1}\\cdot W_{h}\\\\\\\\\n","h_t &= \\tanh(hh + xh + b)\\\\\n","\\end{align*}\n","$$  \n","\n","<br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xc2RyhhXUYMU"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","implement the function as described above"]},{"cell_type":"code","metadata":{"id":"GZZoqj7loguX"},"source":["def rnn_step_forward(x, prev_h, Wx, Wh, b):\n","\n","    # calculate dot product from x and Wx\n","    xh = ??\n","    \n","    # calculate dot product from prev_h and Wh\n","    hh = ??\n","    \n","    # calculate the output activation for next_h using np.tahn()\n","    next_h = ??\n","\n","    # store intermediate value for backward pass\n","    cache = (x, prev_h, Wx, Wh, b, next_h)\n","    \n","    return next_h, cache"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KHC4xjFWogua"},"source":["\n","After doing so run the following to check your implementation. \n","You should see errors on the order of `e-8` or less."]},{"cell_type":"code","metadata":{"id":"fjEk_2L8oguc"},"source":["N, D, H = 3, 10, 4\n","\n","x      = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n","prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n","Wx     = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n","Wh     = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n","b      = np.linspace(-0.2, 0.4, num=H)\n","\n","next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n","\n","expected_next_h = np.asarray([\n","  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n","  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n","  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n","\n","print('next_h error: ', rel_error(expected_next_h, next_h))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Nzkhj9Roguf"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","    next_h error:  6.292421426471037e-09"]},{"cell_type":"markdown","metadata":{"id":"lQScqynTogug"},"source":["---\n","## 2 - Single Backward Step\n","\n","Next, implement the backward pass for a single step of a vanilla RNN in `rnn_step_backward()` function below.\n","\n","For the tanh function, you can compute the local derivative in terms of the output value from tanh. \n","\n","<br>\n","\n","$$ \\partial\\tanh(x) = 1-\\tanh^2(x) $$\n","\n","<br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oBET8ki7c26O"},"source":["---\n","with \n","<table>\n","  <tr>\n","    <td><font size=3>$\\partial h_t$</font></td>\n","    <td>:</td>\n","    <td><font size=3><pre>dh</pre></font></td>\n","    <td>|<br>|<br>|<br>|</td>\n","    <td><font size=3>$h_{t+1}$</font></td>\n","    <td>:</td>\n","    <td><font size=3><pre>next_h</pre></font></td>\n","    <td>|<br>|<br>|<br>|</td>\n","    <td><font size=3>$\\partial h_{t+1}$</font></td>\n","    <td>:</td>\n","    <td><font size=3><pre>dnext_h</pre></font></td> \n","    <td>|<br>|<br>|<br>|</td>\n","    <td><font size=3>$\\partial h_{t-1}$</font></td>\n","    <td>:</td>\n","    <td><font size=3><pre>dprev_h</pre></font></td>\n","    <td>|<br>|<br>|<br>|</td>\n","    <td><font size=3>$h_{t-1}$</font></td>\n","    <td>:</td>\n","    <td><font size=3><pre>prev_h</pre></font></td>\n","  </tr>\n","</table>\n","\n","<br>\n","\n","The complete derivative of bacward RNN step is as below:\n","\n","<br>\n","\n","$$\n","\\begin{align*}\n","\\partial h_t &= (1-(h_{t+1})^2) * \\partial h_{t+1} &|& & \\partial W_x &= x^T\\cdot \\partial h_t\\\\\\\\\n","\\partial x &= \\partial h_t \\cdot (W_x)^T &|& & \\partial W_h &= (h_{t-1})^T\\cdot \\partial h_t\\\\\\\\\n","\\partial h_{t-1} &= \\partial h_t \\cdot (W_h)^T &|& & \\partial b &= \\sum(\\partial h_t)\n","\\end{align*}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"AJ94khS-OsJy"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","implement the function as described above"]},{"cell_type":"code","metadata":{"id":"dkCNkf_Kogui"},"source":["def rnn_step_backward(dnext_h, cache):\n","\n","    # extract intermediate cache\n","    x, prev_h, Wx, Wh, b, next_h = cache\n","\n","    # calculate local gradient of hiddent state, \n","    # which is local gradient of tanh (1-next_h^2)\n","    # multiplied by the upstream gradient dnext_h,\n","    # as stated above,\n","    dh = ??\n","    \n","    # calculate gradient of x,\n","    # which is a dot product of dh with transpose of Wx\n","    dx = ??\n","    \n","    # calculate gradient of previous hidden state, \n","    # which is a dot product of dh with transpose of Wh\n","    dprev_h = ??\n","    \n","    # calculate gradient of Wx, \n","    # which is a dot product of x transpose with dh\n","    dWx = ??\n","    \n","    # calculate gradient of Wh, \n","    # which is a dot product of transposed previous hidden state with dh\n","    dWh = ??\n","    \n","    # calculate gradient of bias, \n","    # which is a sum of dh over axis=0\n","    db = ??\n","\n","    return dx, dprev_h, dWx, dWh, db"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q50kbaqxogul"},"source":["After doing so run the following to numerically gradient check your implementation. \n","\n","You should see errors on the order of `e-8` or less."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"7KBkscx_ogum"},"source":["np.random.seed(231)\n","N, D, H = 4, 5, 6\n","\n","x  = np.random.randn(N, D)\n","h  = np.random.randn(N, H)\n","Wx = np.random.randn(D, H)\n","Wh = np.random.randn(H, H)\n","b  = np.random.randn(H)\n","\n","out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uSttUhvPPOPH"},"source":["dnext_h = np.random.randn(*out.shape)\n","\n","fx  = lambda x:  rnn_step_forward(x, h, Wx, Wh, b)[0]\n","fh  = lambda prev_h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","fb  = lambda b:  rnn_step_forward(x, h, Wx, Wh, b)[0]\n","\n","dx_num      = eval_numerical_gradient_array(fx,  x,  dnext_h)\n","dprev_h_num = eval_numerical_gradient_array(fh,  h,  dnext_h)\n","dWx_num     = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n","dWh_num     = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n","db_num      = eval_numerical_gradient_array(fb,  b,  dnext_h)\n","\n","dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n","\n","print('dx error      : ', rel_error(dx_num, dx))\n","print('dprev_h error : ', rel_error(dprev_h_num, dprev_h))\n","print('dWx error     : ', rel_error(dWx_num, dWx))\n","print('dWh error     : ', rel_error(dWh_num, dWh))\n","print('db error      : ', rel_error(db_num, db))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q4Gn5nCIogup"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>\n","    dx error      :  4.0192769090159184e-10\n","    dprev_h error :  2.5632975303201374e-10\n","    dWx error     :  8.820222259148609e-10\n","    dWh error     :  4.703287554560559e-10\n","    db error      :  7.30162216654e-11"]},{"cell_type":"markdown","metadata":{"id":"BuaL5BOrogur"},"source":["---\n","## 3 - Forward Pass\n","Now that you have implemented the forward and backward passes for a single timestep of a vanilla RNN, you will combine these pieces to implement a RNN that processes an entire sequence of data.\n","\n","\n","* We assume an input sequence composed of $T$ vectors, each of dimension $D$.\n","\n","* The RNN uses a hidden size of $H$, and we work over a minibatch containing $N$ sequences.\n","\n","After running the RNN forward, we return the hidden states for all timesteps.\n"]},{"cell_type":"markdown","metadata":{"id":"Uq4UjLXB-Azw"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","Implement forward pass for a vanilla RNN running on a sequence of input data. \n"]},{"cell_type":"code","metadata":{"id":"a7mF9yArogus"},"source":["def rnn_forward(x, h0, Wx, Wh, b):\n","\n","    # get the size of N, T, H, and D\n","    N, T, D = x.shape\n","    N, H    = h0.shape\n","    \n","    # create a zeros matrix in shape of (N, T, H)\n","    # to store the hidden activation\n","    h = ??\n","    \n","    # set h0 as the previous hidden activation \n","    prev_h = h0\n","    \n","    #initialize cache dictionary\n","    cache = {}\n","\n","    # start RNN forward \n","    # loop over the timestep\n","    for i in range(T):\n","        \n","        # select x at timestep i \n","        xt = x[:, i, :]\n","        \n","        # call rnn_step_forward() function with input xt, prev_h, \n","        # and its weights Wx, Wh, and b\n","        current_h, cache_i = ??\n","        \n","        # store the current hidden activation to the i-th timestep in matrix h\n","        h[:, i, :] = current_h        \n","        \n","        # store the cache_i result to cache dictionary\n","        cache[i] = cache_i\n","        \n","        # for the next iteration, set current hidden activation as previous activation\n","        prev_h = current_h\n","\n","    return h, cache"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KKa06Umvoguu"},"source":["After doing so run the following to check your implementation. You should see errors on the order of `e-7` or less."]},{"cell_type":"code","metadata":{"id":"XVtXJWTWoguw"},"source":["N, T, D, H = 2, 3, 4, 5\n","\n","x  = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n","h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n","Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n","Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n","b  = np.linspace(-0.7, 0.1, num=H)\n","\n","h, _ = rnn_forward(x, h0, Wx, Wh, b)\n","\n","\n","expected_h = np.asarray([\n","  [[-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n","    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n","    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525]],\n","  [[-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n","    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n","    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]\n","  ])\n","\n","print('h error: ', rel_error(expected_h, h))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wb4HFQbYogu0"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","    h error:  7.728466151011529e-08"]},{"cell_type":"markdown","metadata":{"id":"WU9LuNVQogu0"},"source":["---\n","## 4 - Backward Pass\n","\n","Implement the backward pass for a vanilla RNN running an entire sequence of data in the function `rnn_backward`. \n","\n","This should run back-propagation over the entire sequence, making calls to the `rnn_step_backward` function that you defined earlier. \n","\n","<br>\n","\n","\n","**NOTE** that `dh` contains the upstream gradients produced by the individual loss functions at each timestep,\n","and ***NOT*** the gradients being passed between timesteps <br>(which you'll have to compute yourself by calling `rnn_step_backward` in a loop).\n"]},{"cell_type":"markdown","metadata":{"id":"cddjH8jd_GOf"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","Implement backward pass for a vanilla RNN running on a sequence of input data. \n"]},{"cell_type":"code","metadata":{"id":"MhJovD8mogu1"},"source":["def rnn_backward(dh, cache):\n","    \n","    # get the x data and D dimension\n","    x = cache[0][0]\n","    D = x.shape[1]\n","    \n","    # get the size of N, T, and H\n","    N, T, H = dh.shape\n","    \n","    # initialize zeros matrix with the appropriate size\n","    dx  = ?? # shaped (N, T, D)\n","    dh0 = ?? # shaped (N, H)\n","    dWx = ?? # shaped (D, H)\n","    dWh = ?? # shaped (H, H)\n","    db  = ?? # shaped (H)\n","\n","    # initialzie zeros matrix for the current gradient h for the loop\n","    # same size as dh0\n","    dprev_h = ??\n","\n","    # backward loop over timestep\n","    for i in reversed(range(T)):\n","        \n","        # select dh at timestep i\n","        dht = dh[:, i, :]\n","        \n","        # gradient of next_h is the sum of dht and dprev_h\n","        dnext_h = ??\n","        \n","        # get i-th cache from cache dictionary\n","        cache_i = cache[i]\n","\n","        # call rnn_step_backward() with input dnext_h and cache_i\n","        dx[:, i, :], dprev_h, dWx_t, dWh_t, db_t = ??\n","\n","        # accumulate the Wx gradient over loop into dWx\n","        dWx = dWx + dWx_t\n","        \n","        # accumulate the Wh gradient over loop into dWh\n","        dWh = ??\n","        \n","        # accumulate the bias gradient over loop into db\n","        db = ??\n","\n","    # store the last gradient of hidden state as dh0\n","    dh0 = dprev_h\n","\n","    return dx, dh0, dWx, dWh, db"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rTBY6mFQogu4"},"source":["Test your implementation. You should see errors on the order of `e-6` or less."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"VGP4W9CYogu4"},"source":["np.random.seed(231)\n","\n","N, D, T, H = 2, 3, 10, 5\n","\n","x  = np.random.randn(N, T, D)\n","h0 = np.random.randn(N, H)\n","Wx = np.random.randn(D, H)\n","Wh = np.random.randn(H, H)\n","b  = np.random.randn(H)\n","\n","out, cache = rnn_forward(x, h0, Wx, Wh, b)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qEHbhwII_3SV"},"source":["dout = np.random.randn(*out.shape)\n","\n","dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n","\n","fx  = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n","fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n","fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n","fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n","fb  = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n","\n","dx_num  = eval_numerical_gradient_array(fx, x, dout)\n","dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n","dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n","dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n","db_num  = eval_numerical_gradient_array(fb, b, dout)\n","\n","print('dx error  : ', rel_error(dx_num, dx))\n","print('dh0 error : ', rel_error(dh0_num, dh0))\n","print('dWx error : ', rel_error(dWx_num, dWx))\n","print('dWh error : ', rel_error(dWh_num, dWh))\n","print('db error  : ', rel_error(db_num, db))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cxWKI0gUogu7"},"source":["\n","**EXPECTED OUTPUT**:\n","\n","<pre>\n","    dx error  :  1.5382468491701097e-09\n","    dh0 error :  3.3839681556240896e-09\n","    dWx error :  7.150535245339328e-09\n","    dWh error :  1.297338408201546e-07\n","    db error  :  1.4889022954777414e-10\n"]},{"cell_type":"markdown","metadata":{"id":"oc3yEiaiogvN"},"source":["---\n","---\n","# [Part 3] Temporal Affine Layer\n","At every timestep we use an affine function to transform the RNN hidden vector at that timestep into scores for each word in the vocabulary. \n","\n","This is very similar to the affine layer that you implemented in previous assignmens, so complete the `temporal_affine_forward` and `temporal_affine_backward` functions below. "]},{"cell_type":"markdown","metadata":{"id":"WK3ZWE_8KkwT"},"source":["---\n","## 1 - Forward Pass\n","Forward pass for a temporal affine layer. \n","\n","The input is a set of $D$-dimensional     vectors arranged into a minibatch of $N$ timeseries, each of length $T$. \n","\n","We use     an affine function to transform each of those vectors into a new vector of     dimension $M$."]},{"cell_type":"markdown","metadata":{"id":"SYVylsZuLn3h"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","implement the function as described above"]},{"cell_type":"code","metadata":{"id":"lWiUZY4rogvP"},"source":["def temporal_affine_forward(x, w, b):\n","\n","    # get the size of N, T, and D\n","    N, T, D = x.shape\n","\n","    # get the size of M\n","    M = b.shape[0]\n","\n","    # reshape x into shape (N*T, D)\n","    x_reshape = ??\n","\n","    # output = dot product x_reshape with w\n","    out = ??\n","    \n","    # reshape output into shape (N, T, M)\n","    out = ??\n","    \n","    # add bias b to output\n","    out = ??\n","\n","    # store cache for backward pass\n","    cache = x, w, b, out\n","\n","    return out, cache"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mDCyXfrrLo3Z"},"source":["---\n","## 2 - Backward Pass Pass\n","\n","Backward pass for a temporal affine layer. \n","\n","Again, it's very much alike normal affine layer, but with temporal input shape\n","\n","Thus you need to reshape the matrix to the appropriate shape before calculating dot product\n"]},{"cell_type":"markdown","metadata":{"id":"QWBTZodiLoMZ"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","implement the function as described above"]},{"cell_type":"code","metadata":{"id":"uaxrzpTsogvQ"},"source":["def temporal_affine_backward(dout, cache):\n","  \n","    # extract cache\n","    x, w, b, out = cache\n","\n","    # get the size of N, T, and D\n","    N, T, D = x.shape\n","\n","    # get the size of M\n","    M = b.shape[0]\n","\n","    # reshape dout into shape (N*T, M)\n","    dout_reshape = ??\n","\n","    # reshape x into shape (N*T, D)\n","    x_reshape = ??\n","\n","    # dx = dot product dout_reshape with w.transpose\n","    dx = ??\n","    \n","    # reshape dx back into shape (N, T, D)\n","    dx = ??\n","    \n","    # dw = dot product dout_reshape.transpose with x_reshape\n","    dw = ??\n","\n","    # transpose back dw\n","    dw = dw.T\n","\n","    # sum dout over axis=(0,1)\n","    db = dout.sum(axis=(0, 1))\n","\n","    return dx, dw, db"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zRqh1pvKogvT"},"source":["Run the following to perform numeric gradient checking on the implementation. You should see errors on the order of `e-9` or less."]},{"cell_type":"code","metadata":{"id":"jS9m_xlaogvU"},"source":["np.random.seed(231)\n","\n","# Gradient check for temporal affine layer\n","N, T, D, M = 2, 3, 4, 5\n","x = np.random.randn(N, T, D)\n","w = np.random.randn(D, M)\n","b = np.random.randn(M)\n","\n","out, cache = temporal_affine_forward(x, w, b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XcNUq-S3ND0J"},"source":["dout = np.random.randn(*out.shape)\n","\n","fx = lambda x: temporal_affine_forward(x, w, b)[0]\n","fw = lambda w: temporal_affine_forward(x, w, b)[0]\n","fb = lambda b: temporal_affine_forward(x, w, b)[0]\n","\n","dx_num = eval_numerical_gradient_array(fx, x, dout)\n","dw_num = eval_numerical_gradient_array(fw, w, dout)\n","db_num = eval_numerical_gradient_array(fb, b, dout)\n","\n","dx, dw, db = temporal_affine_backward(dout, cache)\n","\n","print('dx error: ', rel_error(dx_num, dx))\n","print('dw error: ', rel_error(dw_num, dw))\n","print('db error: ', rel_error(db_num, db))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZMwYRQu1ogvd"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","    dx error:  2.9215945034030545e-10\n","    dw error:  1.5772088618663602e-10\n","    db error:  3.252200556967514e-11"]},{"cell_type":"markdown","metadata":{"id":"dS4RiHoPogvd"},"source":["---\n","## 3 - Temporal Softmax loss\n","In an RNN language model, at every timestep we produce a score for each word in the vocabulary. We know the ground-truth word at each timestep, so we use a softmax loss function to compute loss and gradient at each timestep. We sum the losses over time and average them over the minibatch.\n","\n","However there is one wrinkle: since we operate over minibatches and different captions may have different lengths, we append `<NULL>` tokens to the end of each caption so they all have the same length. We don't want these `<NULL>` tokens to count toward the loss or gradient, so in addition to scores and ground-truth labels our loss function also accepts a `mask` array that tells it which elements of the scores count towards the loss.\n","\n","Since this is very similar to the softmax loss function you implemented in previous assignment, we have implemented this loss function for you; look at the `temporal_softmax_loss` function below."]},{"cell_type":"markdown","metadata":{"id":"rPh2-38nNtih"},"source":["We assume that we are     making predictions over a vocabulary of size $V$ for each timestep of a    timeseries of length $T$, over a minibatch of size $N$. \n","\n","The input $x$ gives scores    for all vocabulary elements at all timesteps, and $y$ gives the indices of the    ground-truth element at each timestep. \n","\n","We use a cross-entropy loss at each    timestep, summing the loss over all timesteps and averaging across the    minibatch.\n","\n","As an additional complication, we may want to ignore the model output at some\n","timesteps, since sequences of different length may have been combined into a\n","minibatch and padded with NULL tokens. The optional mask argument tells us\n","which elements should contribute to the loss.\n"]},{"cell_type":"code","metadata":{"id":"maChXoh1ogve"},"source":["def temporal_softmax_loss(x, y, mask, verbose=False):\n","\n","    N, T, V = x.shape\n","\n","    x_flat = x.reshape(N * T, V)\n","    y_flat = y.reshape(N * T)\n","    \n","    mask_flat = mask.reshape(N * T)\n","\n","    probs  = np.exp(x_flat - np.max(x_flat, axis=1, keepdims=True))\n","    probs /= np.sum(probs, axis=1, keepdims=True)\n","    loss   = -np.sum(mask_flat * np.log(probs[np.arange(N * T), y_flat])) / N\n","\n","    dx_flat = probs.copy()\n","    dx_flat[np.arange(N * T), y_flat] -= 1\n","    dx_flat /= N\n","    dx_flat *= mask_flat[:, None]\n","\n","    if verbose: print('dx_flat: ', dx_flat.shape)\n","\n","    dx = dx_flat.reshape(N, T, V)\n","\n","    return loss, dx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"utzF6dj-ogvg"},"source":["Run the following cell to sanity check the loss and perform numeric gradient checking on the function. You should see an error for dx on the order of `e-7` or less."]},{"cell_type":"code","metadata":{"id":"Q-j7iPk5ogvg"},"source":["N, T, V = 100, 1, 10\n","\n","def check_loss(N, T, V, p):\n","    x = 0.001 * np.random.randn(N, T, V)\n","    y = np.random.randint(V, size=(N, T))\n","    mask = np.random.rand(N, T) <= p\n","    print(temporal_softmax_loss(x, y, mask)[0])\n","\n","check_loss(100, 1, 10, 1.0)   # Should be about 2.3\n","check_loss(100, 10, 10, 1.0)  # Should be about 23\n","check_loss(5000, 10, 10, 0.1) # Should be about 2.3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8wqQ0bdfogvj"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","    2.3027781774290146  (Should be about 2.3)\n","    23.025985953127226  (Should be about 23) \n","    2.2643611790293394  (Should be about 2.3)"]},{"cell_type":"markdown","metadata":{"id":"lZka-7oyOWlN"},"source":["Gradient check for temporal softmax loss"]},{"cell_type":"code","metadata":{"id":"dGxXN66Jogvl"},"source":["N, T, V = 7, 8, 9\n","\n","x = np.random.randn(N, T, V)\n","y = np.random.randint(V, size=(N, T))\n","mask = (np.random.rand(N, T) > 0.5)\n","\n","loss, dx = temporal_softmax_loss(x, y, mask, verbose=False)\n","\n","dx_num = eval_numerical_gradient(lambda x: temporal_softmax_loss(x, y, mask)[0], x, verbose=False)\n","\n","print('dx error: ', rel_error(dx, dx_num))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kWK0Vtnpogvn"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","    dx error:  2.583585303524283e-08\n"]},{"cell_type":"markdown","metadata":{"id":"-HRsmhbdOoSZ"},"source":["---\n","---\n","# [Part 4] Helper Functions\n","\n","To further implement the image captioning process, we need several other funtions, such as Affine Layer functions and Optimizer functions. Which you have already made before.\n","\n","Thus for the rest of the function, we've provided you with the implementation\n","\n","But let's walk through each of them"]},{"cell_type":"markdown","metadata":{"id":"-uvZH8hcogu8"},"source":["---\n","## 1 - Word Embedding\n","In deep learning systems, we commonly represent words using vectors. \n","\n","Each word of the vocabulary will be associated with a vector, and these vectors will be learned jointly with the rest of the system.\n","\n","Below we've provided you with the simple implementation of Forward and Backward pass of Word Embedding"]},{"cell_type":"markdown","metadata":{"id":"5svJQT60ogu9"},"source":["---\n","### a. Forward Pass\n","\n","Function `word_embedding_forward` below is the implementation of the forward pass for word embeddings. \n","\n","The function will convert words (represented by integers) into vectors. \n","\n","* operate on minibatches of size $N$ where    each sequence has length $T$.\n","\n","* assume a vocabulary of $V$ words, assigning each    word to a vector of dimension $D$.\n"]},{"cell_type":"code","metadata":{"id":"1-nBgQBjogu-"},"source":["def word_embedding_forward(x, W):\n","    \n","    out   = W[x, :]\n","    cache = x, W\n","\n","    return out, cache"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4AJEZ06IogvH"},"source":["---\n","### b. Backward Pass\n","Function `word_embedding_backward` below is the implementation of the backward pass for the word embedding.\n","\n","We cannot back-propagate into the words     since they are integers, so we only return gradient for the word embedding     matrix."]},{"cell_type":"code","metadata":{"id":"qRn7q9RbogvI"},"source":["def word_embedding_backward(dout, cache):\n","\n","    x, W = cache\n","    dW   = np.zeros_like(W)\n","    np.add.at(dW, x, dout)\n","\n","    return dW"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wE7GDDaEogvo"},"source":["---\n","## 2 - Affine Functions\n","\n","Affine implementation as you have implemented in previous assignments"]},{"cell_type":"markdown","metadata":{"id":"ymDAvPRoPqTm"},"source":["---\n","### a. Forward Pass\n","\n","Computes the forward pass for an affine (fully-connected) layer.\n","\n","The input $x$ has shape $(N, d_1, ..., d_k)$ where $x[i]$ is the $i^{th}$ input.\n","\n","We multiply this against a weight matrix of shape $(D, M)$ "]},{"cell_type":"code","metadata":{"id":"d1aW5oMTogvp"},"source":["def affine_forward(x, w, b):\n","  \n","    out = x.reshape(x.shape[0], -1).dot(w) + b\n","    cache = (x, w, b)\n","\n","    return out, cache"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QwCGxm1QQM0b"},"source":["---\n","### b. Backward Pass\n","\n","Computes the backward pass for an affine layer."]},{"cell_type":"code","metadata":{"id":"GqwAalTFogvr"},"source":["def affine_backward(dout, cache):\n","\n","    x, w, b = cache\n","\n","    dx = dout.dot(w.T).reshape(x.shape)\n","    dw = x.reshape(x.shape[0], -1).T.dot(dout)\n","    db = np.sum(dout, axis=0)\n","    \n","    return dx, dw, db"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"__qeInmcogvt"},"source":["---\n","## 3 - Optimizer Functions\n","\n","Here we also provide you with `SGD` and `Adam` optimization function"]},{"cell_type":"markdown","metadata":{"id":"tx2xQ7xoQawH"},"source":["---\n","### a. SGD Optimizer\n"," Performs vanilla stochastic gradient descent.\n"]},{"cell_type":"code","metadata":{"id":"l3CFV_WAogvv"},"source":["def sgd(w, dw, config=None):\n","\n","    if config is None: config = {}\n","    config.setdefault('learning_rate', 1e-2)\n","\n","    w -= config['learning_rate'] * dw\n","\n","    return w, config"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6UOYmOgAQhhj"},"source":["---\n","### b. Adam Optimizer\n","Uses the Adam update rule, which incorporates moving averages of both the     gradient and its square and a bias correction term.\n"]},{"cell_type":"code","metadata":{"id":"m-i8sXXyogvy"},"source":["def adam(x, dx, config=None):\n","\n","    if config is None: config = {}\n","    config.setdefault('learning_rate', 1e-3)\n","    config.setdefault('beta1', 0.9)\n","    config.setdefault('beta2', 0.999)\n","    config.setdefault('epsilon', 1e-8)\n","    config.setdefault('m', np.zeros_like(x))\n","    config.setdefault('v', np.zeros_like(x))\n","    config.setdefault('t', 0)\n","\n","    next_x = None\n","    beta1, beta2, eps = config['beta1'], config['beta2'], config['epsilon']\n","    t, m, v = config['t'], config['m'], config['v']\n","\n","    m = beta1 * m + (1 - beta1) * dx\n","    v = beta2 * v + (1 - beta2) * (dx * dx)\n","    t += 1\n","\n","    alpha = config['learning_rate'] * np.sqrt(1 - beta2 ** t) / (1 - beta1 ** t)\n","\n","    x -= alpha * (m / (np.sqrt(v) + eps))\n","\n","    config['t'] = t\n","    config['m'] = m\n","    config['v'] = v\n","\n","    next_x = x\n","\n","    return next_x, config"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RirFRqwXogv2"},"source":["---\n","---\n","# [Part 5] RNN for image captioning\n","Now that you have implemented the necessary layers, you can combine them to build an image captioning model. \n","\n","\n","Implement the forward and backward pass of the model in the `loss` function. \n","\n","For now you only need to implement the case where `cell_type='rnn'` for vanialla RNNs; you will implement the LSTM case later. \n","\n","After doing so, run the following to check your forward pass using a small test case; you should see error on the order of `e-10` or less."]},{"cell_type":"markdown","metadata":{"id":"xljDKDuJogv2"},"source":["---\n","## 1 - Weights Initialization Function\n","\n","Below is the function we've provided to initialize weights of the RNN"]},{"cell_type":"code","metadata":{"id":"HGAmlOAIogv2"},"source":["def init_weights(word_to_idx, input_dim=512, wordvec_dim=128, hidden_dim=128, dtype=np.float32):\n","    \n","    vocab_size = len(word_to_idx)\n","    idx_to_word = {i: w for w, i in word_to_idx.items()}\n","    weights = {}\n","\n","    # Initialize word vectors\n","    # Word embedding matrix\n","    weights['W_embed'] = np.random.randn(vocab_size, wordvec_dim)\n","    weights['W_embed'] /= 100\n","\n","    # Initialize CNN -> hidden state projection parameters    \n","    # Weight and bias for the affine transform from image features to initial\n","    # hidden state\n","    weights['W_proj'] = np.random.randn(input_dim, hidden_dim)\n","    weights['W_proj'] /= np.sqrt(input_dim)\n","    weights['b_proj'] = np.zeros(hidden_dim)\n","\n","    # Initialize parameters for the RNN\n","    # Input-to-hidden, hidden-to-hidden, and biases for the RNN\n","    # dim_mul = {'lstm': 4, 'rnn': 1}\n","    dim_mul = 1\n","    weights['Wx'] = np.random.randn(wordvec_dim, dim_mul * hidden_dim)\n","    weights['Wx'] /= np.sqrt(wordvec_dim)\n","    weights['Wh'] = np.random.randn(hidden_dim, dim_mul * hidden_dim)\n","    weights['Wh'] /= np.sqrt(hidden_dim)\n","    weights['b'] = np.zeros(dim_mul * hidden_dim)\n","\n","    # Initialize output to vocab weights\n","    # Weight and bias for the hidden-to-vocab transformation.\n","    weights['W_vocab'] = np.random.randn(hidden_dim, vocab_size)\n","    weights['W_vocab'] /= np.sqrt(hidden_dim)\n","    weights['b_vocab'] = np.zeros(vocab_size)\n","\n","    # Cast parameters to correct dtype\n","    for k, v in weights.items():\n","        weights[k] = v.astype(dtype)\n","\n","    return weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zk1BSWhBogv4"},"source":["---\n","## 2 - Captioning Step\n","\n","\n","Implement the forward and backward passes for the RNN Model.\n","\n","In the forward pass you will need to do the following:                   \n","* Use an affine transformation to compute the initial hidden state from the image features. This should produce an array of shape (N, H)\n","\n","* Use a word embedding layer to transform the words in captions_in from indices to vectors, giving an array of shape (N, T, W).         \n","* Use a vanilla RNN to process the sequence of input word vectors and produce hidden state vectors for all timesteps, producing an array of shape (N, T, H).\n","* Use a (temporal) affine transformation to compute scores over the vocabulary at every timestep using the hidden states, giving an array of shape (N, T, V).                                            \n","* Use (temporal) softmax to compute loss using captions_out, ignoring the points where the output word is <NULL> using the mask defined.     \n","                                                                         \n","In the backward pass you will need to compute the gradient of the loss with respect to all model parameters.\n","* `grads[k]` should give the gradients for `weights[k]`."]},{"cell_type":"markdown","metadata":{"id":"iMJ5qtRYgHVa"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","implement the function as described above"]},{"cell_type":"code","metadata":{"id":"Gu1boCoxogv5"},"source":["def rnn_step(weights, features, captions, word_to_idx):\n","    \n","    # create special token\n","    token_null  = word_to_idx['<NULL>']\n","    token_start = word_to_idx.get('<START>', None)\n","    token_end   = word_to_idx.get('<END>', None)\n","\n","\n","    # Cut captions into two pieces: \n","    #   captions_in has everything but the last word and will be input to the RNN; \n","    #   captions_out has everything but the first word and this is what we will expect the RNN to generate. \n","\n","    # These are offset by one relative to each other because the RNN should produce word (t+1)\n","    # after receiving word t. \n","\n","    # The first element of captions_in will be the START token, \n","    # and the first element of captions_out will be the first word.\n","    captions_in  = captions[:, :-1]\n","    captions_out = captions[:, 1:]\n","\n","    # mask to calculate temporal softmax loss\n","    mask = (captions_out != token_null)\n","\n","    # Weight and bias for the affine transform from image features to initial\n","    # hidden state\n","    W_proj, b_proj = weights['W_proj'], weights['b_proj']\n","\n","    # Word embedding matrix\n","    W_embed = weights['W_embed']\n","\n","    # Input-to-hidden, hidden-to-hidden, and biases for the RNN\n","    Wx, Wh, b = weights['Wx'], weights['Wh'], weights['b']\n","\n","    # Weight and bias for the hidden-to-vocab transformation.\n","    W_vocab, b_vocab = weights['W_vocab'], weights['b_vocab']\n","    \n","    # initialize gradients dictionary\n","    grads = {}\n","    \n","    ############################################################################\n","    # TODO: Implement the forward and backward passes for the CaptioningRNN.   #\n","    ############################################################################\n","    \n","    # ------------------------------------------------\n","    # Forward Pass\n","    # ------------------------------------------------\n","    \n","    # (1) project the input image feature into hidden vector\n","    # call affine_forward() function with input features, with W_proj and b_proj as the projection weights\n","    hidden_init, cache_init = ??\n","\n","    # (2) convert the caption of image batch into word embedding\n","    # call word_embedding_forward() function with input caption_in and W_embed weight\n","    captions_in_init, cache_embed = ??\n","\n","    # (3) RNN Forward Pass\n","    # call rnn_forward() function with input captions_in_init and hidden_init\n","    # also pass the weights Wx, Wh, and bias b weights into the function\n","    hidden_rnn, cache_rnn = ??\n","    \n","    # (4) calculate temporal scores\n","    # call temporal_affine_forward() function with input hidden_rnn also W_vocab and b_vocab\n","    scores, cache_scores = ??\n","        \n","    # ------------------------------------------------\n","    # Calculate Temporal Softmax Loss\n","    # ------------------------------------------------\n","    # call temporal_softmax_loss() function with input scores, caption_out, and mask that was defined above\n","    loss, dscores = ??\n","\n","   \n","    # ------------------------------------------------\n","    # Backward Pass\n","    # ------------------------------------------------   \n","        \n","    # (4) backward pass temporal affine\n","    # call temporal_affine_backward() function with input dscores and its appropriate cache (see the forward pass)\n","    dhidden_rnn, grads['W_vocab'], grads['b_vocab'] = ??\n","\n","    # (3) RNN Backward Pass\n","    # call rnn_backward() function with input dhidden_rnn and its appropriate cache (see the forward pass)\n","    dcaptions_in_init, dhidden_init, grads['Wx'], grads['Wh'], grads['b'] = ??\n","\n","\n","    # (2) backward pass embedding\n","    # call word_embedding_backward() function with input dcaption_in_init and its appropriate cache (see the forward pass)\n","    grads['W_embed'] = ??\n","\n","    # (1) backward pass affine\n","    # call affine_backward() function with input dhidden_init and its appropriate cache (see the forward pass)\n","    dfeatures, grads['W_proj'], grads['b_proj'] = ??\n","    \n","    \n","    return loss, grads"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hAe3Kzo5ogv7"},"source":["After doing so, run the following to check your forward pass using a small test case; \n","\n","you should see error on the order of `e-10` or less."]},{"cell_type":"code","metadata":{"id":"pwwr1S65ogv8"},"source":["N, D, W, H = 10, 20, 30, 40\n","\n","word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n","V = len(word_to_idx)\n","T = 13\n","\n","model = init_weights(word_to_idx, input_dim=D, wordvec_dim=W, hidden_dim=H, dtype=np.float64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Hhlm_6vefAP"},"source":["# Set all model parameters to fixed values\n","for k, v in model.items():\n","    model[k] = np.linspace(-1.4, 1.3, num=v.size).reshape(*v.shape)\n","\n","features = np.linspace(-1.5, 0.3, num=(N * D)).reshape(N, D)\n","captions = (np.arange(N * T) % V).reshape(N, T)\n","\n","loss, grads   = rnn_step(model, features, captions, word_to_idx)\n","expected_loss = 9.83235591003\n","\n","print('loss          : ', loss)\n","print('expected loss : ', expected_loss)\n","print('difference    : ', abs(loss - expected_loss))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bFrTtTAEogv-"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","    loss          :  9.832355910027387\n","    expected loss :  9.83235591003\n","    difference    :  2.6130209107577684e-12"]},{"cell_type":"markdown","metadata":{"id":"0LvhcQE8ogv-"},"source":["Run the following cell to perform numeric gradient checking; you should see errors around the order of `e-6` or less."]},{"cell_type":"code","metadata":{"id":"cM0eBQsnogv-"},"source":["np.random.seed(231)\n","\n","batch_size  = 2\n","timesteps   = 3\n","input_dim   = 4\n","wordvec_dim = 5\n","hidden_dim  = 6\n","word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n","vocab_size  = len(word_to_idx)\n","\n","captions = np.random.randint(vocab_size, size=(batch_size, timesteps))\n","features = np.random.randn(batch_size, input_dim)\n","\n","model = init_weights(word_to_idx, input_dim=input_dim, wordvec_dim=wordvec_dim, hidden_dim=hidden_dim, dtype=np.float64)\n","\n","loss, grads = rnn_step(model, features, captions, word_to_idx)\n","\n","prettyfy = ['',' ', '', '     ', '     ', '      ', ' ', '']\n","i=0\n","for param_name in sorted(grads):\n","    f = lambda _: rnn_step(model, features, captions, word_to_idx)[0]\n","    param_grad_num = eval_numerical_gradient(f, model[param_name], verbose=False, h=1e-6)\n","    \n","    e = rel_error(param_grad_num, grads[param_name])\n","\n","    print('%s relative error  %s : %e' % (param_name, prettyfy[i], e))\n","    i+=1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5ApfTQNogwA"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","    W_embed relative error   : 2.331070e-09\n","    W_proj relative error    : 1.112417e-08\n","    W_vocab relative error   : 4.274379e-09\n","    Wh relative error        : 5.858117e-09\n","    Wx relative error        : 1.590657e-06\n","    b relative error         : 9.727211e-10\n","    b_proj relative error    : 1.934807e-08\n","    b_vocab relative error   : 7.087097e-11"]},{"cell_type":"markdown","metadata":{"id":"Uu1aYAbiogwB"},"source":["---\n","## 3 - Training Function\n","\n","Similar to the training function that we used to train image classification models on the previous assignment.\n","\n","Here we also provide you the training function for the RNN Captioning. \n","\n","Read through the `rnn_captioning_train()` function below; it should look very familiar.\n","\n","<br>\n","\n","On each epoch it will:\n","1. batch the training input \n","\n","2. call `rnn_step` function to perform forward and backward pass and calculate the gradient\n","3. perform weight update using the optimization function passed in `optimizer`"]},{"cell_type":"code","metadata":{"id":"zC92rlj8ogwC"},"source":["def rnn_captioning_train(weights, data, word_to_idx, \n","                         optimizer = sgd, optim_config={},\n","                         epochs=10, batch_size=100, lr_decay=1.0, \n","                         print_every=10, verbose=1):\n","\n","    best_val_acc = 0\n","    best_params  = {}\n","\n","    loss_history      = []\n","    train_acc_history = []\n","    val_acc_history   = []\n","    \n","    # Make a deep copy of the optim_config for each parameter\n","    optim_configs = {}\n","    for p in weights:\n","        d = {k: v for k, v in optim_config.items()}\n","        optim_configs[p] = d\n","\n","    num_train = data['train_captions'].shape[0]\n","\n","    # total iteration per epoch\n","    num_iter  = max(num_train // batch_size, 1)\n","    \n","    #start iteration counts\n","    it = 0\n","\n","    # Start Train\n","    for ep in range(epochs):    \n","\n","        # loop over batch iteration\n","        for i in range(num_iter):\n","        \n","            # ------------------------------------------------\n","            # 1. Sample Minibatch of Training Data\n","            # ------------------------------------------------\n","            captions, features, urls = sample_coco_minibatch(data, \n","                                                             batch_size=batch_size, \n","                                                             split='train')\n","\n","            # ------------------------------------------------\n","            # 2. Compute loss and gradient\n","            # ------------------------------------------------\n","            loss, grads = rnn_step(weights, features, captions, word_to_idx)\n","            loss_history.append(loss)\n","\n","\n","            # ------------------------------------------------\n","            # 3. Parameter Update\n","            # ------------------------------------------------   \n","            # Perform a parameter update\n","            for p, w in weights.items():\n","                dw     = grads[p]\n","                config = optim_configs[p]\n","\n","                next_w, next_config = optimizer(w, dw, config)\n","                weights[p]          = next_w\n","                optim_configs[p]    = next_config\n","            \n","            # iteration count\n","            it +=1\n","\n","            # Maybe print training loss\n","            if verbose==1 and it % print_every == 0:\n","                print ('iteration',it,'(epoch', ep,'/',epochs, '): loss =', loss_history[-1])\n","\n","        # At the end of one epoch:\n","        # Decay learning rate        \n","        for k in optim_configs:\n","            optim_configs[k]['learning_rate'] *= lr_decay\n","\n","\n","    print('Training Done')\n","    return loss_history, weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b-OrfRvpogwE"},"source":["---\n","## 4 - Overfit small data\n","\n","Once you have familiarized yourself with the function, run the following to make sure your model overfits a small sample of 100 training examples. You should see a final loss of less than 0.1.\n","\n","First, initialize the model"]},{"cell_type":"code","metadata":{"id":"cqmMFCYoogwE"},"source":["np.random.seed(231)\n","\n","small_data = load_coco_data(max_train=50)\n","\n","small_rnn_model = init_weights(\n","    word_to_idx = data['word_to_idx'], \n","    input_dim   = data['train_features'].shape[1],\n","    wordvec_dim = 256, \n","    hidden_dim  = 512,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WyUjZulXiIi-"},"source":["Overfit small data"]},{"cell_type":"code","metadata":{"id":"uMoXr9cCiFLn"},"source":["loss_history, small_rnn_model = rnn_captioning_train(\n","    small_rnn_model,\n","    data         = small_data, \n","    word_to_idx  = data['word_to_idx'], \n","    optimizer    = adam, \n","    optim_config = {'learning_rate': 5e-3,},\n","    epochs       = 100,\n","    batch_size   = 64,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-xEzEbZK82DU"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","    loss shoud start around 10 and end around 0.005"]},{"cell_type":"markdown","metadata":{"id":"C4VPpNA5iHX-"},"source":["Plot the training losses"]},{"cell_type":"code","metadata":{"id":"iquPyX3WiF4v"},"source":["plt.figure(figsize=(6,4))\n","plt.plot(loss_history)\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","plt.title('Training loss history')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ArddwQa5ogwI"},"source":["---\n","---\n","# [Part 6] RNN Captioning Testing \n","Unlike classification models, image captioning models behave very differently at training time and at test time. \n","\n","At training time, we have access to the ground-truth caption, so we feed ground-truth words as input to the RNN at each timestep.\n","\n","While at test time, we sample from the distribution over the vocabulary at each timestep, and feed the sample as input to the RNN at the next timestep.\n","\n","Therefore we need to implement slightly different forward pass"]},{"cell_type":"markdown","metadata":{"id":"jFrkw-FyogwI"},"source":["---\n","## 1 - Forward Testing Function\n","\n","Implement a test-time forward pass for the model, sampling captions for input feature vectors.\n","\n","At each timestep, we embed the current word, pass it and the previous hidden state to the RNN to get the next hidden state, use the hidden state to get scores for all vocab words, and choose the word with the highest score as the next word. The initial hidden state is computed by applying an affine transform to the input image features, and the initial word is the `<START>` token\n","\n","\n","You will need to initialize the hidden state of the RNN by applying the learned affine transform to the input image features. The first word that you feed to the RNN should be the `<START>` token; its value is stored in the variable `token_start`. \n","    "]},{"cell_type":"markdown","metadata":{"id":"YlzlxzXAji4y"},"source":["\n","First, you need to project the input feature. Then, at each timestep you will need to do to:  \n"," 1. Embed the previous word using the learned word embeddings \n","\n"," 2. Make an `RNN step` using the previous hidden state and the embedded current word to get the next hidden state.\n"," 3. Apply the learned affine transformation to the next hidden state to get scores for all words in the vocabulary                          \n"," 4. Select the word with the `highest score` as the next word, writing it (the word index) to the appropriate slot in the captions variable   \n","                                                                        \n","For simplicity, you do not need to stop generating after an `<END>` token is sampled, but you can if you want to.                                 \n","\n","NOTE: we are still working over minibatches in this function.      "]},{"cell_type":"markdown","metadata":{"id":"nWev3ZIqi3Ob"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Run a test-time forward pass for the model, sampling captions for input    feature vectors."]},{"cell_type":"code","metadata":{"id":"JfSXZQPCogwJ"},"source":["def rnn_captioning_test(weights, features, word_to_idx, max_length=30):\n","\n","    # create special token\n","    token_null  = word_to_idx['<NULL>']\n","    token_start = word_to_idx.get('<START>')\n","    token_end   = word_to_idx.get('<END>')\n","    \n","    N = features.shape[0]\n","    captions = token_null * np.ones((N, max_length), dtype=np.int32)\n","\n","    # Unpack parameters\n","    W_proj, b_proj   = weights['W_proj'], weights['b_proj']\n","    W_embed          = weights['W_embed']\n","    Wx, Wh, b        = weights['Wx'], weights['Wh'], weights['b']\n","    W_vocab, b_vocab = weights['W_vocab'], weights['b_vocab']\n","    \n","    # (0) project the input image feature into current hidden vector\n","    # call affine_forward() function with input features, W_proj, and b_proj as the projection weights\n","    current_h, _ = ??\n","\n","    # initialize array to accumulate the generated words\n","    words = np.zeros(N, dtype=int)\n","    words.fill(token_start)\n","\n","    # start generating caption\n","    for step in range(max_length):\n","        \n","        # (1) Embedding the output word for the next iteration\n","        #     call word_embedding_forward() function with input words and W_embed weight\n","        word_embed, _ = ??\n","        \n","        # (2) Make an RNN step using the previous hidden state and the embedded current word to get the next hidden state;\n","        #     call rnn_step_forward() function with input word_embed and current_h\n","        #     also pass weights Wx, Wh, and bias b weights into the function\n","        current_h, _ = ??\n","\n","        # (3) Apply the learned affine transformation to the next hidden state to get scores for all words in the vocabulary \n","        #     call affine_forward() function with input current_h, W_vocab and b_vocab\n","        scores, _ = ??\n","\n","        # (4) Select the word with the highest score as the next word\n","        #     use np.argmax() to scores with axis=1\n","        captions[:, step] = \n","\n","        # store the generated caption as input word for the next iteration\n","        words = captions[:, step]\n","        \n","    return captions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Ry4WejR296o"},"source":["---\n","## 2 - Image Captioning Testing Function\n","\n","Now define function to load several image and generate the caption"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"X02DNfWPogwN"},"source":["def generate_caption(model, data, split, batch_size):\n","    print('Generating Caption from ',split,'set images')\n","    print('-------------------------------------------')\n","    \n","    gt_captions, features, urls = sample_coco_minibatch(data, split=split, batch_size=batch_size)\n","    gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n","\n","    sample_captions = rnn_captioning_test(model, features, data['word_to_idx'])\n","    sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n","\n","    for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n","        fig = plt.figure(figsize=(8,6))\n","        plt.imshow(image_from_url(url))\n","        fig.suptitle('Caption Result : %s\\nGround Truth   : %s' % (sample_caption, gt_caption), x=0, y=0.96, ha='left', size=15)\n","        plt.axis('off')\n","        plt.show()\n","        print('\\n-----------------------------------------------------\\n')\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e-v49HktogwN"},"source":["---\n","## 3 - Test-time sampling\n","\n","After you finish the testing function implementations, run the following to sample from your overfitted model on both training and validation data. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"NQdihb7VT2nz"},"source":["---\n","### a. Test on Training Set\n","\n","The samples on training data should be very good; \n"]},{"cell_type":"code","metadata":{"id":"bkORBYxt3TFd"},"source":["generate_caption(model=small_rnn_model, data=small_data, split='train', batch_size=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a57l4AvI3XCB"},"source":["---\n","### b. Test on Validation  Set\n","Though, the samples on validation data probably won't make any sense. \n","<pre>\n","    ¯\\_(ツ)_/¯ </pre>"]},{"cell_type":"code","metadata":{"id":"WJKC19BY3S5s"},"source":["generate_caption(model=small_rnn_model, data=small_data, split='val', batch_size=3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7QVEFWxFogwP"},"source":["---\n","---\n","# [Part 7] Train your own model\n","\n","* You can continue the training on bigger set of data and test it \n","* Or you can change the model to a bigger model"]},{"cell_type":"markdown","metadata":{"id":"IKskq82nogwP"},"source":["---\n","## 1 - Get a bigger set of sample data\n","\n","load a bigger sample set from coco dataset\n"]},{"cell_type":"code","metadata":{"id":"1I5bKC_yogwQ"},"source":["bigger_data = load_coco_data(max_train=512)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CrwlCiWxogwS"},"source":["---\n","## 2 - Generate new and bigger Model\n","create a new model, (or just use the previous model)\n"]},{"cell_type":"code","metadata":{"id":"ag3_p0j7ogwT"},"source":["new_model = init_weights(\n","    word_to_idx = data['word_to_idx'], \n","    input_dim   = data['train_features'].shape[1],\n","    wordvec_dim = 256, \n","    hidden_dim  = 512,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XfzB4U6pogwU"},"source":["---\n","## 3 - Train your model\n","\n","you can generate new model\n","\n","you can run this part over-and-over to overfit the data"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"LbThTZxPogwW"},"source":["loss_history, new_model = rnn_captioning_train(\n","    new_model,\n","    data         = bigger_data, \n","    word_to_idx  = data['word_to_idx'], \n","    optimizer    = adam, \n","    optim_config = {'learning_rate': 5e-3,},\n","    epochs       = 100,\n","    batch_size   = 128,\n","    lr_decay     = 0.9,\n","    print_every  = 20,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gVWoPkvT9DSg"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","    loss shoud start around 37 and end around 1.7"]},{"cell_type":"markdown","metadata":{"id":"6squGVisl46i"},"source":["Plot the training losses"]},{"cell_type":"code","metadata":{"id":"S6Ib0JBMl3mP"},"source":["plt.figure(figsize=(6,4))\n","plt.plot(loss_history)\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","plt.title('Training loss history')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KEhrzSrHogwX"},"source":["---\n","## 4 - Test the trained model\n","\n","Now test it"]},{"cell_type":"markdown","metadata":{"id":"4PbFluDdUEi7"},"source":["---\n","### a. Test on Training Set\n","\n","The samples on training data should be very good; \n"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"Ryt0SAGmogwX"},"source":["generate_caption(model=new_model, data=bigger_data, split='train', batch_size=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q0ClIoNf33WU"},"source":["after several training step, you should see that the validation captioning is getting better"]},{"cell_type":"markdown","metadata":{"id":"GXaeOR0TUHNj"},"source":["---\n","### b. Test on Validation  Set\n","You should see that some of the captions are starting to get it right\n","\n"]},{"cell_type":"code","metadata":{"id":"Hsl53ZLy3prI"},"source":["generate_caption(model=new_model, data=bigger_data, split='val', batch_size=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xLSMxAh6xnEC"},"source":["\n","You can train it more to increase the performance"]},{"cell_type":"markdown","metadata":{"id":"aQqQ_gcO92ub"},"source":["\n","---\n","\n","# Congratulation, You've Completed Exercise 17\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2020 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"id":"7_Hkr9kg9Do-"},"source":["![footer](https://i.ibb.co/yX0jfMS/footer2020.png)"]}]}