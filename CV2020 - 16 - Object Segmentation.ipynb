{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CV2020 - 16 - Object Segmentation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"f_ChrQTZyAZU"},"source":["![title](https://i.ibb.co/f2W87Fg/logo2020.png)\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"MQjL2XtX7uKJ"},"source":["<table  class=\"tfo-notebook-buttons\" align=\"left\"><tr><td>\n","    \n","<a href=\"https://colab.research.google.com/github/adf-telkomuniv/CV2020_Exercises/blob/main/CV2020 - 16 - Object Segmentation.ipynb\" source=\"blank\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n","</td><td>\n","<a href=\"https://github.com/adf-telkomuniv/CV2020_Exercises/blob/main/CV2020 - 16 - Object Segmentation.ipynb\" source=\"blank\" ><img src=\"https://i.ibb.co/6NxqGSF/pinpng-com-github-logo-png-small.png\"></a>\n","    \n","</td></tr></table>"]},{"cell_type":"markdown","metadata":{"id":"8WGg98rBctSg"},"source":["\n","# Task 16 - Object Segmentation\n","\n","<br>\n","\n","In this exercise, we will build a simplified version of Image segmentation model using a modified <a href=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\" class=\"external\">U-Net</a>, that trained from scratch on the Oxford Pets dataset\n","\n","\n","## What is image segmentation?\n","So far you have tried image classification, where the task of the network is to assign a label or class to an input image, and a little of object detection, where the task is to assign class and determine the object's location. \n","\n","However, suppose now we want to know not only where an object is located in the image, but also the shape of that object, and which pixel belongs to which object. \n","\n","In this case what we need is to to segment the image, i.e., each pixel of the image is given a label. \n","\n","<center><img src=\"https://i.ibb.co/8XBwgS0/CV-Tasks.png\" width=\"80%\"></center>\n","\n","<br>\n","\n","The task of image segmentation is to train a neural network to output a pixel-wise mask of the image. This helps in understanding the image at a much lower level, i.e., the pixel level. Image segmentation has many applications in medical imaging, self-driving cars and satellite imaging to name a few.\n","\n","The dataset that will be used for this tutorial is the [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/), created by Parkhi *et al*. The dataset consists of images, their corresponding labels, and pixel-wise masks. The masks are basically labels for each pixel. Each pixel is given one of three categories :\n","\n","*   Class 1 : Pixel belonging to the pet.\n","*   Class 2 : Pixel bordering the pet.\n","*   Class 3 : None of the above/ Surrounding pixel.\n"]},{"cell_type":"markdown","metadata":{"id":"SF71bN55cZzi"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"S5yg44U8cZzk"},"source":["## --- start your code here ----\n","\n","NIM  = ??\n","Nama = ??\n","\n","## --- end your code here ----"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6VAE_sE-Bgk1"},"source":["---\n","---\n","#[Part 0] Import Libraries "]},{"cell_type":"markdown","metadata":{"id":"WvTLgxWsfaJm"},"source":["---\n","## 1 - Import Module\n","\n","import the necessary modules"]},{"cell_type":"code","metadata":{"id":"JC_hj76sMEnF"},"source":["import numpy as np\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.image import resize, flip_left_right\n","\n","from tensorflow.keras import Model, Input, backend\n","from tensorflow.keras.layers import Conv2D, SeparableConv2D, Conv2DTranspose\n","from tensorflow.keras.layers import MaxPool2D, UpSampling2D\n","from tensorflow.keras.layers import Activation, BatchNormalization, add\n","\n","from tensorflow.keras.preprocessing.image import array_to_img\n","from tensorflow.keras.utils import plot_model\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JaSWjfuNy05Z"},"source":["---\n","## 2 - Helper Functions\n","\n","Various helper functions for visualization"]},{"cell_type":"markdown","metadata":{"id":"7DQBlqCJRcB3"},"source":["### Display Images\n","Function to display a set of image : \n","* input image, \n","* it's ground truth segmentation mask, and \n","* the model's mask prediction"]},{"cell_type":"code","metadata":{"id":"3N2RPAAW9q4W"},"source":["def display(image_list):\n","  plt.figure(figsize=(5*len(image_list), 5))\n","\n","  title = ['Input Image', 'True Mask', 'Predicted Mask']\n","\n","  for i in range(len(image_list)):\n","    plt.subplot(1, len(image_list), i+1)\n","    plt.title(title[i])\n","    plt.imshow(array_to_img(image_list[i]))\n","    plt.axis('off')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FpH6vOiTRwRJ"},"source":["### Plot History\n","Function to plot the train/val loss and accuracy history"]},{"cell_type":"code","metadata":{"id":"BucUemnYRF1O"},"source":["def plot_my_history(history):\n","  plt.rcParams['figure.figsize'] = [14, 3.5]\n","  plt.subplots_adjust(wspace=0.2)\n","\n","  plt.subplot(121)\n","  # Plot training & validation accuracy values\n","  plt.plot(history.history['accuracy'])\n","  plt.plot(history.history['val_accuracy'])\n","  plt.title('Model accuracy')\n","  plt.ylabel('Accuracy')\n","  plt.xlabel('Epoch')\n","  plt.legend(['Train', 'Test'])\n","\n","  plt.subplot(122)\n","  # Plot training & validation loss values\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  plt.title('Model loss')\n","  plt.ylabel('Loss')\n","  plt.xlabel('Epoch')\n","  plt.legend(['Train', 'Test'])\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qpeMsOz1f4te"},"source":["---\n","---\n","#[Part 1] Oxford-IIIT Pets dataset\n","\n","\n","The Oxford-IIIT pet dataset is a 37 category pet image dataset with roughly 200 images for each class. The images have large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed.\n","\n","<center><img src=\"https://www.robots.ox.ac.uk/~vgg/data/pets/pet_annotations.jpg\" width=\"80%\"></center>\n","\n","https://www.robots.ox.ac.uk/~vgg/data/pets/\n"]},{"cell_type":"markdown","metadata":{"id":"4iQUsChlpEG5"},"source":["---\n","## 1 - Download Dataset\n","\n","\n","The dataset is already included in TensorFlow datasets, all that is needed to do is download it. The segmentation masks are included in version 3+."]},{"cell_type":"code","metadata":{"id":"zAUygyZnn_cH"},"source":["dataset, ds_info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p3juOrC6tdX1"},"source":["---\n","## 2 - View Dataset\n","Now let's take a look at the dataset details. Here we use the `tfds.as_dataframe()` function to view the dataset  a pandas dataframe.\n","\n","However, you should see that the `segmentation_mask` is just a black box of image. This happen because the mask is just a label of `{1,2,3}`, which just result in black images if we tried to display it raw"]},{"cell_type":"code","metadata":{"id":"3_NIuyEFouRd"},"source":["tfds.as_dataframe(dataset['train'].take(4), ds_info)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zJosLH4WwhZF"},"source":["---\n","## 3 - View Segmentation Mask\n","\n","For that, we use the other `tfds` visualization method, which is `tfds.show_examples()`.\n","\n","Now you should be able to see the segmentation mask visualizations"]},{"cell_type":"code","metadata":{"id":"sDdcT_GCrX-C"},"source":["fig1 = tfds.show_examples(dataset['train'].take(6), ds_info, image_key='image')\n","fig2 = tfds.show_examples(dataset['train'].take(6), ds_info, image_key='segmentation_mask')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pOyn6CIbw2ZW"},"source":["---\n","## 4 - Data Augmentation\n","\n","Here, we will define a function to perform a simple augmentation of flipping an image. In addition, image is normalized to `[0,1]`. \n","\n","Finally, as mentioned above the pixels in the segmentation mask are labeled either `{1, 2, 3}`. For the sake of convenience, let's subtract 1 from the segmentation mask, resulting in labels that are : `{0, 1, 2}`."]},{"cell_type":"markdown","metadata":{"id":"Qnd0hKc8c7Dg"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Complete the Augmentation function.\n","\n","Use the `flip_left_right()` and `resize()` function from the `tensorflow.image` module to simpy resize the image to the desired size, then randomly mirror the trainset images along with its masks.\n"]},{"cell_type":"code","metadata":{"id":"UdjBnEfuwzTz"},"source":["@tf.function\n","def load_image_train(datapoint):\n","    # resize image to the desired size\n","    # call resize() function with input datapoint['image'] and img_size\n","    input_image = ??\n","    \n","    # call resize() function with input datapoint['segmentation_mask'] and img_size\n","    input_mask  = ??\n","\n","    # randomly horizontal flip the image and its mask\n","    if tf.random.uniform(()) > 0.5:\n","        # call flip_left_right() function with input input_image\n","        input_image = ??\n","\n","        # call flip_left_right() function with input input_image\n","        input_mask  = ??\n","\n","    # normalize input into [0,1] range\n","    input_image = tf.cast(input_image, tf.float32) / 255.0\n","    input_mask -= 1\n","\n","    return input_image, input_mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LzVRlTzkUAVw"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","perform the same resizing and normalization process, but without the augmentation part for the testset images.\n"]},{"cell_type":"code","metadata":{"id":"X1EDsiq6xV_L"},"source":["def load_image_test(datapoint):\n","    # resize image to the desired size\n","    # call resize() function with input datapoint['image'] and img_size\n","    input_image = ??\n","    \n","    # call resize() function with input datapoint['segmentation_mask'] and img_size\n","    input_mask  = ??\n","\n","    # normalize input into [0,1] range (see implementation above)\n","    input_image = ??\n","    input_mask  = ??\n","\n","    return input_image, input_mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iZoVmHXAxUjE"},"source":["---\n","## 5 - Data Generator\n","\n","Now let's build a data generator to preprocess (augment and normalize) the data in batches.\n","\n","The dataset already contains the required splits of test and train and so let's continue to use the same split.\n","\n","First we define the batch size and the image size"]},{"cell_type":"code","metadata":{"id":"ycUneVnrxzcT"},"source":["num_train       = ds_info.splits['train'].num_examples\n","img_size        = (160, 160)\n","num_classes     = 3\n","batch_size      = 64\n","buffer_size     = 1000\n","steps_per_epoch = num_train // batch_size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JKJWjf6rxoLz"},"source":["Then generate the train data generator"]},{"cell_type":"code","metadata":{"id":"YtO9AWOcyF63"},"source":["train_generator = dataset['train'].map(load_image_train, \n","                                       num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","train_dataset   = train_generator.cache().shuffle(buffer_size).batch(batch_size).repeat()\n","train_dataset   = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PM4xglzOU_t7"},"source":["Do the same for test set"]},{"cell_type":"code","metadata":{"id":"_di8k9X0ycEV"},"source":["test_generator = dataset['test'].map(load_image_test)\n","\n","test_dataset   = test_generator.batch(batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xa3gMAE_9qNa"},"source":["Let's take a look at an image example and it's correponding mask from the dataset."]},{"cell_type":"code","metadata":{"id":"JdeZPfkhzR2r"},"source":["for image, mask in test_generator.take(3):\n","    sample_image, sample_mask = image, mask\n","    display([sample_image, sample_mask])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wxK30RTsz9l4"},"source":["---\n","---\n","# [Part 2] U-Net Architecture\n","\n","The model being used here is a modified U-Net. A U-Net consists of an encoder (downsampler) and decoder (upsampler). In-order to learn robust features, and reduce the number of trainable parameters, a pretrained model can be used as the encoder. \n","<center><img src=\"https://zslpublications.onlinelibrary.wiley.com/cms/asset/ee790e9d-99bd-4cc4-8a92-c9d6c52cc1b3/rse2111-fig-0004-m.jpg\" width=\"80%\"></center>\n","\n","But here we're not going to use the original U-Net architecture. Here we're going to build a smaller U-Net-like architecture using Xception architecture as the base.\n"]},{"cell_type":"markdown","metadata":{"id":"auXyWkyO11IU"},"source":["---\n","## 1 - U-Net Encoder\n","Our Encoder Model will consist of several blocks of identical convolution layer with different filter size to extract feature at different depth. \n","\n","Here we will build the Blocks and the Encoder itself in separate models to make them easier to build and understand.\n"]},{"cell_type":"markdown","metadata":{"id":"BSjfW-kO-0_y"},"source":["### a. Encoder Block\n","\n","Now define the model builder for an Encoder block of a filter size.\n","\n","Each block consist of two sub-block of:\n","* Relu Activation\n","* $3\\times3$ Separable Convolution, and\n","* Batch Normalization Layer\n","\n","The sub-blocks are followed by a $3\\times3$ stride $2$ Max Pooling Layer.\n","\n","The block is then added with a residual connection of <br> $1\\times1$ stride $2$ Conv2D received from the input block"]},{"cell_type":"markdown","metadata":{"id":"jTd_WajOVTA7"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement the function as defined above. \n","\n","Remember, here we use the Functional API model building.\n","\n","hint: use Activation() layer for relu\n"]},{"cell_type":"code","metadata":{"id":"HlNOC9hG3O4i"},"source":["def build_encoder_block(input_shape, filters):\n","\n","    # define the input block as input_shape\n","    block_input = Input(shape=input_shape)\n","\n","    # Sub-block 1\n","    # add relu activation after block_input\n","    x = ?? (??)\n","    # add SeparableConv2D() with input filters, kernel_size, and padding='same' after x\n","    x = ?? (??)\n","    # add Batch Normalization after x\n","    x = ?? (??)\n","\n","    # Sub-block 2\n","    # add relu activation after x\n","    x = ?? (??)\n","    # add SeparableConv2D() with input filters, kernel_size, and padding='same' after x\n","    x = ?? (??)\n","    # add Batch Normalization after x\n","    x = ?? (??)\n","\n","    # add MaxPool2D with input pool_size, strides, and padding='same' after x\n","    x = ?? (??)\n","\n","    # Project residual from input\n","    # add Conv2D with input filters, kernel_size=1, stride=2, and padding='same' after block_input\n","    residual = ??(??)\n","\n","    # Add x and residual as output\n","    block_output = add([x, residual])  \n","\n","    # instantiate model from block_input to block_output\n","    block_model = Model(??, ??, name='enc_block_'+str(filters))\n","\n","    return block_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t6j1dF8RfAHi"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"UuRUkAMq4Ujl"},"source":["filters       = 32\n","input_shape   = (28, 28, 1)\n","encoder_block = build_encoder_block(input_shape, filters)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RDZy7hzgY3aq"},"source":["Print the Summary"]},{"cell_type":"code","metadata":{"id":"prK1ksck44HY"},"source":["encoder_block.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kWpREQRp7l0s"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Model: \"enc_block_32\"\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_? (InputLayer)            [(None, 28, 28, 1)]  0                                            \n","activation_? (Activation)       (None, 28, 28, 1)    0           input_?[0][0]                    \n","separable_conv2d_? (SeparableCo (None, 28, 28, 32)   73          activation_?[0][0]                 \n","batch_normalization_? (BatchNor (None, 28, 28, 32)   128         separable_conv2d_?[0][0]           \n","...\n","max_pooling2d_? (MaxPooling2D)  (None, 14, 14, 32)   0           batch_normalization_?[0][0]      \n","conv2d_? (Conv2D)               (None, 14, 14, 32)   64          input_1[0][0]                    \n","add_? (Add)                     (None, 14, 14, 32)   0           max_pooling2d[0][0]              \n","                                                                 conv2d[0][0]                     \n","==================================================================================================\n","Total params: 1,737\n","Trainable params: 1,609\n","Non-trainable params: 128"]},{"cell_type":"markdown","metadata":{"id":"IZGct5ADZ1VF"},"source":["Let's also visualize the model"]},{"cell_type":"code","metadata":{"id":"j2ZNy4GE46qJ"},"source":["plot_model(encoder_block, dpi=65)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BCu8yLsxaZ66"},"source":["**EXPECTED OUTPUT**:\n","\n","<img src=\"https://i.ibb.co/5WhqZG9/unet-enc-block.png\" height=400>"]},{"cell_type":"markdown","metadata":{"id":"PwFElAxU-xCw"},"source":["### b. Encoder Model\n","Now we're done with the Encoder Block, let's define the Encoder Model builder function\n","\n","The builder function receives a list of filters, and iteratively builds and stacks a block for each filter size"]},{"cell_type":"markdown","metadata":{"id":"xJ0EqZzidThr"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement the function as defined above. \n","\n","Remember, here we use the Functional API model building.\n"]},{"cell_type":"code","metadata":{"id":"Y1syQ_ok3m3c"},"source":["def build_encoder_model(input_shape, filter_list):\n","\n","    # define the input model as input_shape\n","    encoder_input = Input(shape=??)\n","\n","    # store input as x for iterative purpose\n","    x = encoder_input\n","\n","    # loop for each filter\n","    for filters in filter_list:\n","        # call build_encoder_block function with input input_shape and filters\n","        encoder_block = ??\n","\n","        # stack new encoder_block to x\n","        x = encoder_block(x)\n","\n","        # get the last block output shape \n","        input_shape = encoder_block.output.shape[1:]\n","\n","    # get the encoder output\n","    encoder_output = x\n","\n","    # instantiate model from encoder_input to encoder_output\n","    encoder_model = Model(??, ??, name='encoder_model')\n","        \n","    return encoder_model\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ETUp0OvdXww"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"OcKKjH3l5h3p"},"source":["filter_list   = [64, 128, 256]\n","input_shape   = (56, 56, 1)\n","encoder_model = build_encoder_model(input_shape, filter_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iPfJFuFWda23"},"source":["Print the Summary"]},{"cell_type":"code","metadata":{"id":"gDRWy6n45taO"},"source":["encoder_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0dLRe_lyddf9"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Model: \"encoder_model\"\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_? (InputLayer)         [(None, 56, 56, 1)]       0         \n","enc_block_64 (Functional)    (None, 28, 28, 64)        5513      \n","enc_block_128 (Functional)   (None, 14, 14, 128)       35904     \n","enc_block_256 (Functional)   (None, 7, 7, 256)         137344    \n","=================================================================\n","Total params: 178,761\n","Trainable params: 176,969\n","Non-trainable params: 1,792"]},{"cell_type":"markdown","metadata":{"id":"gPZJcnL1dp0X"},"source":["Let's also visualize the model"]},{"cell_type":"code","metadata":{"id":"HQRe7CK55tXf"},"source":["plot_model(encoder_model, show_shapes=True, dpi=65)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6zDzcGtdd1w3"},"source":["**EXPECTED OUTPUT**:\n","\n","<img src=\"https://i.ibb.co/4VvgrLv/unet-enc.png\" height=250>"]},{"cell_type":"markdown","metadata":{"id":"kzT2ium1EtR2"},"source":["---\n","## 2 - U-Net Decoder\n","The Decoder model will consist of several blocks with identical transpose convolution layer with different filter size to reconstruct feature at different depth. These blocks are similar but mirrored of the Encoder blocks.\n","\n","Again, we will build the Blocks and the Decoder itself in separate models to make them easier to build and understand.\n"]},{"cell_type":"markdown","metadata":{"id":"iXxphykPA7ZY"},"source":["###a. Decoder Block\n","\n","Now define the model builder for a Decoder block of a filter size.\n","\n","Each block, again, consist of two sub-blocks of:\n","* Relu Activation\n","* $3\\times3$ Transpose Convolution, and\n","* Batch Normalization Layer\n","\n","The sub-blocks are followed by a $2\\times2$ UpSampling3D layer. Then added with a residual connection of <br> $1\\times1$ Conv2D received from  the input block"]},{"cell_type":"markdown","metadata":{"id":"jWfh01FAAYZf"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement the function as defined above. \n","\n","Remember, here we use the Functional API model building.\n","\n","hint: use Activation() layer for relu\n"]},{"cell_type":"code","metadata":{"id":"n_PCMp-cEtR2"},"source":["def build_decoder_block(input_shape, filters):\n","\n","    # define the input block as input_shape\n","    block_input = Input(shape=??)\n","\n","    # Sub-block 1\n","    # add relu activation after block_input\n","    x = ?? (??)\n","    # add Conv2DTranspose() with input filters, kernel_size, and padding='same' after x\n","    x = ?? (??)\n","    # add Batch Normalization after x\n","    x = ?? (??)\n","\n","    # Sub-block 2\n","    # add relu activation after x\n","    x = ?? (??)\n","    # add Conv2DTranspose() with input filters, kernel_size, and padding='same' after x\n","    x = ?? (??)\n","    # add Batch Normalization after x\n","    x = ?? (??)\n","\n","    # add UpSampling2D with input size=2 after x\n","    x = ?? (??)\n","\n","    # Project residual from input\n","    # add UpSampling2D with input size=2 after block_input\n","    residual = ?? (??)\n","    # add Conv2D with input filters, kernel_size=1, and padding='same' after residual\n","    residual = ?? (??)\n","\n","    # Add x and residual as output\n","    block_output = add([x, residual]) \n","\n","    # instantiate model from block_input to block_output\n","    block_model = Model(??, ??, name='dec_block_'+str(filters))\n","\n","    return block_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bkA-VaRCBHVG"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"B4vswCUtEtR3"},"source":["filters       = 32\n","input_shape   = (28, 28, 1)\n","decoder_block = build_decoder_block(input_shape, filters)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w668Vwi9BKDC"},"source":["Print the Summary"]},{"cell_type":"code","metadata":{"id":"8Sx10ObSEtR3"},"source":["decoder_block.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tqMJdB3zBLB8"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Model: \"dec_block_32\"\n","Layer (type)                    Output Shape         Param #     Connected to     \n","==================================================================================================              \n","input_? (InputLayer)            [(None, 28, 28, 1)]  0                                            \n","activation_? (Activation)       (None, 28, 28, 1)    0           input_?[0][0]                    \n","conv2d_transpose_? (Conv2DTrans (None, 28, 28, 32)   320         activation_?[0][0]               \n","batch_normalization_? (BatchNor (None, 28, 28, 32)   128         conv2d_transpose_>[0][0]           \n","...        \n","up_sampling2d_? (UpSampling2D)  (None, 56, 56, 1)    0           input_?[0][0]                    \n","up_sampling2d_? (UpSampling2D)  (None, 56, 56, 32)   0           batch_normalization_?[0][0]      \n","conv2d_? (Conv2D)               (None, 56, 56, 32)   64          up_sampling2d_?[0][0]            \n","add_? (Add)                     (None, 56, 56, 32)   0           up_sampling2d_?[0][0]              \n","                                                                 conv2d_?[0][0]                   \n","==================================================================================================\n","Total params: 9,888\n","Trainable params: 9,760\n","Non-trainable params: 128"]},{"cell_type":"markdown","metadata":{"id":"mPzXFevsCB-D"},"source":["Let's also visualize the model"]},{"cell_type":"code","metadata":{"id":"0oLhpNF-EtR3"},"source":["plot_model(decoder_block, dpi=65)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bv4S_nTQCN8k"},"source":["**EXPECTED OUTPUT**:\n","\n","<img src=\"https://i.ibb.co/PWwvTJM/unet-dec-block.png\" height=400>"]},{"cell_type":"markdown","metadata":{"id":"0MvsY0CrBkWc"},"source":["### b. Decoder Model\n","Now we're done with the Decoder Block, let's define the Decoder Model builder function\n","\n","The builder function receives a list of filters, and iteratively builds and stacks a block for each filter size\n"]},{"cell_type":"markdown","metadata":{"id":"W0N1cuVAIEq4"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement the function as defined above. \n","\n","Remember, here we use the Functional API model building.\n"]},{"cell_type":"code","metadata":{"id":"9eZNPxEmEtR3"},"source":["def build_decoder_model(input_shape, filter_list):\n","\n","    # define the input model as input_shape\n","    decoder_input = Input(shape=??)\n","\n","    # store input as x for iterative purpose\n","    x = decoder_input\n","\n","    # loop for each filter\n","    for filters in filter_list:\n","        # call build_decoder_block function with input input_shape and filters\n","        decoder_block = ??\n","\n","        # stack new decoder_block to x\n","        x = decoder_block(x)\n","\n","        # get the last block output shape \n","        input_shape = decoder_block.output.shape[1:]\n","\n","    # get the decoder output\n","    decoder_output = x\n","    \n","    # instantiate model from decoder_input to decoder_output\n","    decoder_model = Model(??, ??, name='decoder_model')\n","\n","    return decoder_model\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PcocEnHKIcxA"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"zBUd5aooEtR3"},"source":["filter_list   = [256, 128, 64]\n","input_shape   = (7, 7, 1)\n","decoder_model = build_decoder_model(input_shape, filter_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QjfUa0lQIiN_"},"source":["Print the Summary"]},{"cell_type":"code","metadata":{"id":"3f54G4h5EtR3"},"source":["decoder_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bln7aVvDIjfh"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Model: \"decoder_model\"\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_? (InputLayer)         [(None, 7, 7, 1)]         0         \n","dec_block_256 (Functional)   (None, 14, 14, 256)       595200    \n","dec_block_128 (Functional)   (None, 28, 28, 128)       476544    \n","dec_block_64 (Functional)    (None, 56, 56, 64)        119488    \n","=================================================================\n","Total params: 1,191,232\n","Trainable params: 1,189,440\n","Non-trainable params: 1,792"]},{"cell_type":"markdown","metadata":{"id":"wVmGCLm-I5Xn"},"source":["Let's also visualize the model"]},{"cell_type":"code","metadata":{"id":"5AqZD1cUEtR3"},"source":["plot_model(decoder_model, show_shapes=True, dpi=65)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T0T6SHQ4I6wf"},"source":["**EXPECTED OUTPUT**:\n","\n","<img src=\"https://i.ibb.co/hH8rQxs/unet-dec.png\" height=250>"]},{"cell_type":"markdown","metadata":{"id":"AkVo_KPBIW5p"},"source":["---\n","## 3 - U-Net Full\n","\n","Now let's combine both the Encoder and Decoder into one complete U-Net Model\n","\n","The Encoder receive an input from Entry block to perform downsampling of inputs, which consists of:\n","* $3\\times3$, stride $2$, Conv2D Layer\n","* Batch Normalization Layer, and\n","* Relu Activation\n","\n","While the output from the Decoder, will be added with the last Conv2D Layer $3\\times3$ to draw the segmentation mask"]},{"cell_type":"markdown","metadata":{"id":"hFr-__YuK7EA"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Implement the function as defined above. \n","\n","Remember, here we use the Functional API model building.\n","\n","hint: use Activation() layer for relu\n"]},{"cell_type":"code","metadata":{"id":"PNrw3r72JPAT"},"source":["def build_u_net(img_size, num_classes, e_filters, d_filters):\n","\n","    # define the input model\n","    inputs = Input(shape=img_size + (3,))\n","\n","    ### First half of the network: downsampling inputs ###\n","    # ----------1. Entry block -------------------------------------------------\n","    # add Conv2D with input filters=32, kernel_size=3, stride=2, and padding='same' after inputs\n","    x = ?? (??)\n","    # add Batch Normalization after x\n","    x = ?? (??)\n","    # add relu activation after x\n","    x = ?? (??)\n","\n","    # ----------2. Encoder blocks ----------------------------------------------\n","    # retrieve the output shape\n","    encoder_shape = x.shape[1:]\n","    # call build_encoder_model with input encoder_shape and e_filters\n","    encoder_model = ??\n","    # stack encoder_model to x\n","    x = encoder_model(x)\n","\n","\n","    ### Second half of the network: upsampling inputs ###\n","    # ----------3. Decoder blocks ----------------------------------------------\n","    # retrieve the output shape\n","    decoder_shape = x.shape[1:]\n","    # call build_decoder_model with input decoder_shape and d_filters\n","    decoder_model = ??\n","    # stack decoder_model to x\n","    x = decoder_model(x)\n","\n","    # ----------4. per-pixel classification layer ------------------------------\n","    # add Conv2D with input filters=num_classes, kernel_size=3,\n","    #     activation='softmax', and padding='same' after x\n","    outputs = ?? (??)\n","\n","    # instantiate model from inputs to outputs\n","    model = Model(??, ??, name='u-net')\n","\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aYJI6tXhM7YC"},"source":["##4 - Build Model\n","\n","Now build our U-Net Model.\n","\n","Let's use 3 blocks of encoder of `[64, 128, 256]` filters and 4 blocks of decoder of  `[256, 128, 64, 32]` filters. \n","\n","We use one less encoder block since in the Downsampling part of the network is already consist of an Entry block."]},{"cell_type":"markdown","metadata":{"id":"1bB5tVHwNZVc"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Build the model as defined above\n","\n","you can try and change the number of filters"]},{"cell_type":"code","metadata":{"id":"bCbklq27MsXP"},"source":["# Free up RAM in case the model definition cells were run multiple times\n","backend.clear_session()\n","\n","# define the filters lists\n","e_filters = ??\n","d_filters = ??\n","\n","call function to build the model\n","model = ??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W9B796yOOngV"},"source":["Check your implementation and print the Summary"]},{"cell_type":"code","metadata":{"id":"GhYe3c5NKahQ"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hBfOxmPeOtiG"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","Model: \"u-net\"\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_? (InputLayer)         [(None, 160, 160, 3)]     0         \n","conv2d_? (Conv2D)            (None, 80, 80, 32)        896       \n","batch_normalization_? (Batch (None, 80, 80, 32)        128       \n","activation_? (Activation)    (None, 80, 80, 32)        0         \n","encoder_model (Functional)   (None, 10, 10, 256)       183008    \n","decoder_model (Functional)   (None, 160, 160, 32)      1874080   \n","conv2d_? (Conv2D)            (None, 160, 160, 3)       867       \n","=================================================================\n","Total params: 2,058,979\n","Trainable params: 2,055,203\n","Non-trainable params: 3,776"]},{"cell_type":"markdown","metadata":{"id":"-NjJMp_VPGhM"},"source":["Let's also visualize the model"]},{"cell_type":"code","metadata":{"id":"OYrtriAaKndJ"},"source":["plot_model(model, show_shapes=True, dpi=65)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3QbBKxMGPIHM"},"source":["**EXPECTED OUTPUT**:\n","\n","<img src=\"https://i.ibb.co/DzB8nCq/unet.png\" height=400>"]},{"cell_type":"markdown","metadata":{"id":"m5vDFJCqPZU4"},"source":["---\n","---\n","# [Part 3] Train Segmentation\n","\n","Now, all that is left to do is to compile and train the model. The loss being used here is `sparse_categorical_crossentropy`. The reason to use this loss function is because the network is trying to assign each pixel a label, just like multi-class prediction. \n","\n","In the true segmentation mask, each pixel has either a `{0,1,2}`. The network here is outputting three channels. Essentially, each channel is trying to learn to predict a class, and `sparse_categorical_crossentropy` is the recommended loss for such a scenario. \n"]},{"cell_type":"markdown","metadata":{"id":"X_EzC0ZsQdJm"},"source":["---\n","## 1 - Mask Visualization\n","\n","Using the output of the network, the label assigned to the pixel is the channel with the highest value. \n","\n","This is what the `create_mask` function is doing."]},{"cell_type":"code","metadata":{"id":"bIzmnVtVK8Si"},"source":["def create_mask(pred_mask):\n","\n","    # get class of max score from prediction\n","    pred_mask = tf.argmax(pred_mask, axis=-1)\n","    pred_mask = pred_mask[..., tf.newaxis]\n","\n","    return pred_mask[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mGrbfSrhRBaB"},"source":["---\n","## 2 - Compile Model"]},{"cell_type":"markdown","metadata":{"id":"8g0U9uedQp2V"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","compile model with `adam` or `rmsprop` optimizer with `sparse_categorical_crossentropy` loss and accuracy metric\n"]},{"cell_type":"code","metadata":{"id":"y3EtVTCiK54a"},"source":["model.??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bZV7i2wWPv4f"},"source":["---\n","## 3 - Visualize Prediction\n","\n","Below is a function to convert the model prediction into mask visualization using previous `create_mask` function, and display it using `display` function"]},{"cell_type":"code","metadata":{"id":"dlfWebxpK8Pi"},"source":["def show_predictions(dataset=None, num=1):\n","  if dataset:\n","    for image, mask in dataset.take(num):\n","      pred_mask = model.predict(image)\n","      display([image[0], mask[0], create_mask(pred_mask)])\n","  else:\n","    display([sample_image, sample_mask,\n","             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jsudOf3BRpHw"},"source":["\n","Let's try out the model to see what it predicts before training."]},{"cell_type":"code","metadata":{"id":"3LxQhE9LK8Mz"},"source":["show_predictions()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4yWLM_eVWEmQ"},"source":["You should see taht the prediction mask is all over the place"]},{"cell_type":"markdown","metadata":{"id":"Ambx9B1CRFx7"},"source":["---\n","## 4 - DisplayCallback\n","\n","Below is a keras callback to visualize the training progress every 5 epoch"]},{"cell_type":"code","metadata":{"id":"b2d-8W2iK8J6"},"source":["class DisplayCallback(tf.keras.callbacks.Callback):\n","\n","  def on_epoch_end(self, epoch, logs=None):\n","    if epoch%5 == 0:\n","        show_predictions()\n","        print ('Sample Prediction after epoch {}'.format(epoch+1))\n","        print ('loss: %0.4f - accuracy: %0.4f - val_loss: %0.4f - val_accuracy: %0.4f\\n' % \n","               (logs['loss'], logs['val_loss'], logs['accuracy'], logs['val_accuracy']))\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"toOTKcXeRwJc"},"source":["---\n","## 5 - Train Model\n"]},{"cell_type":"markdown","metadata":{"id":"T4GQZMaOT27K"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","Now train the model for 26 epochs. \n","\n","Train the model for a longer epoch to achieve more accurate results.\n","\n"]},{"cell_type":"code","metadata":{"id":"dNb9DwskLgVG"},"source":["epochs   = 26\n","num_val  = 5\n","val_step = ds_info.splits['test'].num_examples//batch_size//num_val\n","\n","model_history = model.fit(train_dataset, epochs=epochs,\n","                          steps_per_epoch=steps_per_epoch,\n","                          validation_steps=val_step,\n","                          validation_data=test_dataset,\n","                          callbacks=[DisplayCallback()], verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YNMlF8jBaOtj"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n","the training loss should start around 2.0 and end around 0.1 after 26 epochs\n","with training accuracy start around 64% and end around 92%"]},{"cell_type":"markdown","metadata":{"id":"FHUrnV6BSyOY"},"source":["Lets visualize the training history."]},{"cell_type":"code","metadata":{"id":"qUvq03wiRnvF"},"source":["plot_my_history(model_history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"unP3cnxo_N72"},"source":["---\n","## 6 - Make predictions\n","\n","Let's make some predictions. "]},{"cell_type":"code","metadata":{"id":"ikrzoG24qwf5"},"source":["show_predictions(test_dataset, 3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aQqQ_gcO92ub"},"source":["\n","---\n","\n","# Congratulation, You've Completed Exercise 16\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2020 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"id":"7_Hkr9kg9Do-"},"source":["![footer](https://i.ibb.co/yX0jfMS/footer2020.png)"]}]}