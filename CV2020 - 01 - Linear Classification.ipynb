{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CV2020 - 01 - Linear Classification.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"281px"},"toc_section_display":true,"toc_window_display":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"Nde_vGtp9Dlr"},"source":["![title](https://i.ibb.co/f2W87Fg/logo2020.png)\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"oUkeuLWzoHoN"},"source":["<table  class=\"tfo-notebook-buttons\" align=\"left\"><tr><td>\n","    \n","<a href=\"https://colab.research.google.com/github/adf-telkomuniv/CV2020_Exercises/blob/master/CV2020 - 01 - Linear Classification.ipynb\" source=\"blank\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n","</td><td>\n","<a href=\"https://github.com/adf-telkomuniv/CV2020_Exercises/blob/master/notebooks/CV2020 - 01 - Linear Classification.ipynb\" source=\"blank\" ><img src=\"https://i.ibb.co/6NxqGSF/pinpng-com-github-logo-png-small.png\"></a>\n","    \n","</td></tr></table>"]},{"cell_type":"markdown","metadata":{"id":"pqfEjzA09Dlx"},"source":["# Task 1 - Simple Linear Classification\n","\n","In this assignment you will practice putting together a simple image classification pipeline, based on the SVM and Softmax classifier. \n","\n","The goals of this assignment are as follows:\n","* understand the basic Image Classification pipeline and the data-driven approach (train/predict stages)\n","* understand the train / val / test splits and the use of validation data for hyperparameter tuning.\n","* develop proficiency in writing efficient vectorized code with numpy\n","\n","* implement and apply a Softmax classifier\n","* implement and apply a Multiclass Support Vector Machine (SVM) classifier\n","\n","* understand the differences and tradeoffs between these classifiers"]},{"cell_type":"markdown","metadata":{"id":"pMlC1Kwa9Dl0"},"source":["---\n","## 0 - Import necessary libraries and informations"]},{"cell_type":"code","metadata":{"id":"SWJMf_Kx9Dl3"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","np.set_printoptions(precision=8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M8molz1J9Dl-"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"CkzQo8MY9DmB"},"source":["## --- start your code here ----\n","\n","NIM = 123456\n","Nama = \"\"\n","\n","## --- end your code here ----"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FNXQVKWr9DmH"},"source":["---\n","---\n","## 1 - Load CIFAR-10 Dataset\n","\n","* First, Obtain Cifar-10 dataset.\n","  There are from various source in Internet like [Keras](https://keras.io/datasets/), [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/datasets), or any other source\n","* Next you will prepare the dataset by first:\n"," * visualizing data\n"," * split into training, validation, and testing set\n"," * normalize data"]},{"cell_type":"markdown","metadata":{"id":"FJ4WkMiR9DmI"},"source":["---\n","### a. Import Data ***CIFAR-10***"]},{"cell_type":"code","metadata":{"id":"tsjp0Rou9DmL"},"source":["import tensorflow as tf\n","\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'forse', 'ship', 'truck']\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a_drJAPL9DmR"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"0xkyOKsX9DmV"},"source":["print('X_train.shape =',X_train.shape)\n","print('y_train.shape =',y_train.shape)\n","print('X_test.shape  =',X_test.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SFmi0aMw9Dmb"},"source":["**Expected Output**: \n","<pre>\n","X_train.shape = (50000, 32, 32, 3)\n","y_train.shape = (50000, 1)\n","X_test.shape  = (10000, 32, 32, 3)\n","y_test.shape  = (10000, 1)\n"]},{"cell_type":"markdown","metadata":{"id":"6YNXtE039Dmd"},"source":["---\n","### b. Visualizing Data\n","\n","\n","Show the first 20 images from X_train"]},{"cell_type":"code","metadata":{"id":"oh3fg51s9Dmf"},"source":["fig, ax = plt.subplots(2,10,figsize=(15,4.5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for j in range(0,2):\n","    for i in range(0, 10):\n","        ax[j,i].imshow(X_train[i+j*10])\n","        ax[j,i].set_title(classes[y_train[i+j*10,0]])\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HAKLJdJK9Dml"},"source":["---\n","### c. Split Training Data"]},{"cell_type":"markdown","metadata":{"id":"doRg25qJhORm"},"source":["\n","#####<font color='red'>**EXERCISE**: </font>\n","* Cut the `last 10000 data` from `Training Set`, and save it as `Validation Set`"]},{"cell_type":"code","metadata":{"id":"lQ8nhpld9Dmm"},"source":["X_train = X_train.astype('float32')\n","X_test  = X_test.astype('float32')\n","\n","X_val = ??\n","y_val = ??\n","\n","X_train = ??\n","y_train = ??\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ISmCCfEH9Dmr"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"pSi7HV2g9Dmu"},"source":["print('X_val.shape   =',X_val.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('X_train.shape =',X_train.shape)\n","print('y_train.shape =',y_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pd-rnDbi9Dmy"},"source":["**Expected Output**: \n","<pre>X_val.shape   = (10000, 32, 32, 3)\n","y_val.shape   = (10000, 1)\n","X_train.shape = (40000, 32, 32, 3)\n","y_train.shape = (40000, 1)"]},{"cell_type":"markdown","metadata":{"id":"Lzcjo5I49Dm2"},"source":["---\n","### d. Normalizing Data"]},{"cell_type":"markdown","metadata":{"id":"_RzFEfM6hT2l"},"source":["##### <font color='red'>**EXERCISE**: </font>\n","* Normalize `X_train`, `X_val`, and `X_test` by *zero-centering* them:\n","    1. calculate the `mean` of training data `X_train`\n","    * subtract `X_train`, `X_val`, and `X_test` using mean of `X_train`"]},{"cell_type":"code","metadata":{"id":"FcpIfZc89Dm3"},"source":["mean_image = ??\n","\n","X_train = ??\n","X_val   = ??\n","X_test  = ??\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MOMpLNHT9Dm6"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"BWC1p_HN9Dm7"},"source":["print('np.mean(X_train) =',np.mean(X_train))\n","print('np.mean(X_val)   =',np.mean(X_val))\n","print('np.mean(X_test)  =',np.mean(X_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GnveLwD19Dm-"},"source":["**Expected Output**: \n","<pre>np.mean(X_train) = -9.2315673e-07\n","np.mean(X_val)   = 0.14794703\n","np.mean(X_test)  = 0.8511902"]},{"cell_type":"markdown","metadata":{"id":"-P4PfeWZhqxN"},"source":["---\n","### e. Reshape Data"]},{"cell_type":"markdown","metadata":{"id":"FQXaVoDOhoN-"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* Reshape each data in `X_train`, `X_val`, and `X_test` into 1-dimensional matrix \n","\n","*Hint: use `np.reshape()`*"]},{"cell_type":"code","metadata":{"id":"w-H-e8_c9Dm_"},"source":["X_train = ??\n","X_val   = ??\n","X_test  = ??\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pOje5Ipx9DnD"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"G7JNj6Iu9DnE"},"source":["print('X_train.shape =',X_train.shape)\n","print('X_val.shape   =',X_val.shape)\n","print('X_test.shape  =',X_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hh7U-aDf9DnJ"},"source":["**Expected Output**: \n","<pre>X_train.shape = (40000, 3072)\n","X_val.shape   = (10000, 3072)\n","X_test.shape  = (10000, 3072)\n"]},{"cell_type":"markdown","metadata":{"id":"Qj004Z-R9DnJ"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* Reshape `y_train`, `y_val`, and `y_test` into a vector \n","\n","*Hint: use `np.ravel()`*"]},{"cell_type":"code","metadata":{"id":"-8OeUi9Z9DnK"},"source":["y_train = ??\n","y_val   = ??\n","y_test  = ??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AmCy39OH9DnM"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"dtLgvhjN9DnO"},"source":["print('y_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jiVm8QKG9DnR"},"source":["**Expected Output**: \n","<pre>y_train.shape = (40000,)\n","y_val.shape   = (10000,)\n","y_test.shape  = (10000,)\n"]},{"cell_type":"markdown","metadata":{"id":"YWdxD4yY9DnS"},"source":["---\n","---\n","## 2 - Linear Function\n","* Complete the forward and backward function of basic linear regression function\n"]},{"cell_type":"markdown","metadata":{"id":"wn91VV7ch6Ig"},"source":["---\n","### a. Forward Function"]},{"cell_type":"markdown","metadata":{"id":"jksrOLnmh3Uz"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* Implement `forward function` for Linear Classifier as follow:\n","\n","$$\n","\\begin{align}\n","f(x, W, b) = x.W + b\n","\\end{align}\n","$$"]},{"cell_type":"code","metadata":{"id":"r5NBU3R09DnU"},"source":["def forward(x, W, b):  \n","    v = ??                      # x dot W + b    \n","    \n","    return v"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2RdksEU79DnY"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"tpCXyrSs9DnY"},"source":["np.random.seed(2020)\n","\n","X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n","W = np.random.random((3, 2)) \n","b = np.zeros((1, 2))\n","\n","v = forward(X, W, b)\n","\n","print('v.shape =', v.shape)\n","print('v =')\n","print(v)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6grD-Rsj9Dnc"},"source":["**Expected Output**: \n","<pre>v.shape = (4, 2) \n","v =\n","[[0.33691873 0.21695427]\n"," [0.84666425 0.48878998]\n"," [1.32319556 1.09034621]\n"," [1.83294108 1.36218193]]"]},{"cell_type":"markdown","metadata":{"id":"2VMpTCmxiA-U"},"source":["---\n","### b. Backward Function"]},{"cell_type":"markdown","metadata":{"id":"g2wHV9esh_XY"},"source":["\n","#####<font color='red'>**EXERCISE**: </font>\n","* Implement `backward function` for Linear Classifier as follow:\n","\n","\n","$$\n","\\begin{align*}\n","\\partial W & = x^T.\\partial out \\\\\n","\\partial b & = \\sum \\partial out \\\\\n","\\partial x & = \\partial out.W^T \\\\\n","\\end{align*}\n","$$\n","\n","*Hint: use `axis=0` and `keepdims=True` to calculate $\\partial b$*"]},{"cell_type":"code","metadata":{"id":"zbgHRRHX9Dne"},"source":["def backward(dout, x, W, b):\n","    dW = ??                        # x.T dot dout\n","    db = ??                        # sum dout, axis=0, keepdims=True\n","    dx = ??                        # dout dot W.T\n","    \n","    return dW, db, dx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m8fq8hUI9Dng"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"HwnaN5QY9Dng"},"source":["np.random.seed(2020)\n","\n","dout = np.random.random((4, 2)) \n","dW, db, dX = backward(dout, X, W, b)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BBi-x1YEfqKK"},"source":["print('dW.shape =', dW.shape)\n","print('dW =')\n","print(dW)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZmBC6KsR9Dnl"},"source":["**Expected Output**: \n","<pre>\n","dW.shape = (3, 2) \n","dW =\n","[[0.61339587 0.56026986]\n"," [0.78622267 0.61515131]\n"," [2.10941822 1.70549752]]\n"]},{"cell_type":"code","metadata":{"id":"YViZwMF79Dnl"},"source":["print('db.shape =', db.shape)\n","print('db =', db)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xr0Er_ix9Dnr"},"source":["**Expected Output**: \n","<pre>db.shape = (1, 2)\n","db = [[2.10941822 1.70549752]]"]},{"cell_type":"code","metadata":{"id":"8o5k8LUv9Dnr"},"source":["print('dX.shape =', dX.shape)\n","print('dX =')\n","print(dX)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VNebgGmG9Dnt"},"source":["**Expected Output**: \n","<pre>dX.shape = (4, 3)\n","dX =\n","[[1.73555547 0.74016932 0.52178124]\n"," [0.74016932 0.33373516 0.23071873]\n"," [0.52178124 0.23071873 0.16058338]\n"," [0.57253207 0.23425843 0.16763411]]\n"]},{"cell_type":"markdown","metadata":{"id":"dsBraIv69Dnu"},"source":["---\n","---\n","## 3 - Softmax Function\n","\n","* A very important concept to understand in numpy is `\"broadcasting\"`. \n","* It is very useful for performing mathematical operations between arrays of different shapes. \n","* For the full details on broadcasting, you can read the official [broadcasting documentation](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)."]},{"cell_type":"markdown","metadata":{"id":"GEb-4C9C9Dnw"},"source":["---\n","### a. Softmax Score"]},{"cell_type":"markdown","metadata":{"id":"pBxRcMnTiKQJ"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* Implement a `softmax score function` using numpy."]},{"cell_type":"markdown","metadata":{"id":"GWVsslj19Dnw"},"source":["\n","for a matrix $x \\in \\mathbb{R}^{m\\times n}$\n","- $ n $ &nbsp; : number of data\n","- $ m $ &nbsp;: number of label\n","$x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$ \n","\n","\n","thus we have\n","$$softmax(x) = softmax\\begin{bmatrix}\n","    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n","    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n","    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n","    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n","\\end{bmatrix} = \\begin{bmatrix}\n","    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n","    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n","    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n","    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n","\\end{bmatrix}$$\n","\n","it is equal to\n","$$softmax(x)  = \\begin{pmatrix}\n","    softmax\\text{(first row of x)}  \\\\\n","    softmax\\text{(second row of x)} \\\\\n","    ...  \\\\\n","    softmax\\text{(last row of x)} \\\\\n","\\end{pmatrix} $$"]},{"cell_type":"code","metadata":{"id":"kGx1tnkt9Dny"},"source":["def softmax(x):\n","    \"\"\"Calculates the softmax for each row of the input x.\n","\n","Argument:\n","    x -- A numpy matrix of shape (n,m)\n","\n","    Returns:\n","    score -- A numpy matrix equal to the softmax of x, of shape (n,m)\n","          -- normalized log probabilities score\n","    \"\"\"\n","    \n","    # 1. COMPUTE THE CLASS PROBABILITIES    \n","    \n","    # shift x by subtracting with its maximum value . Use np.max(...)\n","    x = ??\n","    \n","    # Apply exp() element-wise to x. Use np.exp(...).    \n","    x_exp = ??\n","\n","    # Create a vector X_sum that sums each row of X_exp.\n","    # Use np.sum(..., axis = 1, keepdims = True).\n","    x_sum = ??\n","    \n","    # Compute softmax(x) score by dividing X_exp by X_sum. \n","    # It should automatically use numpy broadcasting.\n","    score = ??\n","    \n","    return score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6rLwS9af9Dn1"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"scUi41em9Dn2"},"source":["np.random.seed(2020)\n","\n","x = np.array([\n","    [1, 3, 0, 1 ,1],\n","    [0, 2, 0, 1, 9],\n","    [0, 1, 0, 1, 0]])\n","\n","W = np.random.random((5, 2)) \n","b = np.zeros((1, 2))\n","\n","score = forward(x, W, b)\n","\n","print(\"score = \" )\n","print(score,'\\n')\n","print(\"softmax(score) = \" )\n","print(softmax(score))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coz3paZb9Dn4"},"source":["**Expected Output**:\n","<pre>score = \n","[[3.65414948 2.18891435]\n"," [9.05539861 2.29728405]\n"," [0.78622267 0.61515131]] \n","\n","softmax(score) = \n","[[0.81233207 0.18766793]\n"," [0.99883993 0.00116007]\n"," [0.54266384 0.45733616]]"]},{"cell_type":"markdown","metadata":{"id":"ZlkgZgQp9Dn4"},"source":["---\n","### b. Softmax Loss"]},{"cell_type":"markdown","metadata":{"id":"Qgwg39DDiWYH"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* Implement a `softmax loss` function using numpy. \n"," \n","- Loss of the `i-th data` is the normalized log probability of the score at the class should be<br>\n","  $$L_i =  -\\log(\\frac{e^{X_{y_i}}}{\\sum_{j}e^{X_{j}}})$$<br><br>\n"," \n","- The Softmax Loss is the average of all data loss<br>\n","  $$L = \\frac{1}{N}\\sum^N_{i=1}L_i$$"]},{"cell_type":"code","metadata":{"id":"mi1S0uZp9Dn5"},"source":["def softmax_loss(score, y):\n","    \"\"\"Calculates the softmax loss for each row of the input x.\n","    \n","    Argument:\n","    score -- A numpy matrix equal to the softmax of x, of shape (n,m)\n","          -- normalized log probabilities score\n","          \n","    y     -- A numpy vector of shape(n,)\n","          -- containing training labels;\n","          -- y[i] = c means that X[i] has label c, where 0 <= c < C.\n","\n","    Returns:\n","    dscore -- A numpy matrix equal to the softmax of x, of shape (n,m)\n","           -- gradient score of softmax\n","    \"\"\"\n","    \n","    num_examples = score.shape[0]\n","        \n","    # 2. COMPUTE THE LOSS : average cross-entropy loss\n","    #make a number list containing [1 2 3 ... n]\n","    number_list = ??\n","    \n","    # calculate the correct log probability of score[number_list,y] by applycing -np.log(...)\n","    corect_logprobs = ??\n","    \n","    # average the correct log probability, use np.sum then divide it by num_examples\n","    loss = ??\n","    \n","    # 3. COMPUTE THE GRADIENT ON SCORES\n","    dscores = score\n","    dscores[range(num_examples),y] -= 1\n","    dscores /= num_examples\n","\n","    return loss, dscores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wkV7BrMh9Dn7"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"y_6zil3N9Dn8"},"source":["np.random.seed(2020)\n","\n","x = np.array([\n","    [1, 3, 0, 1 ,1],\n","    [0, 2, 0, 1, 9],\n","    [0, 1, 0, 1, 0]])\n","\n","y = np.array([\n","    0, 1, 0\n","])\n","\n","W = np.random.random((5, 2)) \n","b = np.zeros((1, 2))\n","\n","score = forward(x, W, b)\n","\n","print(\"score = \" )\n","print(score, '\\n')\n","\n","score = softmax(score)\n","print(\"softmax(score) = \" )\n","print(score,'\\n')\n","\n","loss, dscore  = softmax_loss(score, y)\n","print('loss   =',loss, '\\n')\n","print('dscore =')\n","print(dscore)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nBvXw_F89DoD"},"source":["**Expected Output**:\n","<pre>score = \n","[[3.65414948 2.18891435]\n"," [9.05539861 2.29728405]\n"," [0.78622267 0.61515131]] \n","\n","softmax(score) = \n","[[0.81233207 0.18766793]\n"," [0.99883993 0.00116007]\n"," [0.54266384 0.45733616]] \n","\n","loss   = 2.526128866233045 \n","\n","dscore =\n","[[-0.06255598  0.06255598]\n"," [ 0.33294664 -0.33294664]\n"," [-0.15244539  0.15244539]]"]},{"cell_type":"markdown","metadata":{"id":"vl2_OZBK9DoE"},"source":["---\n","---\n","## 4 - Linear Classifier: Softmax Classifier\n","* Combine linear function and softmax function that you have completed to train a multiclass softmax classifier"]},{"cell_type":"markdown","metadata":{"id":"_KO3lGVv9DoF"},"source":["---\n","### a. Weight Initialization"]},{"cell_type":"markdown","metadata":{"id":"EJ6YPXO4ieLn"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* Implement `Weight initialization function`\n","    - receive input size `n`, number of neuron `d`, and standard deviation `std`\n","    - generate normal random matrix `W` of size `(n x d)` and multiply it by `std`\n","    - generate zeros matrix `b` of size `(1 x d)`"]},{"cell_type":"code","metadata":{"id":"HsDrps-i9DoF"},"source":["def initialize_weights(n, d, std):\n","    W = ??                            # np.random.random * std\n","    b = ??\n","    \n","    return W, b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t-vNHyqY5PGM"},"source":["W, b = initialize_weights(20, 10, 0.001)\n","print('shape W:', W.shape)\n","print('shape b:', b.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3v-bzjwa5PGX"},"source":["**Expected Output**:\n","<pre>shape W: (20, 10)\n","shape b: (1, 10)"]},{"cell_type":"markdown","metadata":{"id":"ITTqUp4c9DoJ"},"source":["---\n","### b. Training Function\n"]},{"cell_type":"markdown","metadata":{"id":"qyN2_42QisO7"},"source":["#####<font color='red'>**EXERCISE**: </font> \n","* Implement **Training Function**\n","    * call `forward function`\n","    * call `softmax function`\n","    * call `softmax_loss function`\n","    * call `backward function`\n","    * implement `weight update`"]},{"cell_type":"code","metadata":{"id":"ISpmp2Oj9DoJ"},"source":["def train(X, y, W=None, b=None, learning_rate=1e-6, reg=1e4, num_iters=100, batch_size=200, verbose=False):\n","    \n","    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n","    \n","    # get number of training data and its dimension from X\n","    num_train, dim = ??\n","    \n","    # initialize new weights if W matrix is not provided\n","    # call initialize_weights function using dim, num_classes, and std=0.001\n","    if W is None:\n","        W, b = ??\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","                     \n","    for it in range(num_iters):\n","        X_batch = None\n","        y_batch = None\n","\n","        # Randomly select indices from training examples\n","        train_rows = np.arange(num_train)\n","        idxs = np.random.choice(train_rows, batch_size, replace=False)\n","  \n","        X_batch = X[idxs]\n","        y_batch = y[idxs]\n","\n","\n","        # calculate class score by calling forward function using X_batch, W, and b\n","        scores = ??\n","        \n","        # calculate softmax score by calling softmax function using scores\n","        softmaX_score = ??\n","        \n","        # evaluate loss and gradient by calling softmax_loss function using softmax_score and y_batch\n","        loss, dout = ??\n","    \n","        # append the loss history\n","        loss_history.append(loss)\n","\n","        # calculate weights gradient by calling backward function using dout, X_batch, W, and b\n","        dW, db, _ = ??\n","        \n","        # perform regulatization gradient\n","        dW += reg*W\n","        \n","        # perform parameter update by subtracting W and b with a fraction of dW and db\n","        # according to the learning rate\n","        W -= ??\n","        b -= ??\n","        \n","        if verbose and it % 100 == 0:\n","            print ('iteration', it,'/',num_iters, ': loss =', loss)\n","    return loss_history, W, b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XbGhYnV49DoL"},"source":["---\n","### c. Train the Softmax Classifier\n","\n","Try the training Function using the initial parameter"]},{"cell_type":"code","metadata":{"id":"RsvCPm849DoM"},"source":["loss, W, b = train(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iLHo9U5O9DoQ"},"source":["Visualize the loss"]},{"cell_type":"code","metadata":{"id":"C0lzRGK49DoR","scrolled":true},"source":["plt.plot(loss)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l_dspMjV9DoT"},"source":["**Expected Output**:\n","The graph should show that the loss is generally decreased as the training goes"]},{"cell_type":"markdown","metadata":{"id":"z6iAfJWB9DoU"},"source":["---\n","### d. Predict Function"]},{"cell_type":"markdown","metadata":{"id":"4jFrHYBMi1N-"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* Implement **Predict Function**\n","    * call `forward function`"]},{"cell_type":"code","metadata":{"id":"DqxCYfon9DoU"},"source":["def predict(X, W, b):    \n","    y_pred = np.zeros(X.shape[1])\n","\n","    \n","    # calculate class score by calling forward function using X, W, and b\n","    y_pred = ??\n","    \n","    # take the maximum prediction and use that column to get the class     \n","    y_pred = y_pred.argmax(axis=-1)\n","    \n","    return y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uCpnbDZO9DoY"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"6JAjNgRU9DoY"},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict(X_train, W, b)\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =', accuracy*100,'%')\n","\n","print('Training label  =', y_test[:15])\n","print('Predicted label =', y_pred[:15])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OplASJoa9Dob"},"source":["**Expected Output**:\n","\n","You should get about **`27-29%`** accuracy on training set using the initial run\n","\n","---"]},{"cell_type":"code","metadata":{"id":"LMeU07P59Doc"},"source":["y_pred = predict(X_val, W, b)\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_test[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XTMNd3Ud9Dof"},"source":["**Expected Output**:\n","\n","You should also get about **`27-29%`** accuracy on validation set\n","\n","You can retrain further the weights by adding the pre-trained `W` and `b` to the arguments when calling training function\n"]},{"cell_type":"markdown","metadata":{"id":"2yJRdoIsjCM2"},"source":["---\n","---\n","## 5 - Hyperparameter Tuning\n","* After you find that the classifier is working properly, next\n","* Find the best Learning Rate and Regularization Strength\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fyocgmtR9Doi"},"source":["---\n","###a. Try Another Hyperparameter Combination\n","\n","Try to add another combination of learning rates and regularization strength \n"]},{"cell_type":"markdown","metadata":{"id":"jQFGSBjJi8kv"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* Use the `validation set` to tune hyperparameters (regularization strength and learning rate). \n","     * Greedily loop over `learning_rates` and `regulatization` to get all combination\n","     \n","     \n","* You **should** experiment with different ranges for the learning rates and regularization strengths; "]},{"cell_type":"code","metadata":{"id":"IZSdjo7V9Dof"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","np.random.seed(None)\n","\n","results = {}\n","best_val = -1\n","best_W = None\n","best_b = None\n","learning_rates = [1e-7, 5e-7]\n","regularization_strengths = [9e4, 1e5]\n","\n","iterations = 2000\n","\n","# Greedily loop over learning_rates and regularization_strengths\n","for lr in ??:\n","    for reg in ??:\n","        print('Running {} iterations, rate = {}, reg = {}'.format(iterations, lr, reg))\n","        # call train function using the learning rate and regularization selected\n","        loss, W, b = train(X_train, y_train, \n","                              learning_rate=lr, reg=reg,\n","                              num_iters=iterations, verbose=False)\n","        \n","        # call predict function using pretrained W and b on X_train and X_val to evaluate\n","        y_train_pred = predict(??, ??, ??)\n","        y_val_pred = predict(??, ??, ??)\n","        \n","        # calculate the accuracy\n","        train_accuracy = np.mean(y_train == y_train_pred)\n","        val_accuracy   = np.mean(y_val == y_val_pred)\n","        \n","        print ('lr = {}, reg = {}, test accuracy = {}, validation accuracy = {}'.format(\n","            lr, reg, train_accuracy, val_accuracy))\n","        \n","        # store the result accuracy combination\n","        results[(lr, reg)] = (train_accuracy, val_accuracy)\n","        \n","        # store the best Weight and Bias\n","        if val_accuracy > best_val:\n","            best_W   = W\n","            best_b   = b\n","            best_val = val_accuracy\n","\n","\n","    \n","# Print out results.\n","for lr, reg in sorted(results):\n","    train_accuracy, val_accuracy = results[(lr, reg)]\n","    print('lr',lr,',reg',reg,', train accuracy: ',train_accuracy*100,'%, val accuracy: ', val_accuracy ,'%')\n","    \n","print()\n","print('best validation accuracy achieved during cross-validation:',best_val*100,'%')\n","print('best Weights and bias are stored in `best_W` and `best_b`')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nvoX3R9b9Doi"},"source":["**Expected Output**:\n","\n","if you are careful you should be able to get a classification accuracy of **over 35%** on the validation set\n"]},{"cell_type":"markdown","metadata":{"id":"7mAms-8F9Doi"},"source":["---\n","### b. Test the Trained Weights"]},{"cell_type":"markdown","metadata":{"id":"IXiiVIEzjHoX"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* Test the **Best Weights and Bias** to the `X_test`"]},{"cell_type":"code","metadata":{"id":"2rxRWuP59Doj","scrolled":true},"source":["y_pred = predict(??, ??, ??)\n","\n","accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n","\n","print('Testing Accuracy =', accuracy*100,'%')\n","print('Test label       =', y_test[:15])\n","print('Predicted label  =', y_pred[:15])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lM85_AmY9Dol"},"source":["**Expected Output**:\n","\n","You should reach at least  **32%** accuracy on test set\n"]},{"cell_type":"markdown","metadata":{"id":"C55BcQVqj--I"},"source":["---\n","---\n","## 6 - Vizualize the Trained Weights \n","\n","Visualize the learned weights for each class for each class\n"]},{"cell_type":"markdown","metadata":{"id":"YPsnDOay9Dom"},"source":["---\n","### a. Retrieve one class weights\n","Try to visualize one trained weights.<br><br>\n","**Note:** You can change the `id`"]},{"cell_type":"code","metadata":{"id":"Ga_gct3-9Dom"},"source":["id = 0\n","\n","# get the id-th column from best weight\n","w = best_W[:,id]\n","\n","print('w.shape =',w.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mroPU69R9Doo"},"source":["**Expected Output**:\n","<pre>\n","w.shape = (3072,)\n"]},{"cell_type":"markdown","metadata":{"id":"1WMK5MRI9Dop"},"source":["---\n","### b. Normalize the weight for visualization\n","Image plot can only receive input range [0..1] for float type, or [0..255] for integer type, and the weights are not in those range.\n","\n","So we need to normalize. Here we use MinMaxScaler from sklearn toolkit"]},{"cell_type":"code","metadata":{"id":"SCoMNGas9Dop"},"source":["from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler((0.05,.95))\n","w = scaler.fit_transform(w.reshape(-1, 1))\n","\n","print('w.shape =',w.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6hJ4PJU79Dor"},"source":["**Expected Output**:\n","<pre>\n","w.shape = (3072, 1)"]},{"cell_type":"markdown","metadata":{"id":"hpUlVHil9Dor"},"source":["---\n","### c. Visualize the weight \n","Now reshape the weight back to (32,32,3), and plot the image. Use `np.reshape()`"]},{"cell_type":"code","metadata":{"id":"op2LEL-x9Dos"},"source":["w = ??\n","\n","plt.figure(figsize=(2,2))\n","plt.imshow(w)\n","plt.title(classes[id])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KHoUj9Tg9Dot"},"source":["---\n","### d. Do it to the other weights"]},{"cell_type":"markdown","metadata":{"id":"2owarICzkwxg"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* complete the implementation to show the trained weight visualizations"]},{"cell_type":"code","metadata":{"id":"c-9SIJDu9Dot","scrolled":false},"source":["fig, ax = plt.subplots(1,10,figsize=(18,5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for i in range(0, 10):\n","    # get the i-th weight from best_W\n","    w = best_W[:,i]\n","    \n","    # normalize the weight (see previous example)\n","    w = ??\n","    \n","    # reshape the weight\n","    w = ??\n","    \n","    ax[i].imshow(w)\n","    ax[i].axis('off')\n","    ax[i].set_title(classes[i])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ucAtH6r9Dox"},"source":["**Expected Output**:\n","\n","You should see that the weights visualization should  match the ilustration below\n","\n","![cifar10](http://cs231n.github.io/assets/templates.jpg)\n"]},{"cell_type":"markdown","metadata":{"id":"Y0ULm3wikPlO"},"source":["---\n","---\n","## 7 - Linear Classifier: Multiclass SVM\n","Similar to before, let's build a Linear classifier to classify cifar-10 dataset, but now using Multiclass SVM loss, or Hinge Loss"]},{"cell_type":"markdown","metadata":{"id":"OEH_UX08kPlS"},"source":["---\n","### a. SVM Loss\n","Calculates the SVM loss for each row of the input x"]},{"cell_type":"markdown","metadata":{"id":"quyxAgXfkGmH"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* Implement `Multiclass SVM Loss` or `Hinge Loss`"]},{"cell_type":"code","metadata":{"id":"mp767_w69Do0"},"source":["def svm_loss(score, y):\n","    \"\"\"    \n","    Argument:\n","    score -- A numpy matrix equal to the softmax of x, of shape (n,m)\n","          -- normalized log probabilities score\n","          \n","    y     -- A numpy vector of shape(n,)\n","          -- containing training labels;\n","          -- y[i] = c means that X[i] has label c, where 0 <= c < C.\n","\n","    Returns:\n","    dscore -- A numpy matrix equal to the softmax of x, of shape (n,m)\n","           -- gradient score of softmax\n","    \"\"\"\n","    \n","    num_examples = score.shape[0]\n","    \n","    \n","    # 1. COMPUTE THE SVM LOSS\n","    \n","    # get the correct class score. Use np.choose( , ) using y and score.T\n","    correct_scores = ??\n","    \n","    # calculate margin: score.T subtracted by correct_score, then add by 1\n","    margin = ??\n","    \n","    # remove all margin below 0. Use np.maximum(0, margin)\n","    margin = ??\n","        \n","    # COMPUTE THE LOSS \n","    # calculate SVM loss of each data: sum of margin of non-target class\n","    # simply total all the margin (use np.sum()), and subtract it by 1\n","    loss_i = ??\n","        \n","    \n","    # average loss_i by num_examples\n","    loss = ??\n","    \n","    \n","    # 2. COMPUTE THE GRADIENT ON SCORES\n","    # margins for the correct class have already been set to 0\n","    dscores = (margin.T > 0).astype(float)\n","    marginsSum = dscores.sum(1) - 1\n","    dscores[range(dscores.shape[0]), y] = -marginsSum\n","        \n","    return loss, dscores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ukym7aAK9Do1"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"YxevLrVy9Do1"},"source":["np.random.seed(2020)\n","\n","x = np.array([\n","    [9, 2, 5, 0, 0],\n","    [7, 5, 0, 0 ,0],\n","    [1, 0, 3, 5 ,0]])\n","\n","y = np.array([\n","    0, 0, 1\n","])\n","\n","W = np.random.random((5, 2)) \n","b = np.zeros((1, 2))\n","\n","score = forward(x, W, b)\n","\n","print(\"score = \" )\n","print(score)\n","print()\n","\n","loss, dscore  = svm_loss(score, y)\n","print('loss   =',loss)\n","print('dscore =')\n","print(dscore)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qKjCMyCY9Do3"},"source":["**Expected Output**:\n","<pre>\n","score = \n","[[11.58057615  9.48897027]\n"," [ 9.45266543  7.47292219]\n"," [ 3.37941873  3.2408327 ]]\n","\n","loss   = 0.37952867436659093\n","dscore =\n","[[-0.  0.]\n"," [-0.  0.]\n"," [ 1. -1.]]\n"]},{"cell_type":"markdown","metadata":{"id":"rGGXv4Zn9Do3"},"source":["---\n","\n","Notice the loss gradients resulted from SVM Loss. \n","\n","What's the difference between those gradients and Softmax Gradients?\n","\n","What are the effects?"]},{"cell_type":"markdown","metadata":{"id":"XFFD2EqlkaLd"},"source":["---\n","### b. Training Function"]},{"cell_type":"markdown","metadata":{"id":"NT_V9WjakYHS"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* The same as the training function using Softmax before, complete the train function below, \n","* but rather than using `softmax_loss`, use `svm_loss`"]},{"cell_type":"code","metadata":{"id":"jYNpfn4g9Do4"},"source":["def train_svm(X, y, W=None, b=None, learning_rate=1e-6, reg=1e4, num_iters=100, batch_size=200, verbose=False):\n","    num_train, dim = X.shape\n","    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n","    \n","    if W is None:\n","        W = 0.001 * np.random.randn(dim, num_classes)\n","    if b is None:\n","        b = np.zeros((1,num_classes))\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","                     \n","    for it in range(num_iters):\n","        X_batch = None\n","        y_batch = None\n","\n","        # Randomly select indices from training examples\n","        train_rows = np.arange(num_train)\n","        idxs = np.random.choice(train_rows, batch_size, replace=False)\n","  \n","        X_batch = X[idxs]\n","        y_batch = y[idxs]\n","\n","\n","        # calculate class score by calling forward function using X_batch, W, and b\n","        scores = ??\n","                \n","        # evaluate loss and gradient by calling svm_loss function using score and y_batch\n","        loss, dout = ??\n","    \n","        # append the loss history\n","        loss_history.append(loss)\n","\n","        # calculate weights gradient by calling backward function using dout, X_batch, W, and b\n","        dW, db, _ = ??\n","        \n","        # perform regulatization gradient\n","        dW += reg*W\n","        \n","        # perform parameter update by subtracting W and b with a fraction of dW and db\n","        # according to the learning rate\n","        W -= ??\n","        b -= ??\n","        \n","        if verbose and it % 100 == 0:\n","            print ('iteration', it,'/',num_iters, ': loss =', loss)\n","    return loss_history, W, b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HV8tZkmckhiI"},"source":["---\n","### c. Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{"id":"tbyNEKuqkfc3"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* Again, complete the codes below similar to the Hyperparameter tuning for `Softmax Classifier`"]},{"cell_type":"code","metadata":{"id":"4_kPgMF79Do6"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","np.random.seed(None)\n","\n","results = {}\n","best_val = -1\n","best_W = None\n","best_b = None\n","learning_rates = [1e-7, 5e-7]\n","regularization_strengths = [1e5, 2e5]\n","\n","iterations = 2000\n","\n","# Greedily loop over learning_rates and regularization_strengths\n","for lr in ??:\n","    for reg in ??:\n","        print('Running {} iterations, rate = {}, reg = {}'.format(iterations, lr, reg))\n","        # call train function using the learning rate and regularization selected\n","        loss, W, b = train_svm(X_train, y_train, \n","                              learning_rate=lr, reg=reg,\n","                              num_iters=iterations, verbose=False)\n","        \n","        # call predict function using pretrained W and b on X_train and X_val to evaluate\n","        y_train_pred = predict(??, ??, ??)\n","        y_val_pred   = predict(??, ??, ??)\n","        \n","        # calculate the accuracy\n","        train_accuracy = np.mean(y_train == y_train_pred)\n","        val_accuracy   = np.mean(y_val == y_val_pred)\n","        \n","        print ('rate = {}, reg = {}, test accuracy = {}, validation accuracy = {}'.format(\n","            lr, reg, train_accuracy, val_accuracy))\n","        \n","        # store the result accuracy combination\n","        results[(lr, reg)] = (train_accuracy, val_accuracy)\n","        \n","        # store the best Weight and Bias\n","        if val_accuracy > best_val:\n","            best_W   = W\n","            best_b   = b\n","            best_val = val_accuracy\n","\n","            \n","# Print out results.\n","for lr, reg in sorted(results):\n","    train_accuracy, val_accuracy = results[(lr, reg)]\n","    print('lr',lr,',reg',reg,', train accuracy: ', train_accuracy*100,'%, val accuracy: ', val_accuracy ,'%')\n","    \n","print()\n","print('best validation accuracy achieved during cross-validation:', best_val*100, '%')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HACKUtEn9Do7"},"source":["**Expected Output**:\n","\n","You should reach `at least`  **`30%`** accuracy on cross-validation"]},{"cell_type":"markdown","metadata":{"id":"kxNcDPzcktSv"},"source":["---\n","### d. Visualize the Trained Weights "]},{"cell_type":"markdown","metadata":{"id":"29NqYRyZkmVY"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* complete the implementation to show the trained weight visualizations"]},{"cell_type":"code","metadata":{"id":"qyG_2yWO9Do8"},"source":["fig, ax = plt.subplots(1,10,figsize=(18,5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for i in range(0, 10):\n","    # get the i-th weight\n","    w = best_W[:,i]\n","    \n","    # normalize the weight\n","    w = ??\n","    \n","    # reshape the weight\n","    w = ??\n","    \n","    ax[i].imshow(w)\n","    ax[i].axis('off')\n","    ax[i].set_title(classes[i])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OGwFLO299Do9"},"source":["### Notice the difference in weight visualization\n","Describe what your visualized SVM weights look like. Think on why they look they way that they do.\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"quMA17JKDnpI"},"source":["---\n","\n","# Congratulation, You've Completed Exercise 1\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2020 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"id":"7_Hkr9kg9Do-"},"source":["![footer](https://i.ibb.co/yX0jfMS/footer2020.png)"]}]}