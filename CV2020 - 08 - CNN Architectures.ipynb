{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CV2020 - 08 - CNN Architectures.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"f_ChrQTZyAZU"},"source":["![title](https://i.ibb.co/f2W87Fg/logo2020.png)\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"oUkeuLWzoHoN"},"source":["<table  class=\"tfo-notebook-buttons\" align=\"left\"><tr><td>\n","    \n","<a href=\"https://colab.research.google.com/github/adf-telkomuniv/CV2020_Exercises/blob/main/CV2020 - 08 - CNN Architectures.ipynb\" source=\"blank\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n","</td><td>\n","<a href=\"https://github.com/adf-telkomuniv/CV2020_Exercises/blob/main/CV2020 - 08 - CNN Architectures.ipynb\" source=\"blank\" ><img src=\"https://i.ibb.co/6NxqGSF/pinpng-com-github-logo-png-small.png\"></a>\n","    \n","</td></tr></table>"]},{"cell_type":"markdown","metadata":{"id":"sjlcHwermNUS"},"source":["\n","# Task 8 - CNN Architectures\n","\n","In this assignment you will practice in replicating famous Convolutional Neural Network Architectures that won the IMAGENET Challange using Keras and TensorFlow.\n","\n","The goals of this assignment are as follows:\n","* understand the architecture advantages\n","* implement and train LeNet-5 on CIFAR-10\n","* implement AlexNet\n","* implement and train small AlexNet on CIFAR-10\n","* implement ZF-Net\n","* implement VGG Net\n","* implement and train small VGG on CIFAR-10\n","* implement and train small Inception Network on CIFAR-10\n","* implement and train small Residual Network on CIFAR-10"]},{"cell_type":"markdown","metadata":{"id":"6VAE_sE-Bgk1"},"source":["---\n","---\n","#[Part 0] Import Libraries and Load Data"]},{"cell_type":"markdown","metadata":{"id":"DvPSXMEIaFm1"},"source":["---\n","## 1 - Import Libraries\n","Import required libraries"]},{"cell_type":"code","metadata":{"id":"4KOPbytzogWG"},"source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import Model\n","from tensorflow.keras.models import Sequential\n","\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, AveragePooling2D\n","from tensorflow.keras.layers import BatchNormalization, Dropout, ZeroPadding2D\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n","\n","from tensorflow.keras.regularizers import l2\n","\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.utils import plot_model\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SF71bN55cZzi"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"S5yg44U8cZzk"},"source":["# ## --- start your code here ----\n","\n","NIM = ??\n","Nama = ??\n","\n","# ## --- end your code here ----"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTMPdtW1mNVv"},"source":["---\n","## 2 - Load CIFAR-10"]},{"cell_type":"code","metadata":{"id":"HJMZF1BZmNVw"},"source":["(X_train_ori, y_train), (X_test_ori, y_test) = tf.keras.datasets.cifar10.load_data()\n","\n","class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c57WBdtqmNVz"},"source":["---\n","## 3 - Split Validation Data"]},{"cell_type":"code","metadata":{"id":"5vuvkKCdmNV1","cellView":"both"},"source":["X_val_ori   = X_train_ori[-10000:,:]\n","y_val       = y_train[-10000:]\n","\n","X_train_ori = X_train_ori[:-10000, :]\n","y_train     = y_train[:-10000]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJZisKPImNV4"},"source":["---\n","## 4 - Normalize and Reshape Data"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"TwP9wbYLmNV5","cellView":"both"},"source":["X_train    = X_train_ori.astype('float32')\n","X_val      = X_val_ori.astype('float32')\n","X_test     = X_test_ori.astype('float32')\n","\n","mean_image = X_train.mean(axis=(0, 1, 2), keepdims=True)\n","std_image  = X_train.std(axis=(0, 1, 2), keepdims=True)\n","\n","X_train    = (X_train - mean_image) /std_image\n","X_val      = (X_val - mean_image) /std_image\n","X_test     = (X_test - mean_image) /std_image\n","\n","X_train    = X_train.astype('float32')\n","X_val      = X_val.astype('float32')\n","X_test     = X_test.astype('float32')\n","\n","print('X_train.shape =',X_train.shape)\n","print('X_val.shape   =',X_val.shape)\n","print('X_test.shape  =',X_test.shape)\n","\n","y_train = y_train.ravel()\n","y_val   = y_val.ravel()\n","y_test  = y_test.ravel()\n","\n","print('\\ny_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GpPgnChbl8jj"},"source":["one hot the label"]},{"cell_type":"code","metadata":{"id":"neQU8KqDIn1_"},"source":["y_train_hot = to_categorical(y_train, 10)\n","y_val_hot   = to_categorical(y_val, 10)\n","y_test_hot  = to_categorical(y_test, 10)\n","\n","print('y_train_hot.shape =',y_train_hot.shape)\n","print('y_val_hot.shape   =',y_val_hot.shape)\n","print('y_test_hot.shape  =',y_test_hot.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4UGIQGBMqFTZ"},"source":["---\n","## 5 - Helper Functions\n","train-val loss and accuracy plot helper function"]},{"cell_type":"code","metadata":{"id":"2Vp0oNNkqrZp"},"source":["def plot_my_history(history):\n","  plt.rcParams['figure.figsize'] = [14, 3.5]\n","  plt.subplots_adjust(wspace=0.2)\n","\n","  plt.subplot(121)\n","  # Plot training & validation accuracy values\n","  plt.plot(history.history['accuracy'])\n","  plt.plot(history.history['val_accuracy'])\n","  plt.title('Model accuracy')\n","  plt.ylabel('Accuracy')\n","  plt.xlabel('Epoch')\n","  plt.legend(['Train', 'Test'])\n","\n","  plt.subplot(122)\n","  # Plot training & validation loss values\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  plt.title('Model loss')\n","  plt.ylabel('Loss')\n","  plt.xlabel('Epoch')\n","  plt.legend(['Train', 'Test'])\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ExtvDLdgNJQ_"},"source":["---\n","# [Part 1] Shallow 4-Layer Neural Net\n","Here you will complete the implementation of 4-Layer Neural Network, just as comparison. "]},{"cell_type":"markdown","metadata":{"id":"Kxp0BuJQly9X"},"source":["---\n","## 1 - Define Model\n","\n","Define the 4-layer Neural net\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PKTnzE8cNdDJ"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","<pre>\n","Implement neuralnet() function\n","the function will return neural net with architecture:\n","    * the input shape is <font color='blue'><b>32x32x3</b></font>  \n","    * Flatten layer\n","    * <font color='blue'><b>200</b></font> neurons in hidden layer 1 and 2 using relu activation\n","    * <font color='blue'><b>100</b></font> neurons in hidden layer 3 also using relu activation\n","    * <font color='blue'><b>10</b></font> neurons in output layer using softmax activation\n","\n"]},{"cell_type":"code","metadata":{"id":"XHF60Qc8qpEd"},"source":["def neuralnet():    \n","    return Sequential([\n","\n","        # flatten input 32x32x3\n","        ??,\n","\n","        # dense 200 relu\n","        ??,\n","\n","        # dense 200 relu\n","        ??,\n","\n","        # dense 100 relu\n","        ??,\n","\n","        # dense 10 softmax\n","        ??, \n","        \n","    ], name=\"neuralnet\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ke5PRP6Ol2Rf"},"source":["---\n","## 2 - Model Summary\n","Visualize the model"]},{"cell_type":"code","metadata":{"id":"MueBsbXW4sJs"},"source":["model_nn = neuralnet()\n","\n","model_nn.summary()  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ozGzjnABcZ0u"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," Total params: 675,910\n"," Trainable params: 675,910\n"," Non-trainable params: 0"]},{"cell_type":"markdown","metadata":{"id":"jV0eJHHll_lA"},"source":["---\n","## 3 - Plot Model\n","\n","Save model architecture as an image"]},{"cell_type":"code","metadata":{"id":"TdQuOH2Hh9qT"},"source":["  plot_model(model_nn, \n","             to_file=model_nn.name+'.png', \n","             show_shapes=True, \n","             show_layer_names=False,\n","             dpi=70\n","            )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rq0voYYUijlW"},"source":["**EXPECTED IMAGE (rotated)**:\n","\n","<center>\n","\n","<img src=\"https://i.ibb.co/pfC2QJR/4layer.png\" height=300>"]},{"cell_type":"markdown","metadata":{"id":"YoOA7l_eZDCo"},"source":["---\n","## 4 - Train Model\n","\n","\n","Run the training process for `10 epochs`, with `batch size = 128`, using `Adam` Optimizer, and `'accuracy'` as its metric"]},{"cell_type":"code","metadata":{"id":"ZjxJaQ96yNvc"},"source":["num_epochs = 10\n","batch_size = 128\n","\n","model_nn.compile(\n","    loss='categorical_crossentropy', \n","    optimizer='adam', \n","    metrics=['accuracy']\n",")\n","\n","\n","history_nn = model_nn.fit(\n","    X_train, y_train_hot, \n","    validation_data=(X_val, y_val_hot),\n","    epochs=num_epochs, \n","    batch_size=batch_size, \n","    verbose=2\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_rqeYZVDeXkb"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," the loss should start around 1.7 and end around 0.9\n"," with training accuracy start around 40% and end around 66%"]},{"cell_type":"markdown","metadata":{"id":"vJ0U_NkcUZ9z"},"source":["---\n","## 5 - Visualize Training History\n","\n","Visualzie the train-validation accuracy"]},{"cell_type":"code","metadata":{"id":"QlJpKQBKUjRV"},"source":["plot_my_history(history_nn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ibLtwIAIn5V"},"source":["---\n","## 6 - Evaluate Model\n","Next, let's evaluate the accuracy of the models that have been trained\n","\n","we should get accuracy around  <font color='blue'>$50\\%$</font>`"]},{"cell_type":"code","metadata":{"id":"VTD6WFU2In5W"},"source":["scores = model_nn.evaluate(X_test, y_test_hot, verbose=0)\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gYNWYSlU4sJ-"},"source":["---\n","---\n","# [Part 2] LeNet-5 (1998)\n","\n","Next you will implement [LeNet-5](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf), the first successful application of convolutional networks, developed for handwritten digits and character recognition by LeCun et al., 1998\n","\n","The architecture consist of 5 layers (2 conv and 3 fc layers). \n","\n","The architecture visualization is as follows\n","\n","<br>\n","\n","<center>\n","<img src=\"https://www.researchgate.net/profile/Vladimir_Golovko3/publication/313808170/figure/fig3/AS:552880910618630@1508828489678/Architecture-of-LeNet-5.png\">"]},{"cell_type":"markdown","metadata":{"id":"DyN4Udn6oJDk"},"source":["---\n","## 1 - Define Model\n","\n","Define the LeNet-5\n","\n","The original network is using **tanh** activation function, \n","\n","but here, we'll use **relu**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0BJQBVv5ntYK"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","<pre>\n","Implement <b>lenet5()</b> function\n","the function will return <b>conv</b>olutional neural net with architecture:    \n","    * the input shape is <font color='blue'><b>32x32x3</b></font>  \n","    * <b>conv</b> layer with <font color='blue'><b>6</b></font> filters of <font color='blue'><b>5x5</b></font>, using relu activation\n","    * <b>average pooling</b> with size <font color='blue'><b>2x2</b></font>\n","    \n","    * <b>conv</b> layer with <font color='blue'><b>16</b></font> filters of <font color='blue'><b>5x5</b></font>, using relu activation\n","    * <b>average pooling</b> with size <font color='blue'><b>2x2</b></font>\n","    * flatten layer\n","    \n","    * <b>dense</b> layer with <font color='blue'><b>120</b></font> neurons using relu activation\n","    * <b>dense</b> layer with <font color='blue'><b>84</b></font> neurons using relu activation\n","    * output <b>dense</b> layer with <font color='blue'><b>10</b></font> neuron using softmax activation\n"]},{"cell_type":"code","metadata":{"id":"nKZKkPjg4sKA"},"source":["def lenet5():\n","    return Sequential([\n","        \n","        # conv 6 (5x5) relu, input 32x32x3\n","        ??,\n","        \n","        # avg pool (2x2)\n","        ??,\n","        \n","        # conv 16 (5x5) relu\n","        ??,\n","        \n","        # avg pool (2x2)\n","        ??,\n","        \n","        # flatten        \n","        ??,\n","        \n","        # dense 120 relu\n","        ??,\n","        \n","        # dense 84 relu\n","        ??,\n","        \n","        # dense 10 softmax\n","        ??,\n","        \n","    ], name=\"lenet5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uw4JEJ6qpKRL"},"source":["---\n","## 2 - Model Summary\n","Visualize the model"]},{"cell_type":"code","metadata":{"id":"yhfcgDLq4sKG"},"source":["model_lenet = lenet5()\n","\n","model_lenet.summary()  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ghuszs9ZuwFC"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," Total params: 62,006\n"," Trainable params: 62,006\n"," Non-trainable params: 0"]},{"cell_type":"markdown","metadata":{"id":"L2G9kfrgpMvj"},"source":["---\n","## 3 - Plot Model\n","\n","Save model architecture as an image"]},{"cell_type":"code","metadata":{"id":"RDZduHW7oy2t"},"source":["plot_model(model_lenet, \n","           to_file=model_lenet.name+'.png', \n","           show_shapes=True, \n","           show_layer_names=False,\n","           rankdir='LR',\n","           dpi=70\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8EzPbCuI14sc"},"source":["**EXPECTED IMAGE**:\n","\n","<center>\n","<img src=\"https://i.ibb.co/n80BRC4/lenet5.png\" width=\"90%\"\">"]},{"cell_type":"markdown","metadata":{"id":"cectvCjuD66q"},"source":["---\n","## 4 - Train Model\n","\n","\n","Run the training process for `10 epochs`, with `batch size = 128`, using `Adam` Optimizer, and `'accuracy'` as its metric"]},{"cell_type":"code","metadata":{"id":"iJE7_3DRppJC"},"source":["num_epochs = 10\n","batch_size = 128\n","\n","model_lenet.compile(\n","    loss='categorical_crossentropy', \n","    optimizer='adam', \n","    metrics=['accuracy']\n",")\n","\n","history_lenet = model_lenet.fit(\n","    X_train, y_train_hot, \n","    validation_data=(X_val, y_val_hot),\n","    epochs=num_epochs, \n","    batch_size=batch_size, \n","    verbose=2\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PI-d7-ZdvUe6"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," the loss should start around 1.7 and end around 0.9\n"," with training accuracy start around 40% and end around 65%"]},{"cell_type":"markdown","metadata":{"id":"UGSSqnOMppJI"},"source":["---\n","## 5 - Visualize Training History\n","\n","Visualize the train-validation accuracy"]},{"cell_type":"code","metadata":{"id":"5VHNw6-bppJL"},"source":["plot_my_history(history_lenet)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q3LK5ynHppJQ"},"source":["---\n","## 6 - Evaluate Model\n","Next, let's evaluate the accuracy of the models that have been trained\n","\n","we should get accuracy around  <font color='blue'>$60\\%$</font>"]},{"cell_type":"code","metadata":{"id":"mRt8iKeDppJR"},"source":["scores = model_lenet.evaluate(X_test, y_test_hot, verbose=0)\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2P3aCHyT4sKL"},"source":["---\n","---\n","# [Part 3] AlexNet (2012)\n","\n","The neural network developed by Krizhevsky, Sutskever, and Hinton in [$2012$](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) was the coming out party for CNNs in the computer vision community. It is widely regarded as one of the most influential publications in the field\n","\n","This was the first time a model performed so well on a historically difficult ImageNet dataset. They win the $2012$ ILSVRC and marked the first year where a CNN was used to achieve a top five test error rate of $15.4%$\n","\n","The network was made up of five conv layers, max-pooling layers, drop-out layers, and three fully connected layers. The network they designed was used for classification with $1,000$ possible categories.\n","\n","Utilizing techniques that are still used today, such as data augmentation and dropout, this paper really illustrated the benefits of CNNs and backed them up with record breaking performance in the competition.\n","\n","<br>\n","\n","<center>\n","<img src='https://miro.medium.com/max/1200/1*wzflNwJw9QkjWWvTosXhNw.png' width=750px>"]},{"cell_type":"markdown","metadata":{"id":"Mk7WEMGIwLWL"},"source":["---\n","## 1 - Define Model\n","\n","Define the AlexNet model\n","\n","The original network is using **custom layer normalization**, \n","\n","but here, we'll use **Batch Normalization**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"19nniRJdwLWT"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Implement alexnet() function\n","    the function will return convolutional neural net with architecture:\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"gVZ3r_09xhim"},"source":["<pre>------------------------------------------------------------------------------------------------------------------------\n","                   1                     |                   2                   |                   3\n","* the input shape is <font color='blue'><b>227x227x3  </b></font>         | * <b>zero padding 2d</b> with size <font color='blue'><b>2x2</b></font>       | * <b>zero padding 2d</b> with size <font color='blue'><b>1x1</b></font>\n","* <b>conv</b> layer with <font color='blue'><b>96</b></font> filters <font color='blue'><b>11x11</b></font>,      | * <b>conv</b> layer with <font color='blue'><b>256</b></font> filters <font color='blue'><b>5x5</b></font>     | * <b>conv</b> layer with <font color='blue'><b>384</b></font> filters of <font color='blue'><b>3x3</b></font> \n","  <font color='blue'><b>stride=4</b></font>, using <b>relu</b> activation        |   using <b>relu</b> activation               |   using <b>relu</b> activation\n","* <b>max pooling</b> with size <font color='blue'><b>3x3</b></font> and <font color='blue'><b>stride=2</b></font> | * <b>max pooling</b> with size <font color='blue'><b>3x3</b></font>, <font color='blue'><b>stride=2</b></font> | \n","* batch normalization layer              | * batch normalization layer           |\n","\n","------------------------------------------------------------------------------------------------------------------------\n","                   4                     |                   5                   |                   6\n","* <b>zero padding 2d</b> with size <font color='blue'><b>1x1</b></font>          | * <b>zero padding 2d</b> with size <font color='blue'><b>1x1</b></font>       | * <b>flatten</b> layer\n","* <b>conv</b> layer with <font color='blue'><b>384</b></font> filters <font color='blue'><b>3x3</b></font>        | * <b>conv</b> layer with <font color='blue'><b>256</b></font> filters <font color='blue'><b>3x3</b></font>     | * <b>dense</b> layer with <font color='blue'><b>4096</b></font> neurons \n","  using <b>relu</b> activation                  |   using <b>relu</b> activation               |   using <b>relu</b> activation\n","                                         | * <b>max pooling</b> with size <font color='blue'><b>3x3</b></font>, <font color='blue'><b>stride=2</b></font> | * <b>dropout</b> layer with <font color='blue'><b>0.5</b></font> probability\n","                                         |                                       |\n","\n","------------------------------------------------------------------------------------------------------------------------\n","                   7                     |                   8                   |\n","* <b>dense</b> layer with <font color='blue'><b>4096</b></font> neurons          | * output <b>dense</b> layer with <font color='blue'><b>1000</b></font> neuron |\n","  using <b>relu</b> activation                  |   using <b>softmax</b> activation            |\n","* <b>dropout</b> layer with <font color='blue'><b>0.5</b></font> probability     |                                       |\n","\n","------------------------------------------------------------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"id":"NLefqzw-wLWX"},"source":["def alexnet():\n","    return Sequential([\n","        # 1 --------------------------------------------------------------------------\n","        # conv2d 96, 11x11, stride=4, relu, input_shape 227x227x3 \n","        # maxpool2d 3x3, stride=2 \n","        # batch normalization \n","        ??,\n","        ??,\n","        ??,\n","        \n","        # 2 --------------------------------------------------------------------------\n","        # zeropadding2d 2x2, \n","\t\t# conv2d 256, 5x5, relu \n","\t\t# maxpool2d 3x3, stride=2\n","\t\t# batch normalization \n","        ??,\n","        ??,\n","        ??,\n","        ??,\n","        \n","        # 3 --------------------------------------------------------------------------\n","\t\t# zero padding 2d 1x1 \n","\t\t# conv2d 384, 3x3, relu \n","        ??,\n","        ??,\n","        \n","        # 4 --------------------------------------------------------------------------\n","\t\t# zeropadding2d 1x1\n","\t\t# conv2d 384, 3x3, relu\n","        ??,\n","        ??,\n","        \n","        # 5 --------------------------------------------------------------------------\n","\t\t# zero padding 2d 1x1 \n","\t\t# conv2d 256, 3x3, relu \n","\t\t# maxpool2d 3x3, stride=2 \n","        ??,\n","        ??,\n","        ??,\n","        \n","        # 6 --------------------------------------------------------------------------\n","\t\t# flatten \n","\t\t# dense 4096, relu \n","\t\t# dropout 0.5 \n","        ??,\n","        ??,\n","        ??,\n","        \n","        # 7 --------------------------------------------------------------------------\n","\t\t# dense 4096, relu \n","\t\t# dropout 0.5  \n","        ??,\n","        ??,\n","        \n","        # 8 --------------------------------------------------------------------------\n","\t\t# dense 1000, softmax \n","        ??,\n","        \n","    ], name=\"alexnet\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dpHydilNwLWg"},"source":["---\n","## 2 - Model Summary\n","Visualize the model"]},{"cell_type":"code","metadata":{"id":"q4LIpkymwLWi"},"source":["model_alex = alexnet()\n","\n","model_alex.summary()  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZUpDJlEwLWo"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," Total params: 62,379,752\n"," Trainable params: 62,379,048\n"," Non-trainable params: 704\n"]},{"cell_type":"markdown","metadata":{"id":"1ahuv3Dy3YMC"},"source":["---\n","## 3 - Plot Model\n","\n","Save model architecture as an image"]},{"cell_type":"code","metadata":{"id":"FN9vTbqB3YMI"},"source":["plot_model(model_alex, \n","           to_file=model_alex.name+'.png', \n","           show_shapes=True, \n","           show_layer_names=False,\n","           rankdir='LR',\n","           dpi=70\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YU6JFH7j5Ayi"},"source":["**EXPECTED IMAGE**:\n","\n","<center>\n","<img src=\"https://i.ibb.co/T4HJfJD/alexnet.png\" width=\"90%\"\">"]},{"cell_type":"markdown","metadata":{"id":"XErlJESQ6X5g"},"source":["---\n","---\n","# [Part 4] Small AlexNet\n","\n","AlexNet was originally designed for ImageNet images with input of $227\\times227\\times3$, which is too big for our **CIFAR-10** dataset\n","\n","Therefore, for now let's define our **small AlexNet** to classify **CIFAR-10** dataset.\n","\n","It has the same **5-conv-3-fc** layer, but with $32\\times32\\times3$ input and $10$ class output"]},{"cell_type":"markdown","metadata":{"id":"XoeiokJg6X5m"},"source":["---\n","## 1 - Define Model\n","\n","Define the Small AlexNet model\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rKnJj5OF6X5r"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Implement alexnet_small() function\n","    the function will return convolutional neural net with architecture:\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"nNIVTJDu6X5t"},"source":["<pre>------------------------------------------------------------------------------------------------------------------------\n","                   1                    |                  2                      |                   3\n","* the input shape is <font color='blue'><b>32x32x3</b></font>            | * <b>conv</b> layer with <font color='blue'><b>96</b></font> filters <font color='blue'><b>5x5</b></font>        | * <b>conv</b> layer with <font color='blue'><b>192</b></font> filters of <font color='blue'><b>3x3</b></font> \n","* <b>conv</b> layer with <font color='blue'><b>48</b></font> filters <font color='blue'><b>7x7</b></font>,       |   padding=<b>same</b>, using <b>relu</b> activation   |   padding=<b>same</b>, using <b>relu</b> \n","  padding=<b>same</b>, using <b>relu</b> activation   | * <b>max pooling</b> with size <font color='blue'><b>2x2, stride=2</b></font>   | \n","* <b>max pooling</b> with size <font color='blue'><b>2x2, stride=2</b></font>   | * batch normalization layer             | \n","* batch normalization layer             |                                         |\n","\n","------------------------------------------------------------------------------------------------------------------------\n","                   4                    |                  5                      |                   6\n","* <b>conv</b> layer with <font color='blue'><b>192</b></font> filters of <font color='blue'><b>3x3</b></font>    | * <b>conv</b> layer with <font color='blue'><b>256</b></font> filters <font color='blue'><b>3x3</b></font>       | * <b>flatten</b> layer\n","  padding=<b>same</b>, using <b>relu</b> activation   |   padding=<b>same</b>, using <b>relu</b> activation   | * <b>dense</b> layer with <font color='blue'><b>512</b></font> neurons \n","                                        | * <b>max pooling</b> with size <font color='blue'><b>2x2, stride=2</b></font>   |   using <b>relu</b> activation\n","                                        |                                         | * <b>dropout</b> layer with <font color='blue'><b>0.5 prob</b></font>\n","                                        |                                         |\n","------------------------------------------------------------------------------------------------------------------------\n","                 7                       |                  8                    |\n","* <b>dense</b> layer with <font color='blue'><b>256</b></font> neurons           | * output <b>dense</b> layer with <font color='blue'><b>10</b></font> neuron   |\n","  using <b>relu</b> activation                  |   using <b>softmax</b> activation            |\n","* <b>dropout</b> layer with <font color='blue'><b>0.5</b></font> probability     |                                       |\n","\n","------------------------------------------------------------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"id":"FEIC3VRf6X5w"},"source":["def alexnet_small():\n","    return Sequential([\n","        # 1 --------------------------------------------------------------------------\n","\t\t# conv2d 48 7x7, padding=same, relu, input shape=32x32x3  \n","\t\t# maxpool2d 2x2, stride=2  \n","\t\t# batch normalization  \n","        ??,\n","        ??,\n","        ??,\n","        \n","        # 2 --------------------------------------------------------------------------\n","\t\t# conv2d 96 5x5, relu, padding=same \n","\t\t# maxpool2d 2x2, stride=2 \n","\t\t# batch normalization \n","        ??,\n","        ??,\n","        ??,\n","        \n","        # 3 --------------------------------------------------------------------------\n","        # conv2d 192 3x3, relu, padding=same \n","        ??,\n","        \n","        # 4 --------------------------------------------------------------------------\n","        # conv2d 192 3x3, relu, padding=same  \n","        ??,\n","        \n","        # 5 --------------------------------------------------------------------------\n","\t\t# conv2d 256 3x3, relu, padding=same  \n","\t\t# maxpool2d 2x2, stride=2  \n","        ??,\n","        ??,\n","        \n","        # 6 --------------------------------------------------------------------------\n","        # flatten\n","\t\t# dense 512, relu \n","\t\t# dropout 0.5 \n","        ??,\n","        ??,\n","        ??,\n","        \n","        # 7 --------------------------------------------------------------------------\n","        # dense 256, relu  \n","\t\t# dropout 0.5  \n","        ??,\n","        ??,\n","        \n","        # 8 --------------------------------------------------------------------------\n","        # dense 10, softmax \n","        ??,\n","        \n","    ], name=\"alexnet_small\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vt7VC-xh6X52"},"source":["---\n","## 2 - Model Summary\n","Visualize the model"]},{"cell_type":"code","metadata":{"id":"4DtG09nE6X53"},"source":["model_alex_small = alexnet_small()\n","\n","model_alex_small.summary()  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3GlMZ6Y_6X58"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," Total params: 3,295,210\n"," Trainable params: 3,294,922\n"," Non-trainable params: 288"]},{"cell_type":"markdown","metadata":{"id":"7_DtRnxG6X5-"},"source":["---\n","## 3 - Plot Model\n","\n","Save model architecture as an image"]},{"cell_type":"code","metadata":{"id":"o7OPPLJ_6X6A"},"source":["plot_model(model_alex_small, \n","           to_file=model_alex_small.name+'.png', \n","           show_shapes=True, \n","           show_layer_names=False,\n","           rankdir='LR',\n","           dpi=70\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Mdtzcr66X6E"},"source":["**EXPECTED IMAGE**:\n","\n","<center>\n","<img src=\"https://i.ibb.co/jfNTjMP/alexnet-small.png\" width=\"90%\"\">"]},{"cell_type":"markdown","metadata":{"id":"BTtvcZv6EETZ"},"source":["---\n","## 4 - Train Model\n","\n","\n","Run the training process for `10 epochs`, with `batch size = 128`, using `Adam` Optimizer, and `'accuracy'` as its metric"]},{"cell_type":"code","metadata":{"id":"AcIrDcfQ_Mi5"},"source":["num_epochs = 10\n","batch_size = 128\n","\n","\n","model_alex_small.compile(\n","    loss='categorical_crossentropy', \n","    optimizer='adam', \n","    metrics=['accuracy'])\n","\n","history_alex_small = model_alex_small.fit(\n","    X_train, y_train_hot, \n","    validation_data=(X_val, y_val_hot),\n","    epochs=num_epochs, \n","    batch_size=batch_size, \n","    verbose=2\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3rSjAsCw_MjB"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," the loss should start around 1.6 and end around 0.4\n"," with training accuracy start around 40% and end around 85%"]},{"cell_type":"markdown","metadata":{"id":"VrAqx7Th_MjD"},"source":["---\n","## 5 - Visualize Training History\n","\n","Visualize the train-validation accuracy"]},{"cell_type":"code","metadata":{"id":"fiddt-AR_MjF"},"source":["plot_my_history(history_alex_small)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aMoS-dE1_MjL"},"source":["---\n","## 6 - Evaluate Model\n","Next, let's evaluate the accuracy of the models that have been trained\n","\n","we should get accuracy around  <font color='blue'>$75\\%$</font>`"]},{"cell_type":"code","metadata":{"id":"hA4FXgqa_MjM"},"source":["scores = model_alex_small.evaluate(X_test, y_test_hot, verbose=0)\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9A3v_zfT4sKX"},"source":["---\n","---\n","# [Part 5] ZF-Net (2013)\n","\n","ZF Net was not only the winner of the competition in $2013$, but also provided great intuition as to the workings on CNNs and illustrated more ways to improve performance. \n","This model achieved an $11.2%$ error rate, which was more of a fine-tuning to the previous AlexNet structure\n","\n","The authors spent a good amount of time explaining a lot of the intuition behind ConvNets and showing how to visualize the filters and weights correctly.\n","\n","The fascinating deconv visualization approach and occlusion experiments described helps not only to explain the inner workings of CNNs, but also provides insight for improvements to network architectures. \n","\n","At this point, Zeiler and Rob Fergus showed the world that CNNs are not black magic which somehow works\n","\n","<br>\n","\n","<center>\n","<img src='https://adeshpande3.github.io/assets/zfnet.png'  width=800px>\n","</center>\n","\n","\n","<br>\n","ZF-Net is very similar architecture to AlexNet except for a few minor modifications.\n","\n","Instead of using $11\\times11$ sized filters in the first layer (which is what AlexNet implemented), ZFNet used filters of size $7\\times7$ and a decreased stride value. \n","\n","The reasoning behind this modification is that a smaller filter size in the first conv layer helps retain a lot of original pixel information in the input volume. \n","\n","A filtering of size $11\\times11$ proved to be skipping a lot of relevant information, especially as this is the first conv layer."]},{"cell_type":"markdown","metadata":{"id":"gxy0fBe-ChCo"},"source":["---\n","## 1 - Define Model\n","\n","Define the ZFNet model\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bPEIc5I4ChCv"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","    Implement zfnet() function\n","    the function will return convolutional neural net with architecture:\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"wbjj_r3hChCz"},"source":["<pre>------------------------------------------------------------------------------------------------------------------------\n","                   1                    |                   2                    |                   3\n","* the input shape is <font color='blue'><b>224x224x3</b></font>          | * <b>conv</b> layer with <font color='blue'><b>256</b></font> filters <font color='blue'><b>5x5 </b></font>     | * <b>conv</b> layer with <font color='blue'><b>384</b></font> filters of <font color='blue'><b>3x3 </b></font>\n","* <b>conv</b> layer with <font color='blue'><b>96</b></font> filters <font color='blue'><b>7x7</b></font>,       |   <font color='blue'><b>strides=2</b></font>, using <b>relu</b> activation     |   padding=<b>same</b>, using <b>relu</b> \n","  <font color='blue'><b>strides=2</b></font>, using <b>relu</b> activation      | * <b>max pooling</b> with size <font color='blue'><b>3x3</b></font>,           | \n","* <b>max pooling</b> with size <font color='blue'><b>3x3</b></font>,            |   <font color='blue'><b>strides=2</b></font>, padding=<b>same</b>              |\n","  <font color='blue'><b>strides=2</b></font>, padding=<b>same</b>               | * <b>batch normalization</b> layer            | \n","* <b>batch normalization</b> layer             |                                        |\n","\n","------------------------------------------------------------------------------------------------------------------------\n","                   4                    |                   5                    |                   6\n","* <b>conv</b> layer with <font color='blue'><b>384</b></font> filters of <font color='blue'><b>3x3</b></font>    | * <b>conv</b> layer with <font color='blue'><b>256</b></font> filters <font color='blue'><b>3x3</b></font>      | * <b>flatten</b> layer\n","  padding=<b>same</b>, using <b>relu</b> activation   |   padding=<b>same</b>, using <b>relu</b> activation  | * <b>dense</b> layer with <font color='blue'><b>4096</b></font> neurons \n","                                        | * <b>max pooling</b> with size <font color='blue'><b>3x3, stride=2</b></font>  |   using <b>relu</b> activation\n","                                        |                                        | * <b>dropout</b> layer with <font color='blue'><b>0.5 prob</b></font>\n","                                        |                                        |\n","------------------------------------------------------------------------------------------------------------------------\n","                 7                      |                  8                     |\n","* <b>dense</b> layer with <font color='blue'><b>4096</b></font> neurons         | * output <b>dense</b> layer with <font color='blue'><b>1000</b></font> neuron  |\n","  using <b>relu</b> activation                 |   using <b>softmax</b> activation             |\n","* <b>dropout</b> layer with <font color='blue'><b>0.5</b></font> probability    |                                        |\n","\n","------------------------------------------------------------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"id":"Xm6bS8hc4sKZ"},"source":["def zfnet():\n","    return Sequential([\n","        \n","        # 1 --------------------------------------------------------------------------\n","\t\t# conv2d 96 7x7, strides=2, relu, input shape=224x224x3 \n","\t\t# maxpool2d 3x3, strides=2, padding=same \n","\t\t# batch normalization \n","        ??,\n","        ??,\n","        ??,\n","\n","        # 2 --------------------------------------------------------------------------\n","\t\t# conv2d 256 5x5 strides=2, relu \n","\t\t# maxpool2d 3x3, strides=2, padding=same \n","\t\t# batch normalization \n","        ??,\n","        ??,\n","        ??,\n","\n","        # 3 --------------------------------------------------------------------------\n","        # conv2d 384 3x3, padding=same, relu \n","        ??,\n","\n","        # 4 --------------------------------------------------------------------------\n","        # conv2d 384 3x3 padding=same, relu \n","        ??,\n","\n","        # 5 --------------------------------------------------------------------------\n","\t\t# conv2d 256 3x3, padding=same, relu \n","\t\t# maxpool2d 3x3, stride=2 \n","        ??,\n","        ??,\n","        \n","        # 6 --------------------------------------------------------------------------\n","\t\t# flatten \n","\t\t# dense 4096 relu \n","\t\t# dropout 0.5 \n","        ??,\n","        ??,\n","        ??,\n","\n","        # 7 --------------------------------------------------------------------------\n","\t\t# dense 4096 relu \n","\t\t# dropout 0.5 \n","        ??,\n","        ??,\n","        \n","        # 8 --------------------------------------------------------------------------\n","        # dense 1000, softmax \n","        ??,\n","        \n","    ], name=\"zfnet\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KAYVonDAEApb"},"source":["---\n","## 2 - Model Summary\n","Visualize the model"]},{"cell_type":"code","metadata":{"id":"sygd4LevCYDX"},"source":["model_zfnet = zfnet()\n","\n","model_zfnet.summary()  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wITWvSiDEDIT"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," Total params: 62,359,016\n"," Trainable params: 62,358,312\n"," Non-trainable params: 704"]},{"cell_type":"markdown","metadata":{"id":"CdsikIwFEIh9"},"source":["---\n","## 3 - Plot Model\n","\n","Save model architecture as an image"]},{"cell_type":"code","metadata":{"id":"fyeHTM0JEIiF"},"source":["plot_model(model_zfnet, \n","           to_file=model_zfnet.name+'.png', \n","           show_shapes=True, \n","           show_layer_names=False,\n","           rankdir='LR',\n","           dpi=70\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZLn-veEpEIiR"},"source":["**EXPECTED IMAGE**:\n","\n","<center>\n","<img src=\"https://i.ibb.co/DMKwLqs/zfnet.png\" width=\"90%\"\">"]},{"cell_type":"markdown","metadata":{"id":"uph60mFi4sKm"},"source":["---\n","---\n","# [Part 6] VGG Net (2014)\n","\n","Simplicity and depth, that's what this network is.\n","\n","[VGG Net](http://arxiv.org/pdf/1409.1556v6.pdf) is one of the most influential papers because it reinforced the notion that convolutional neural networks have to have a deep network of layers in order for this hierarchical representation of visual data to work. \n","\n","It placed 2nd in ILSVRC 2014 with its 7.3% error rate.\n","\n","<br>\n","\n","\n","<center>\n","<img src='http://www.sanko-shoko.net/note.php?img=y1kl'  width=700px>\n","  \n","  </center>\n","  \n","  \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PakYyl5ewVw4"},"source":["\n","The network that was submitted to the competition consist of **5 convolution blocks** and **3 fc layers**. \n","<pre>\n","Keep it deep. Keep it simple.\n","* Each convolution layer uses a <b>3x3</b> filter and <b>padding=1 </b> (same).\n","* Each convolution block ends with maxpooling <b>2x2</b>\n","* Each layer uses relu activation\n","</pre>\n","\n","<center>\n","  <img src='https://www.yjpark.me/assets/expressions/VGG-config.png' width=50%>\n","</center>\n","\n","There are **5 variations** of the network submitted, each with different number of convolution layers in the convolution block. \n","\n","Each network variation is known by name according to the number of actual layers in the network. \n","\n","    E.g.\n","    * VGG16 has 16 layers (13 conv + 3 fc),\n","    * VGG19 has 19 layers (16 conv + 3 fc)\n","    "]},{"cell_type":"markdown","metadata":{"id":"b7i5wIpxIs3u"},"source":["---\n","## 1 - Define VGG16 Model\n","\n","Define the VGG16 model\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yXmiwsIxIs32"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","<pre>\n","Implement <b>vgg16()</b> function\n","the function will return convolutional neural net with architecture:    \n","    * Each convolution layer uses a <font color='blue'><b>3x3</b></font> filter and <b>padding='same'</b>. \n","    * Each convolution block ends with maxpooling <font color='blue'><b>2x2</b></font>\n","    * Each layer uses relu activation\n"]},{"cell_type":"markdown","metadata":{"id":"cLZ_78nfIs37"},"source":["\n","<pre>------------------------------------------------------------------------------------------------------------------------\n","             block 1                               block 2                               block 3\n","* the input shape is <font color='blue'><b>224x224x3</b></font>    | * <font color='blue'><b>2x</b></font> <b>conv</b> layer with <font color='blue'><b>128</b></font> filter     | * <font color='blue'><b>3x</b></font> <b>conv</b> layer with <font color='blue'><b>256</b></font> filter  \n","* <font color='blue'><b>2x</b></font> <b>conv</b> layer with <font color='blue'><b>64</b></font> filter    | * <b>maxpool</b> layer                     |   (4x for vgg19)\n","* <b>maxpool</b> layer                   |                                     | * <b>maxpool</b> layer\n","\n","------------------------------------------------------------------------------------------------------------------------\n","             block 4                               block 5                              FC block 1\n","* <font color='blue'><b>3x</b></font> <b>conv</b> layer with <font color='blue'><b>512</b></font> filter   | * <font color='blue'><b>3x</b></font> <b>conv</b> layer with <font color='blue'><b>512</b></font> filter     | * <b>flatten</b> layer \n","  (4x for vgg19)                  |   (4x for vgg19)                    | * <b>dense</b> layer with <font color='blue'><b>4096</b></font> neurons \n","* <b>maxpool</b> layer                   | * <b>maxpool</b> layer                     | * <b>dropout</b> layer with <font color='blue'><b>0.5 prob</b></font>\n","\n","------------------------------------------------------------------------------------------------------------------------\n","           FC block 2             |                FC block 3           |\n","* <b>dense</b> layer with <font color='blue'><b>4096</b></font> neurons   | * output <b>dense</b> layer <font color='blue'><b>1000</b></font> neuron    |\n","* <b>dropout</b> layer with <font color='blue'><b>0.5 prob</b></font>     |   using <b>softmax</b> activation          |\n","\n","------------------------------------------------------------------------------------------------------------------------\n","\n"]},{"cell_type":"code","metadata":{"id":"xXZVKaLG4sKn"},"source":["def vgg16():\n","    return Sequential([\n","        \n","        # 1-2 (block 1)\n","\t\t# conv2d 64, input shape=224x224x3\n","\t\t# conv2d 64\n","\t\t# maxpool2d  \n","        ??,\n","        ??,\n","        ??,\n","\n","        # 3-4 (block 2)\n","\t\t# conv2d 128 (2x)\n","\t\t# maxpool2d \n","        ??,\n","        ??,\n","        ??,\n","\n","        # 5-7 (block 3)\n","\t\t# conv2d 256 (3x)\n","\t\t# maxpool2d \n","        ??,\n","        ??,\n","        ??,\n","        ??,\n","        \n","        # 8-10 (block 4)\n","\t\t# conv2d 512 (3x)\n","\t\t# maxpool2d \n","        ??,\n","        ??,\n","        ??,\n","        ??,\n","\n","        # 11-13 (block 5)\n","\t\t# conv2d 512 (3x)\n","\t\t# maxpool2d \n","        ??,\n","        ??,\n","        ??,\n","        ??,\n","\n","        # 14\n","\t\t# flatten \n","\t\t# dense 4096 \n","\t\t# dropout 0.5 \n","        ??,\n","        ??,\n","        ??,\n","        \n","        # 15\n","\t\t# dense 4096 \n","\t\t# dropout 0.5 \n","        ??,\n","        ??,\n","        \n","        # 16\n","\t\t# dense 1000, softmax \n","        ??,\n","        \n","    ], name='vgg16')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AmYGz8p7NJyx"},"source":["---\n","## 2 - Model Summary\n","Visualize the model"]},{"cell_type":"code","metadata":{"id":"k9snrFGENJy3"},"source":["model_vgg = vgg16()\n","\n","model_vgg.summary()  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uu217F4vNJzC"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," Total params: 138,357,544\n"," Trainable params: 138,357,544\n"," Non-trainable params: 0"]},{"cell_type":"markdown","metadata":{"id":"RFamHkVyNdl3"},"source":["---\n","## 3 - Plot Model\n","\n","Save model architecture as an image"]},{"cell_type":"code","metadata":{"id":"-HtyXuOlNdl9"},"source":["plot_model(model_vgg, \n","           to_file=model_vgg.name+'.png', \n","           show_shapes=True, \n","           show_layer_names=False,\n","           rankdir='LR',\n","           dpi=70\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nWxJHzB9NdmK"},"source":["**EXPECTED IMAGE**:\n","\n","<center>\n","<img src=\"https://i.ibb.co/85ZnYcn/vgg16.png\" width=\"90%\"\">"]},{"cell_type":"markdown","metadata":{"id":"yZT7-HAWQUpZ"},"source":["---\n","---\n","# [Part 7] Small VGG\n","\n","Just like we did with AlexNet,\n","let's create a vgg_small model to classify the CIFAR-10 dataset\n","\n","but with $32\\times32\\times3$ input and $10$ class output, we can only use **3-conv blocks** and **3-fc** layer, instead of 5-conv blocks. \n","\n","This is due to that the conv block performs maxpool, then after the third max pool, it left us with input size of $4\\times4$"]},{"cell_type":"markdown","metadata":{"id":"6w43UcDZQtwz"},"source":["---\n","## 1 - Define Model\n","\n","Define the Small VGG model\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jRMc6v7gQtw6"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","<pre>\n","    Implement <b>vgg_small()</b> function\n","    the function will return convolutional neural net with architecture:\n","    \n","    * Each convolution layer uses a <font color='blue'><b>3x3</b></font> filter and <b>padding='same'</b>. \n","    * Each convolution block ends with maxpooling <font color='blue'><b>2x2</b></font>\n","    * Each layer uses relu activation\n"]},{"cell_type":"markdown","metadata":{"id":"xuGYzNQfQtw_"},"source":["<pre>------------------------------------------------------------------------------------------------------------------------\n","             block 1                               block 2                               block 3\n","* the input shape is <font color='blue'><b>32x32x3</b></font>      | * <font color='blue'><b>3x</b></font> <b>conv</b> layer with <font color='blue'><b>64</b></font> filter      | * <font color='blue'><b>3x</b></font> <b>conv</b> layer with <font color='blue'><b>128</b></font> filter  \n","* <font color='blue'><b>3x</b></font> <b>conv</b> layer with <font color='blue'><b>32</b></font> filter    | * <b>maxpool</b> layer                     | * <b>maxpool</b> layer\n","* <b>maxpool</b> layer                   |                                     | \n","\n","------------------------------------------------------------------------------------------------------------------------\n","           FC block 1             |               FC block 2            |              FC block 3             \n","* <b>flatten</b> layer                   | * <b>dense</b> layer with <font color='blue'><b>256</b></font> neurons      | * output <b>dense</b> layer <font color='blue'><b>10</b></font> neuron    \n","* <b>dense</b> layer with <font color='blue'><b>512</b></font> neurons    | * <b>dropout</b> layer with <font color='blue'><b>0.5</b></font> prob       |   using <b>softmax</b> activation          \n","* <b>dropout</b> layer with <font color='blue'><b>0.5</b></font> prob     |                                     |\n","\n","------------------------------------------------------------------------------------------------------------------------\n","\n"]},{"cell_type":"code","metadata":{"id":"v6nsxtaYQtxB"},"source":["def vgg_small():\n","    return Sequential([\n","        \n","        # 1-3 (block 1)\n","\t\t# conv2d 32, input shape=32x32x3 \n","\t\t# conv2d 32\n","\t\t# conv2d 32\n","\t\t# maxpool2d \n","        ??,\n","        ??,\n","        ??,\n","        ??,\n","\n","        # 4-6 (block 2)\n","\t\t# conv2d 64 (3x)\n","\t\t# maxpool2d \n","        ??,\n","        ??,\n","        ??,\n","        ??,\n","\n","        # 7-9 (block 3)\n","\t\t# conv2d 128 (3x)\n","\t\t# maxpool2d \n","        ??,\n","        ??,\n","        ??,\n","        ??,\n","\n","\n","        # 10\n","\t\t# flatten \n","\t\t# dense 512 \n","\t\t# dropout 0.5 \n","        ??,\n","        ??,\n","        ??,\n","        \n","        # 11\n","\t\t# dense 256 \n","\t\t# dropout 0.5 \n","        ??,\n","        ??,\n","        \n","        # 12\n","\t\t# dense 10, softmax \n","        ??,\n","        \n","    ], name='vgg16_small')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pk_BQRpHQUpv"},"source":["---\n","## 2 - Model Summary\n","Visualize the model"]},{"cell_type":"code","metadata":{"id":"65ysmJVpQUpx"},"source":["model_vgg_small = vgg_small()\n","\n","model_vgg_small.summary()  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NRaKzEYqQUp2"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," Total params: 1,663,754\n"," Trainable params: 1,663,754\n"," Non-trainable params: 0"]},{"cell_type":"markdown","metadata":{"id":"crBdTnizQUp4"},"source":["---\n","## 3 - Plot Model\n","\n","Save model architecture as an image"]},{"cell_type":"code","metadata":{"id":"xxWYvhHQQUp6"},"source":["plot_model(model_vgg_small, \n","           to_file=model_vgg_small.name+'.png', \n","           show_shapes=True, \n","           show_layer_names=False,\n","           rankdir='LR',\n","           dpi=70\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zdgz5pvtQUp-"},"source":["**EXPECTED IMAGE**:\n","\n","<center>\n","<img src=\"https://i.ibb.co/CwCmfPk/vgg-small.png\" width=\"90%\"\">"]},{"cell_type":"markdown","metadata":{"id":"xk7le9JtCnFr"},"source":["---\n","## 4 - Train Model\n","\n","\n","Run the training process for `10 epochs`, with `batch size = 128`, Adam Optimizer, and `'accuracy'` as its metric"]},{"cell_type":"code","metadata":{"id":"yU5XzzGSQUqC"},"source":["num_epochs = 10\n","batch_size = 128\n","\n","model_vgg_small.compile(loss='categorical_crossentropy', \n","                    optimizer='adam', metrics=['accuracy'])\n","\n","history_vgg_small = model_vgg_small.fit(\n","    X_train, y_train_hot, \n","    validation_data=(X_val, y_val_hot),\n","    epochs=num_epochs, \n","    batch_size=batch_size, \n","    verbose=2\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tuJ35VKiQUqH"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," the loss should start around 1.8 and end around 0.5\n"," with training accuracy start around 30% and end around 83%"]},{"cell_type":"markdown","metadata":{"id":"20phQCe0QUqH"},"source":["---\n","## 5 - Visualize Training History\n","\n","Visualize the train-validation accuracy"]},{"cell_type":"code","metadata":{"id":"dvTvcYJuQUqL"},"source":["plot_my_history(history_vgg_small)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kvFv9R6eQUqP"},"source":["---\n","## 6 - Evaluate Model\n","Next, let's evaluate the accuracy of the models that have been trained\n","\n","we should get accuracy around  <font color='blue'>$75\\%$</font>"]},{"cell_type":"code","metadata":{"id":"k2Qmf6JVQUqR"},"source":["scores = model_vgg_small.evaluate(X_test, y_test_hot, verbose=0)\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v2QlD3RcS5zT"},"source":["**Hey**, it's lower than small AlexNet\n","\n","But you can see that with small AlexNet, the network starts to overfit even with only 10 epochs.\n","\n","However with our small VGG, the gap between train and validation is still small, which may indicates that it can still be trained further"]},{"cell_type":"markdown","metadata":{"id":"t5jgHXyF4sLF"},"source":["---\n","---\n","# [Part 8] GoogLeNet (2014)\n","\n","[GoogLeNet](http://arxiv.org/abs/1512.00567) was one of the first models that introduced the idea that CNN layers didnt always have to be stacked up sequentially. \n","\n","Coming up with the **Inception module**, the authors showed that a creative structuring of layers can lead to improved performance and computationally efficiency. The authors of the paper emphasized that this new model places notable consideration on memory and power usage.\n","\n","This paper has really set the stage for some amazing architectures that we could see in the coming years.\n","\n","<img src='https://adeshpande3.github.io/assets/GoogLeNet.png'>"]},{"cell_type":"markdown","metadata":{"id":"MrB33A2NUR0E"},"source":["The original GoogLeNet uses 9 Inception Module with equivalent with over 100 layers\n","\n","**That's DEEP**\n","\n","open this [link](https://adeshpande3.github.io/assets/GoogleNet.gif) to see the detailed gif of the network\n","<center>\n","<img src='https://adeshpande3.github.io/assets/GoogleNet.gif' width=30%>"]},{"cell_type":"markdown","metadata":{"id":"OQL44rKa3mN0"},"source":["---\n","## 1 - Inception Module\n","\n","Building an Inception Module is quite hairy so let's just build the small version for our CIFAR-10\n","\n","The inception module consist of 4 branches (also called tower) which incorporate three different types of convolution and pooling\n","\n","<center>\n","<img src='https://miro.medium.com/max/1108/1*sezFsYW1MyM9YOMa1q909A.png'>\n","  \n","</center>\n","\n","To implement it using TensorFlow and Keras, it's better if you use Functional API modelling"]},{"cell_type":"markdown","metadata":{"id":"y99Nzfu85gVR"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","<pre>\n","Implement <b>inception()</b> function\n","Use <b>Functional API</b>\n","Each convolution layer uses <b>relu</b> activation and <b>padding=same</b>. \n","\n","with input <b>x</b> and number of filter <b>nF</b>, the module consist of 4 branch:    \n","    * tower 0 = conv with nF filters of 1x1 (x)\n","\n","    * tower 1 = conv with nF filters of 1x1 (x)\n","    * tower 1 = conv with nF filters of 3x3 (tower 1)\n","    \n","    * tower 2 = conv with nF filters of 1x1 (x)\n","    * tower 2 = conv with nF filters of 5x5 (tower 2)\n","    \n","    * tower 3 = maxpool 3x3 stride=1 padding=same (x)\n","    * tower 3 = conv nF, 1x1 (tower 3)\n","    \n","return the concatenation of all four inputs\n","(use tf.keras.layers.concatenate)\n","\n"]},{"cell_type":"code","metadata":{"id":"lKr3ju5mYvg9"},"source":["from tensorflow.keras.layers import concatenate\n","\n","def inception(x, nF):\n","    '''\n","    inputs:\n","    - x : input tensor\n","    - nF: number of convolution filter\n","\n","    '''\n","  \n","    tower_0 = ??        # conv with nF filters of 1x1 (x)\n","           \n","    tower_1 = ??        # conv with nF filters of 1x1 (x)\n","    tower_1 = ??        # conv with nF filters of 3x3 (tower 1)\n","       \n","    tower_2 = ??        # conv with nF filters of 1x1 (x)\n","    tower_2 = ??        # conv with nF filters of 5x5 (tower 2)\n","       \n","    tower_3 = ??        # maxpool 3x3 stride=1 padding=same (x)\n","    tower_3 = ??        # conv nF, 1x1 (tower 3)\n","    \n","    output = concatenate([tower_0, tower_1, tower_2, tower_3], axis = 3)\n","    \n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xiQicQoecLHl"},"source":["---\n","## 2 - Small GoogLeNet Model\n","\n","Define the Small GoogLeNet model\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CAooL7pQ5gwK"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","<pre>\n","Implement <b>googlenet_small()</b> function\n","Use <b>Functional API</b>\n","\n","the function will return convolutional neural net with architecture:\n","    * input_img = Input layer with input shape is 32x32x3 \n","    \n","    * x = conv layer with 32 filters of 3x3, padding=same and relu activation (input_img)\n","    * x = inception module with input x and 32 filters\n","    * x = inception module with input x and 64 filters\n","    * x = max pool layer (x)\n","    * x = inception module with input x and 128 filters\n","    * x = inception module with input x and 128 filters\n","    * x = global average pooling (x)\n","    \n","    * out = dense layer with 10 neurons and softmax activation (x)\n","    \n","define model from input=input_img to output=out\n","    \n"]},{"cell_type":"code","metadata":{"id":"965YpVJJ4sLG"},"source":["def googlenet_small():\n","    \n","    # Input layer\n","    input_img = Input(shape = (32, 32, 3))\n","    \n","    # conv layer with 32 filters of 3x3, padding=same and relu activation\n","    x = ?? (input img)\n","    \n","    # inception module with input x and 32 filters\n","    x = ??\n","  \n","    # inception module with input x and 64 filters\n","    x = ??\n","    \n","    # max pool\n","    x = ?? (x)\n","    \n","    # 2x inception module with input x and 128 filters\n","    x = ??\n","    x = ??\n","    \n","    # global average pooling\n","    x = ?? (x)\n","  \n","    # output dense of 10 class with softmax activation\n","    out = ?? (x)\n","\n","    model = Model(inputs = input_img, outputs = out, name='googlenet_small')\n","    \n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTLZw4uBcG6z"},"source":["---\n","## 3 - Model Summary\n","Visualize the model"]},{"cell_type":"code","metadata":{"id":"rrtdnoPgcG62"},"source":["model_googlenet = googlenet_small()\n","\n","model_googlenet.summary()  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e-kPf10_cG69"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," Total params: 1,726,410\n"," Trainable params: 1,726,410\n"," Non-trainable params: 0"]},{"cell_type":"markdown","metadata":{"id":"eKsaNGYochOD"},"source":["---\n","## 4 - Plot Model\n","\n","Save model architecture as an image"]},{"cell_type":"code","metadata":{"id":"cTttDtV2chOI"},"source":["plot_model(model_googlenet, \n","           to_file=model_googlenet.name+'.png', \n","           show_shapes=True, \n","           show_layer_names=False,\n","           rankdir='LR',\n","           dpi=55\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6S1n69jGclhd"},"source":["**EXPECTED IMAGE**:\n","\n","<center>\n","  \n","\n","<a href=\"https://i.ibb.co/PCjJMf9/googlenet.png\" target=\"_blank\">\n","  <img align=\"center\"  src=\"https://i.ibb.co/PCjJMf9/googlenet.png\"/>\n","</a>\n"]},{"cell_type":"markdown","metadata":{"id":"338s_A-28BZu"},"source":["---\n","## 5 - Train Model\n","\n","\n","Run the training process for `10 epochs`, with `batch size = 128`, using `Adam` Optimizer, and `'accuracy'` as its metric"]},{"cell_type":"code","metadata":{"id":"nmLN8ij1jSyC"},"source":["num_epochs = 10\n","batch_size = 128\n","\n","model_googlenet.compile(\n","    loss='categorical_crossentropy', \n","    optimizer='adam', \n","    metrics=['accuracy']\n",")\n","\n","history_googlenet = model_googlenet.fit(\n","    X_train, y_train_hot, \n","    validation_data=(X_val, y_val_hot),\n","    epochs=num_epochs, \n","    batch_size=batch_size, \n","    verbose=2\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IxQV84wJjSyI"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," the loss should start around 1.6 and end around 0.40\n"," with training accuracy start around 40% and end around 85%\n"," it runs about 90 seconds per epoch"]},{"cell_type":"markdown","metadata":{"id":"1qstAlOTjSyJ"},"source":["---\n","## 6 - Visualize Training History\n","\n","Visualize the train-validation accuracy"]},{"cell_type":"code","metadata":{"id":"7oIjQeSDjSyL"},"source":["plot_my_history(history_googlenet)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zy2JtnnBjSyQ"},"source":["---\n","## 7 - Evaluate Model\n","Next, let's evaluate the accuracy of the models that have been trained\n","\n","we should get accuracy around  <font color='blue'>$79\\%$</font>"]},{"cell_type":"code","metadata":{"id":"3OQemQMzjSyS"},"source":["scores = model_googlenet.evaluate(X_test, y_test_hot, verbose=0)\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Fbx_ErA4sLS"},"source":["---\n","---\n","# [Part 9] Residual Network (2015)\n","\n","3.6% error rate. That itself should be enough to convince you. \n","\n","The [ResNet](https://arxiv.org/abs/1512.03385) model is the best CNN architecture that we currently have and is a great innovation for the idea of residual learning. \n","\n","ResNet is a new 152-layer network architecture that Microsoft Research Asia came up with in late 2015, which set new records in classification, detection, and localization through one incredible architecture\n","\n","\n","<img src='https://adeshpande3.github.io/assets/ResNet.gif' width=300px>"]},{"cell_type":"markdown","metadata":{"id":"IxG7ZE9G7SDn"},"source":["---\n","## 1 - Inner Layer\n","\n","Resnet Inner Layers are the convolution layer inside the skip connection.\n","\n","There are several combinations for the inner layer troughout resnet. \n","\n","The combination involves the position of the convolution, presence of activation, and addition of batch norm"]},{"cell_type":"markdown","metadata":{"id":"4lKEWahr-oZo"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","<pre>\n","Implement <b>resnet_layer()</b> function\n","Use <b>Functional API</b>\n","    * conv = <b>Conv2D</b> layer with number of filter, kernel size, and stride according to the input arguments, padding=same\n","    * add conv layer according to the conv_first argument\n","    * add batchnorm layer according to the batch_normalization argument\n","    * add activation layer according to the activation argument\n","\n"]},{"cell_type":"code","metadata":{"id":"RVzp_wDs4sLS"},"source":["def resnet_layer(inputs, \n","                 num_filters=16,\n","                 kernel_size=3,\n","                 strides=1,\n","                 activation='relu',\n","                 batch_normalization=True,\n","                 conv_first=True):\n","    \n","    conv = Conv2D(num_filters, \n","                  kernel_size=kernel_size,\n","                  strides=strides, \n","                  padding='same',\n","                  kernel_initializer='he_normal',\n","                  kernel_regularizer=l2(1e-4)\n","                 )\n","\n","    # set x as input\n","    x = inputs\n","    \n","    # if conv_first is true, x = conv (x)\n","    if ??:\n","        x = conv(x)\n","        \n","    # if batch_normalization is true, x = BatchNormalization() (x)\n","    if ??:\n","        x = ??(x)\n","        \n","    # if activation is not None, x = Activation(activation) (x)\n","    if ??:\n","        x = ??(x)\n","    \n","    # if conv_first is false, x = conv (x)\n","    if ??:\n","        x = ??(x)\n","        \n","    return x\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m_QRWZxxBDr6"},"source":["---\n","## 2 - Small ResNet Model"]},{"cell_type":"markdown","metadata":{"id":"ondUJE_H-vQR"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","<pre>\n","Implement <b>resnet_small()</b> function\n"]},{"cell_type":"code","metadata":{"id":"AbBsMoj8BHz6"},"source":["def resnet_small(depth = 20):\n","  \n","    input_shape = (32, 32, 3)\n","    \n","    # Start model definition.\n","    num_filters = 16\n","    num_res_blocks = int((depth - 2) / 6)\n","\n","    # add Input layer\n","    inputs = Input(shape=input_shape)\n","    \n","    # call resnet_layer funtion with inputs=inputs\n","    x = ??\n","    \n","    # Instantiate the stack of residual units\n","    for stack in range(3):\n","      \n","        for res_block in range(num_res_blocks):\n","            \n","            strides = 1\n","            \n","            # check if it is the first layer but not first stack\n","            if stack > 0 and res_block == 0:\n","                # downsample for the first layer\n","                strides = 2\n","                \n","            # call resnet_layer function with inputs = x, \n","            # num_filters=num_filters, and strides=strides\n","            y = ??\n","            \n","            # call resnet_layer function with inputs = y, \n","            # num_filters=num_filters, and activation=None\n","            y = ??\n","            \n","            # check if it is the first layer but not first stack\n","            if stack > 0 and res_block == 0:  \n","                # linear projection residual shortcut connection to match\n","                # changed dims\n","              \n","                # call resnet_layer function with inputs = x,\n","                # num_filters=num_filters, kernel_size=1, strides=strides, \n","                # activation=None, and batch_normalization=False\n","                x = ??\n","                \n","            # add layer output with the skip connection\n","            x = tf.keras.layers.add([x, y])\n","        \n","            # add relu activation\n","            x = Activation('relu')(x)\n","            \n","        # double the number of filter\n","        num_filters *= 2\n","\n","    # Add classifier on top.\n","    # v1 does not use BN after last shortcut connection-ReLU    \n","    x = AveragePooling2D(pool_size=8)(x)\n","    \n","    # flatten the output average\n","    y = Flatten()(x)\n","    \n","    # create output dense of 10 class with softmax activation\n","    outputs = Dense(10,\n","                    activation='softmax',\n","                    kernel_initializer='he_normal')(y)\n","\n","    # Instantiate model.\n","    model = Model(inputs=inputs, outputs=outputs, name='resnet_small')\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bMAXmFCz--MY"},"source":["---\n","## 3 - Model Summary\n","Visualize the model"]},{"cell_type":"code","metadata":{"id":"1BSzBEYO4sLY"},"source":["model_resnet = resnet_small()\n","\n","model_resnet.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zam37xZt_BZ5"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," Total params: 274,442\n"," Trainable params: 273,066\n"," Non-trainable params: 1,376"]},{"cell_type":"markdown","metadata":{"id":"5tu-OEKA9iAt"},"source":["---\n","## 4 - Plot Model\n","\n","Save model architecture as an image"]},{"cell_type":"code","metadata":{"id":"YAi-DCZt9iA1"},"source":["plot_model(model_resnet, \n","           to_file=model_resnet.name+'.png', \n","           show_shapes=True, \n","           show_layer_names=False,\n","           rankdir='TB',\n","           dpi=55\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m7cCaJJUpuNr"},"source":["---\n","## 5 - Learning Rate Schedule\n","\n","before training this network, let's create a custom Adam Optimizer and learning rate scheduling that will periodically decrease the learning rate to improve the accuracy andavoid overfitting"]},{"cell_type":"markdown","metadata":{"id":"3hW0gCDgz87z"},"source":["---\n","### a. Scheduling Function"]},{"cell_type":"code","metadata":{"id":"A7eBdBKfzmiU"},"source":["def lr_schedule(epoch):\n","  \n","    # initial learning rate\n","    lr = 1e-3\n","    \n","    # check and reduce the learning rate \n","    # according to the epoch\n","    if epoch > 8:\n","        lr *= 1e-3\n","    elif epoch > 6:\n","        lr *= 1e-2\n","    elif epoch > 4:\n","        lr *= 1e-1\n","        \n","    print('Learning rate: ', lr)\n","    \n","    return lr\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BRHWTZUY1KxS"},"source":["---\n","### b. Scheduling Callback\n","\n","create a scheduling callback based on the scheduling function"]},{"cell_type":"code","metadata":{"id":"vpXzmXSe1I_A"},"source":["from tensorflow.keras.callbacks import LearningRateScheduler\n","\n","lr_callback = LearningRateScheduler(lr_schedule)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2DJEP8nO0_lY"},"source":["---\n","### c. Custom Adam Optimizer\n","\n","create custom Adam Optimizer with initial leraning rate = 0.001"]},{"cell_type":"code","metadata":{"id":"1NgVIyFN1hMw"},"source":["from tensorflow.keras.optimizers import Adam\n","\n","myAdam = tf.keras.optimizers.Adam(1e-3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N0mfpo6lCsmy"},"source":["---\n","## 6 - Train Model\n","\n","\n","Run the training process for `10 epochs`, with `batch size = 128`, Adam Optimizer, and `'accuracy'` as its metric"]},{"cell_type":"code","metadata":{"id":"TppSaHzapuN3"},"source":["num_epochs = 10\n","batch_size = 100\n","\n","model_resnet.compile(\n","    loss='categorical_crossentropy', \n","    optimizer=myAdam, \n","    metrics=['accuracy']\n","    )\n","\n","history_resnet = model_resnet.fit(\n","    X_train, y_train_hot, \n","    validation_data=(X_val, y_val_hot),\n","    epochs=num_epochs, \n","    batch_size=batch_size, \n","    callbacks=[lr_callback],\n","    verbose=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UaJin4sYpuN_"},"source":["**EXPECTED OUTPUT**:\n","<pre>\n"," the loss should start around 1.5 and end around 0.56\n"," with training accuracy start around 50% and end around 86%"]},{"cell_type":"markdown","metadata":{"id":"usMwKoVspuOA"},"source":["---\n","## 7 - Visualize Training History\n","\n","Visualize the train-validation accuracy"]},{"cell_type":"code","metadata":{"id":"egPj2EO5puOC"},"source":["plot_my_history(history_resnet)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tDNiFstvpuOI"},"source":["---\n","## 8 - Evaluate Model\n","Next, let's evaluate the accuracy of the models that have been trained\n","\n","we should get accuracy around <font color='blue'>$77\\%$</font>"]},{"cell_type":"code","metadata":{"id":"Pe4f9rGFpuOJ"},"source":["scores = model_resnet.evaluate(X_test, y_test_hot, verbose=0)\n","\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aQqQ_gcO92ub"},"source":["\n","---\n","\n","# Congratulation, You've Completed Exercise 8\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2020 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"id":"7_Hkr9kg9Do-"},"source":["![footer](https://i.ibb.co/yX0jfMS/footer2020.png)"]}]}