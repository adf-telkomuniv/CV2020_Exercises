{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"CV2020 - 13 - Neural Style Transfer (PyTorch).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"f_ChrQTZyAZU"},"source":["![title](https://i.ibb.co/f2W87Fg/logo2020.png)\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"MQjL2XtX7uKJ"},"source":["<table  class=\"tfo-notebook-buttons\" align=\"left\"><tr><td>\n","    \n","<a href=\"https://colab.research.google.com/github/adf-telkomuniv/CV2020_Exercises/blob/main/CV2020 - 13 - Neural Style Transfer (PyTorch).ipynb\" source=\"blank\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n","</td><td>\n","<a href=\"https://github.com/adf-telkomuniv/CV2020_Exercises/blob/main/CV2020 - 13 - Neural Style Transfer (PyTorch).ipynb\" source=\"blank\" ><img src=\"https://i.ibb.co/6NxqGSF/pinpng-com-github-logo-png-small.png\"></a>\n","    \n","</td></tr></table>"]},{"cell_type":"markdown","metadata":{"id":"1JBASSKXL_y3"},"source":["\n","# Task 13 - Neural Style Transfer (PyTorch)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6q7pjAiUktMI"},"source":["In this notebook we will implement the style transfer technique from [\"Image Style Transfer Using Convolutional Neural Networks\" (Gatys et al., CVPR 2015)](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf).\n","\n","The general idea is to take two images, and produce a new image that reflects the content of one but the artistic \"style\" of the other. We will do this by first formulating a loss function that matches the content and style of each respective image in the feature space of a deep network, and then performing gradient descent on the pixels of the image itself.\n","\n","The deep network we use as a feature extractor is [SqueezeNet](https://arxiv.org/abs/1602.07360), a small model that has been trained on ImageNet. You could use any network, but we chose SqueezeNet here for its small size and efficiency.\n","\n","Here's an example of the images you'll be able to produce by the end of this notebook:\n","\n","<center>\n","  \n","![caption](https://image.ibb.co/gnqJkA/example-styletransfer.png)\n"]},{"cell_type":"markdown","metadata":{"id":"SF71bN55cZzi"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"S5yg44U8cZzk"},"source":["## --- start your code here ----\n","\n","NIM  = ??\n","Nama = ??\n","\n","## --- end your code here ----"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6VAE_sE-Bgk1"},"source":["---\n","---\n","#[Part 0] Import Libraries and Load Data"]},{"cell_type":"code","metadata":{"id":"F2s9tmfkhhTu"},"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as T\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from scipy.ndimage.filters import gaussian_filter1d\n","from PIL import Image\n","from collections import namedtuple\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","SQUEEZENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","SQUEEZENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WmMOQpzthhT3"},"source":["---\n","## 1 - Helper Functions\n","\n","Our pretrained model was trained on images that had been preprocessed by subtracting the per-color mean and dividing by the per-color standard deviation.\n","\n","We provide you with some helper functions to deal with images, since for this part of the assignment we're dealing with real JPEGs, not CIFAR-10 data.\n","\n","We define a few helper functions for performing and undoing this preprocessing. \n","\n","You don't need to do anything in these cells."]},{"cell_type":"markdown","metadata":{"id":"okLO72JrwBCR"},"source":["---\n","### a. Preprocess Image\n","\n","Preprocess an input PIL JPG image to become a Pytorch tensor by normalizing (zero centering) according to `squeezenet` model\n","\n","you need to change the function if you use another pretrained model"]},{"cell_type":"code","metadata":{"id":"5t1E3E9MwZO-"},"source":["# preprocess image input\n","def preprocess(img, size=512):\n","\n","    transform = T.Compose([\n","        T.Resize(size),                              # Resize the image (preserving aspect ratio) until the shortest side is of length `size`\n","        T.ToTensor(),                                # Convert the PIL Image to a Pytorch Tensor.\n","        T.Normalize(mean=SQUEEZENET_MEAN.tolist(),   # Normalize the mean of the image pixel values to be SqueezeNet's expected mean, and\n","                    std=SQUEEZENET_STD.tolist()),    #   the standard deviation to be SqueezeNet's expected std dev.\n","        T.Lambda(lambda x: x[None]),                 # Add a batch dimension in the first position of the tensor: aka, a tensor of shape (H, W, C) will become -> (1, H, W, C).\n","    ])\n","    \n","    return transform(img)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VsYs1KO9wD7Y"},"source":["---\n","### b. De-Process Image\n","Denormalize back Pytorch tensor from the output of CNN model into a PIL JPG image for viewing purpose.\n","\n","It consists of two functions: rescale and the deprocess itself\n","\n","again, you need to change the function if you use another pretrained model"]},{"cell_type":"code","metadata":{"id":"90RVUlwswvkv"},"source":["# Rescale elements of x linearly to be in the interval [0, 1]\n","def rescale(x):\n","\n","    low, high = x.min(), x.max()\n","    x_rescaled = (x - low) / (high - low)\n","\n","    return x_rescaled\n","\n","\n","# deprocess image result\n","def deprocess(img):\n","\n","    transform = T.Compose([\n","        T.Lambda(lambda x: x[0]),                                                         # Remove the batch dimension at the first position by accessing the slice at index 0.\n","        T.Normalize(mean=[0, 0, 0], std=[1.0 / s for s in SQUEEZENET_STD.tolist()]),      # Normalize the standard deviation: multiply each channel of the output tensor by 1/s,\n","        T.Normalize(mean=[-m for m in SQUEEZENET_MEAN.tolist()], std=[1, 1, 1]),          # Normalize the mean: subtract the mean (hence the -m) from each channel of the output tensor\n","        T.Lambda(rescale),                                                                # Rescale all the values in the tensor so that they lie in the interval [0, 1] to prepare for transforming it into image pixel values.\n","        T.ToPILImage(),                                                                   # Convert the Pytorch Tensor to a PIL Image.\n","    ])\n","    \n","    return transform(img)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCnbsppvwD-Q"},"source":["---\n","### c. Relative Error Function\n","\n","Function to calculate difference between your matrix and our expected results"]},{"cell_type":"code","metadata":{"id":"ueEXSBg3hhT4"},"source":["# calculate relative error between two matrices\n","def rel_error(x,y):\n","      return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ChFnN6aNktMf"},"source":["---\n","## 2 - CPU/GPU Setting\n","This is the optional setting if you're using CPU Processing"]},{"cell_type":"code","metadata":{"id":"HkAsz5K2ktMh"},"source":["dtype = torch.cuda.FloatTensor \n","\n","## Uncomment out the following line if you're on a machine with a CPU set up for PyTorch!\n","# dtype = torch.FloatTensor "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DVKcTRxihhUQ"},"source":["---\n","## 3 - Load Images\n","We have provided a few images to try our Style Transfer\n","\n","There are 1 content image and 4 style images provided\n","* content image\n"," * tubingen.jpg\n","* style images\n"," * composition_vii.jpg\n"," * muse.jpg\n"," * starry_night.jpg\n"," * the_scream.jpg\n","\n","you can add more image to your liking\n"]},{"cell_type":"code","metadata":{"id":"FbkK2G9zkM2J"},"source":["# content image\n","!wget -q 'https://github.com/adf-telkomuniv/CV2020_Exercises/raw/main/resources/images/tubingen.jpg'\n","\n","# style images\n","!wget -q 'https://github.com/adf-telkomuniv/CV2020_Exercises/raw/main/resources/images/composition_vii.jpg'\n","!wget -q 'https://github.com/adf-telkomuniv/CV2020_Exercises/raw/main/resources/images/muse.jpg'\n","!wget -q 'https://github.com/adf-telkomuniv/CV2020_Exercises/raw/main/resources/images/starry_night.jpg'\n","!wget -q 'https://github.com/adf-telkomuniv/CV2020_Exercises/raw/main/resources/images/the_scream.jpg'\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C0m1RBxo6NBW"},"source":["---\n","## 4 - Load File Checker\n","\n","we also provided a reference matrix to check whether the matrix produced by your implementation matched our expected output"]},{"cell_type":"code","metadata":{"id":"qWOvhJih6NBa"},"source":["!wget -q 'https://github.com/adf-telkomuniv/CV2020_Exercises/raw/main/resources/style-transfer-checks.npz'\n","\n","answers = dict(np.load('style-transfer-checks.npz'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T2s-U4ST-IFD"},"source":["---\n","## 5 - Library Checking\n","\n","Older versions of `scipy.misc.imresize` yield different results\n","from newer versions, so we check to make sure scipy is up to date."]},{"cell_type":"code","metadata":{"id":"Iq3FvavD-F0t"},"source":["def check_scipy():\n","  \n","    import scipy\n","    vnum = int(scipy.__version__.split('.')[1])\n","    major_vnum = int(scipy.__version__.split('.')[0])\n","    \n","    assert vnum >= 16 or major_vnum >= 1, \"You must install SciPy >= 0.16.0 to complete this notebook.\"\n","\n","print(check_scipy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ccIn5pxA8nLJ"},"source":["**Expected Output**:\n","<pre>\n","None"]},{"cell_type":"markdown","metadata":{"id":"4qjR4RqnhhUJ"},"source":["---\n","---\n","# [Part 1] SqueezeNet Pretrained Model\n","\n","For all of our image generation experiments, we will start with a convolutional neural network which was pretrained to perform image classification on ImageNet. We can use any model here, but for the purposes of this assignment we will use SqueezeNet [1], which achieves accuracies comparable to AlexNet but with a significantly reduced parameter count and computational complexity.\n","\n","Using SqueezeNet rather than AlexNet or VGG or ResNet means that we can easily perform all image generation experiments on CPU.\n","\n","But you can try any other model from PyTorch. See the list [here](https://pytorch.org/docs/stable/torchvision/models.html)\n","\n","[1] Iandola et al, \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5MB model size\", arXiv 2016"]},{"cell_type":"markdown","metadata":{"id":"baK1VzEjNGGJ"},"source":["---\n","## 1 - Load SqueezeNet\n","\n","Download and load the pretrained SqueezeNet model."]},{"cell_type":"code","metadata":{"id":"Vw568akRhhUL"},"source":["# load squeezenet\n","cnn_model = torchvision.models.squeezenet1_1(pretrained=True).features\n","\n","# move the model to the device you are using (GPU or CPU)\n","cnn_model.type(dtype)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5zGYQYoxNNeH"},"source":["---\n","## 2 - Freeze the Model\n","We don't want to train the model, so tell PyTorch not to compute gradients\n","with respect to model parameters.\n","\n","you may see warning regarding initialization deprecated, that's fine, please continue to next steps\n"]},{"cell_type":"code","metadata":{"id":"wF7oWrJpNVED"},"source":["for param in cnn_model.parameters():  \n","    param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h3FFysuWktM1"},"source":["---\n","----\n","# --- Note: Loss Functions ---\n","\n","We're going to compute the **THREE COMPONENTS** of our loss function now. \n","\n","The loss function is a **weighted sum** of three terms: \n","\n","<font size=4><pre>content loss + style loss + total variation loss</pre></font>\n","\n","The weights define the result image whether it'll be generated more inclined toward **content** or **style**.\n","\n","You'll fill in the functions that compute these weighted terms below."]},{"cell_type":"markdown","metadata":{"id":"ZoVjtyNvktM3"},"source":["---\n","---\n","# [Part 2] Content loss\n","We can generate an image that reflects the content of one image and the style of another by incorporating both in our loss function. \n","\n","We want to penalize deviations from the content of the content image and deviations from the style of the style image. \n","\n","We can then use this hybrid loss function to perform gradient descent **not on the parameters** of the model, but instead **ON THE PIXEL VALUES** of our original image.\n","\n","Let's first write the content loss function. "]},{"cell_type":"markdown","metadata":{"id":"AoRn7fY6ktM6"},"source":["---\n","## 1 - Feature maps\n","Content loss measures how much the feature map of the generated image differs from the feature map of the source image. \n","* We only care about the content representation of one layer of the network (say, layer $\\ell$), <br>that has feature maps $A^\\ell \\in \\mathbb{R}^{1 \\times C_\\ell \\times H_\\ell \\times W_\\ell}$.\n","\n","* $C_\\ell$ is the number of filters/channels in layer $\\ell$,\n","\n","* $H_\\ell$ and $W_\\ell$ are the height and width."]},{"cell_type":"markdown","metadata":{"id":"jZdtj0PWktMu"},"source":["---\n","### a. Feature Map Extraction Function\n","We provide this helper code which takes an image, a model (cnn), and returns a list of feature maps, one per layer."]},{"cell_type":"code","metadata":{"id":"EL9Xj0lxktMx"},"source":["def extract_features(x, model):\n","    \"\"\"\n","    Use the CNN mdoel to extract features from the input image x.\n","    \n","    Inputs:\n","    - x     : A PyTorch Tensor of shape (N, C, H, W) \n","              holding a minibatch of images that will be fed to the CNN.\n","    - model : A PyTorch model that we will use to extract features.\n","    \n","    Returns:\n","    - features:\n","        A list of feature for the input images x extracted using the cnn model.\n","        features[i] is a PyTorch Tensor of shape (N, C_i, H_i, W_i); \n","        recall that features from different layers of the network may have\n","        different numbers of channels (C_i) and spatial dimensions (H_i, W_i).\n","    \"\"\"\n","    \n","    features = []\n","    prev_feat = x\n","    \n","    for i, module in enumerate(model._modules.values()):\n","        next_feat = module(prev_feat)\n","        features.append(next_feat)\n","        prev_feat = next_feat\n","        \n","    return features\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ll1EHs7z7kGN"},"source":["---\n","### b. Feature Map from Image\n","Combine the feature extraction function with image loading function"]},{"cell_type":"markdown","metadata":{"id":"cWcFMn5W9pyk"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","complete the implementation of the `features_from_img` function.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"EbB7_gEM1t4o"},"source":["def features_from_img(img_path, img_size, model):\n","  # load image\n","  img = Image.open(img_path)\n","\n","  # call preprocess() function using input img and img_size\n","  img = ??\n","\n","  # move img to the device you are using (GPU or CPU)\n","  # by calling .type() from img with input dtype\n","  img_var = ??\n","  \n","  # call extract_features() function with input img_var and model\n","  feature = ??\n","  \n","  return feature, img_var"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AP_nM3DSktM8"},"source":["---\n","## 2 - Content Loss Function\n","Each row of $F^\\ell$ or $P^\\ell$ represents the vectorized activations of a particular filter, convolved over all positions of the image. \n","\n","Finally, let $w_c$ be the weight of the content loss term in the loss function.\n","\n","Then the content loss is given by:\n","\n","$$\n","L_c = w_c \\times \\sum_{i,j} (F_{ij}^{\\ell} - P_{ij}^{\\ell})^2\n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4aZwXofRXGfp"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","complete the implementation of the `content_loss` function.\n","\n","the code is:\n","* `loss = weights * sum( (current - original)^2 )`\n","* note: use `torch.sum()`\n","* remember how to calculate `power of 2` in python\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"aw3b7HRNktM9"},"source":["def content_loss(content_weight, content_current, content_original):\n","    \"\"\"\n","    Compute the content loss for style transfer.\n","    \n","    Inputs:\n","    - content_weight  : Scalar giving the weighting for the content loss.\n","    - content_current : features of the current image; \n","                        this is a PyTorch Tensor of shape (1, C_l, H_l, W_l).\n","    - content_target  : features of the content image, \n","                        Tensor with shape (1, C_l, H_l, W_l).\n","    \n","    Returns:\n","    - scalar content loss\n","    \"\"\"\n","\n","    # calculate loss = content weight* sum( (content current - content original)^2 )\n","    loss = ??\n","    \n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H-8PFqAdC4dQ"},"source":["---\n","## 3 - Test Content Loss\n","\n","Now, let's check your implementation\n","\n","You should see errors less than 0.0001."]},{"cell_type":"code","metadata":{"id":"z2bJH0UektNJ"},"source":["content_image  = 'tubingen.jpg'\n","image_size     =  192\n","\n","content_layer  = 3\n","content_weight = 6e-2\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P1KGD_9XDN1Y"},"source":["---\n","### a. Extract Content Image\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aorPmaAuI8px"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","Extract Feature from content image"]},{"cell_type":"code","metadata":{"id":"uFSZXycn7J9n"},"source":["# call features_from_img() function with input content_image, image_size, and cnn_model\n","c_feats, content_img_var = ??\n","\n","# get the exact layer feature\n","c_feats = c_feats[content_layer]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DFfrAv-y9D3o"},"source":["---\n","### b. Extract Zeros Image\n"]},{"cell_type":"markdown","metadata":{"id":"Nbb4RwaXI-Xz"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Create a zeros image, then extract its feature as comparison"]},{"cell_type":"code","metadata":{"id":"65SMguBp7d5S"},"source":["# prepare zero image\n","zero_img = torch.zeros(*content_img_var.data.size()).type(dtype)\n","\n","# call extract_features() function with input zero_img and cnn_model\n","z_feats = ??\n","\n","# get the exact layer feature\n","z_feats = z_feats[content_layer]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R3l-ju3XJgrf"},"source":["---\n","### c. Calculate Content Loss"]},{"cell_type":"markdown","metadata":{"id":"v8VS7PZO9ZQx"},"source":["#### <font color='red'>**EXERCISE:** </font>\n"]},{"cell_type":"code","metadata":{"id":"Rukzzbr28-EH"},"source":["# call content_loss() function with input content_weight, z_feats, and c_feats\n","student_output = ??\n","\n","# transfer tensor from GPU to CPU for reading\n","student_output = student_output.cpu().data.numpy()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MxJ2Qefp7YJe"},"source":[" You should see errors less than 0.0001.\n"]},{"cell_type":"code","metadata":{"id":"ouSSTJiS9ADk"},"source":["# calculate difference between expected values and current output\n","error = rel_error(answers['cl_out'], student_output)\n","\n","print('Maximum error is {:.5f}'.format(error))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FInFDNMEktNN"},"source":["**Expected Output**:\n","<pre>\n","Maximum error is 0.00018"]},{"cell_type":"markdown","metadata":{"id":"h_phhZbzktNP"},"source":["---\n","---\n","# [Part 3] Gram Matrix\n","\n","Next, compute the Gram matrix $G$ which represents the correlations between the responses of each filter, where $F$ is as defined above. \n","\n","The Gram matrix is an approximation to the covariance matrix, we want the activation statistics of our generated image to match the activation statistics of our style image, and matching the (approximate) covariance is one way to do that. \n","\n","There are a variety of ways you could do this, but the Gram matrix is nice because it's easy to compute and in practice shows good results.\n"]},{"cell_type":"markdown","metadata":{"id":"JjhMD__Tm3_X"},"source":["---\n","## 1 - Gram Matrix Formula\n","\n","As already defined before, we are going to use similar feature map, which are:\n","* represented by $F^\\ell \\in \\mathbb{R}^{1 \\times C_\\ell \\times H_\\ell \\times W_\\ell}$\n","* $C_\\ell$ is the number of filters/channels in layer $\\ell$, $H_\\ell$ and $W_\\ell$ are the height and width.\n","\n","We will work with reshaped versions of these feature maps that combine all spatial positions into one dimension. \n","* Let $V^\\ell \\in \\mathbb{R}^{C_\\ell \\times M_\\ell}$ be the **unrolled** feature map for the current image \n","* where $M_\\ell=H_\\ell\\times W_\\ell$ is the number of elements in each feature map. \n","\n","<br>\n","\n","Given a feature map $F^\\ell$ that is unrolled into feature maps $V^\\ell$ of shape $(C_\\ell, M_\\ell)$, the Gram matrix has shape $(C_\\ell, C_\\ell)$ and its elements are given by:\n","\n","$$G_{ij}^\\ell  = \\sum_k V^{\\ell}_{ik} V^{\\ell}_{jk}$$\n","\n","Or it can be simplified by calculating the dot product of $V_\\ell$\n","\n","$$\n","G_\\ell = V_\\ell.V_\\ell^T\n","$$"]},{"cell_type":"markdown","metadata":{"id":"wvGnEic6gABH"},"source":["---\n","## 2 - Unroll and Transpose\n","\n","\n","We can see from the formula above that we need a function to `unroll` (`reshape`) and `transpose` a matrix\n","\n","First, let's familiarize ourselves with `reshape` and `transpose` functions in PyTorch, since it's a little bit different than using NumPy"]},{"cell_type":"markdown","metadata":{"id":"1Gy8LN6rktNR"},"source":["---\n","### a. Feature maps\n","\n","For this example, let's define a random feature map below"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"cepjL6wgktNS"},"source":["torch.manual_seed(1)\n","\n","n, c, h, w = 1, 2, 3, 4\n","\n","F = torch.randint(10,(n, c, h, w))\n","\n","print('F shape:', F.shape)\n","print('F value:\\n', F)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Byb4NgdxgVcI"},"source":["**Expected Output**:\n","<pre>\n","F shape: torch.Size([1, 2, 3, 4])\n","F value:\n"," tensor([[[[5, 9, 4, 8],\n","          [3, 3, 1, 1],\n","          [9, 2, 8, 9]],\n","         [[6, 3, 3, 0],\n","          [2, 1, 2, 6],\n","          [0, 3, 6, 4]]]])"]},{"cell_type":"markdown","metadata":{"id":"rMZu7OlbktNY"},"source":["---\n","### b. Unrolled Feature map\n","\n","`reshape` function in PyTorch is called `.view()` function\n","\n","So let's try it\n"]},{"cell_type":"markdown","metadata":{"id":"wVDo8mgPglKv"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","Unroll tensor `F` of shape `1,2,3,4` into `V` of shape `1,2,12`\n"]},{"cell_type":"code","metadata":{"id":"v_nNqSeQktNc"},"source":["unroll_shape = (1,2,12)\n","\n","# call .view() function from tensor F with input unroll_shape\n","V = ??\n","\n","print('V shape:', V.shape)\n","print('V value:\\n', V)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tUiv9wosktNg"},"source":["**Expected Output**:\n","<pre>\n","V shape: torch.Size([1, 2, 12])\n","V value:\n"," tensor([[[5, 9, 4, 8, 3, 3, 1, 1, 9, 2, 8, 9],\n","         [6, 3, 3, 0, 2, 1, 2, 6, 0, 3, 6, 4]]])"]},{"cell_type":"markdown","metadata":{"id":"Mry0dhAFktNh"},"source":["---\n","### c. Transpose Feature map\n","\n","transposing multi-dimensional matrix using PyTorch is as simple as switching the dimension order.\n","\n","in PyTorch, we use `.permute()` function to perform it\n","\n","for example, if we were to transpose a matrix from dimension`[10, 20, 30]` into dimension `[30, 20, 10]`, we use `.permute(2, 1, 0)`\n"]},{"cell_type":"markdown","metadata":{"id":"VTWWhS7oh5mi"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Transpose tensor `V` from shape `[1,2,12]` into `[1,12,2]`\n"]},{"cell_type":"code","metadata":{"id":"43DAFqkZktNi"},"source":["# call .permute() function from tensor V with input 0, 2, and 1\n","Vt = ??\n","\n","print('Vt shape:', Vt.shape)\n","print('Vt value:\\n', Vt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W69cV6TlktNq"},"source":["**Expected Output**:\n","<pre>\n","Vt shape: torch.Size([1, 12, 2])\n","Vt value:\n"," tensor([[[5, 6],\n","         [9, 3],\n","         [4, 3],\n","         [8, 0],\n","         [3, 2],\n","         [3, 1],\n","         [1, 2],\n","         [1, 6],\n","         [9, 0],\n","         [2, 3],\n","         [8, 6],\n","         [9, 4]]])"]},{"cell_type":"markdown","metadata":{"id":"EOxvM9IDktNs"},"source":["---\n","## 3 - Gram Matrix Function\n","\n","\n","\n","Now we calculate the Gram Matrix. \n","\n","Use `.matmul()` function to perform dot product\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"Xq1hNg3SpAfZ"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Complete the gram_matrix() function"]},{"cell_type":"code","metadata":{"id":"Q87R6EfFktNu"},"source":["def gram_matrix(features, normalize=True):\n","    \"\"\"\n","    Compute the Gram matrix from features.\n","    \n","    Inputs:\n","    - features  : PyTorch Tensor of shape (N, C, H, W) giving features \n","                  for a batch of N images.\n","    - normalize : optional, whether to normalize the Gram matrix\n","                  If True, divide the Gram matrix \n","                  by the number of neurons (H * W * C)\n","    \n","    Returns:\n","    - gram  : PyTorch Tensor of shape (N, C, C) giving the\n","              (optionally normalized) Gram matrices for the N input images.\n","    \"\"\"\n","    \n","    N, C, H, W = features.size()\n","    \n","    # unroll features into tensor V, \n","    # use .view() with input shape (N, C, H*W)\n","    V = ??\n","    \n","    # transpose tensor V into Vt by using .permute() with order (0, 2, 1)\n","    Vt = ??\n","\n","\n","    # dot product the tensor V and Vt, \n","    # use .matmul() function from tensor V\n","    gram = ??\n","        \n","    # if normalize is true, divide the Gram matrix by the number of neurons\n","    if normalize:\n","        gram /= (H * W * C)\n","        \n","    return gram\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eUvPDyT7ktNz"},"source":["---\n","## 4 - Test Gram Matrix\n","Test your Gram matrix code. You should see errors less than 0.0001."]},{"cell_type":"markdown","metadata":{"id":"qEKpu59H--Em"},"source":["---\n","### a. Extract Style Image"]},{"cell_type":"markdown","metadata":{"id":"kJv7LoCe_DSn"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","Extract Feature from style image"]},{"cell_type":"code","metadata":{"id":"ZrvWakgM_DTF"},"source":["style_image  = 'starry_night.jpg'\n","style_size   = 192\n","\n","# call features_from_img() function with input style_image, style_size, and cnn_model\n","s_feats, _   = ??\n","\n","# get the exact layer feature\n","s_feats      = s_feats[5].clone()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RX_QhVaW_Tfq"},"source":["---\n","### b. Calculate Gram Matrix"]},{"cell_type":"markdown","metadata":{"id":"CauwR2Y9_Zdx"},"source":["#### <font color='red'>**EXERCISE:** </font>\n"]},{"cell_type":"code","metadata":{"id":"mFSSptaO_ZeN"},"source":["# call gram_matrix() function with input s_feats\n","student_output = ??\n","\n","# transfer tensor from GPU to CPU for reading\n","student_output = student_output.cpu().data.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zl6xxBMK_rWU"},"source":[" You should see errors less than 0.0001.\n"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"xxMAkBmSktN0"},"source":["# calculate difference between expected values and current output\n","error = rel_error(answers['gm_out'], student_output)\n","\n","print('Maximum error is {:.5f}'.format(error))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"grWmtOT9ktN5"},"source":["**Expected Output**:\n","<pre>\n","Maximum error is 0.00000"]},{"cell_type":"markdown","metadata":{"id":"kxcC4neXktOE"},"source":["---\n","---\n","# [Part 4] Style Loss\n","Now we can tackle the style loss. \n"]},{"cell_type":"markdown","metadata":{"id":"SHyOo5WsktOF"},"source":["---\n","## 1 - Style Loss Function\n","\n","Similar to Content Loss, for a given layer $\\ell$, the style loss is defined as follows:\n","\n","\n","Assuming $G^\\ell$ is the Gram matrix from the feature map of the current image, $A^\\ell$ is the Gram Matrix from the feature map of the source style image, and $w_\\ell$ a scalar weight term, then the style loss for the layer $\\ell$ is simply the weighted Euclidean distance between the two Gram matrices:\n","\n","$$L_s^\\ell = w_\\ell \\sum_{i, j} \\left(G^\\ell_{ij} - A^\\ell_{ij}\\right)^2$$\n","\n","In practice we usually compute the style loss at a set of layers $\\mathcal{L}$ rather than just a single layer $\\ell$; then the total style loss is the sum of style losses at each layer:\n","\n","$$L_s = \\sum_{\\ell \\in \\mathcal{L}} L_s^\\ell$$\n"]},{"cell_type":"markdown","metadata":{"id":"qwDiiaFlvWQs"},"source":["---\n","#### <font color='red'>**EXERCISE:** </font>\n","\n","complete the implementation of the `style_loss` function.\n","\n","\n","\n","same as before, the code is:\n","* `loss_layer = weights * sum( (gram G - gram A)^2 )`\n","* note: use `torch.sum()`\n","* remember how to calculate `power of 2` in python\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"HpZbsv2sktOG"},"source":["def style_loss(features, style_layers, gram_targets, style_weights):\n","    \"\"\"\n","    Computes the style loss at a set of layers.\n","    \n","    Inputs:\n","    - features     : List of the features at every layer of the current image, \n","                     as produced by the extract_features() function.\n","    - style_layers : List of layer indices into features giving the layers \n","                     to include in the style loss.\n","    - gram_targets : List of the same length as style_layers,\n","                     where gram_targets[i] is a PyTorch Tensor \n","                     giving the Gram matrix of the source style \n","                     image computed at layer style_layers[i].\n","    - style_weights: List of the same length as style_layers, \n","                     where style_weights[i] is a scalar \n","                     giving the weight for the style loss \n","                     at layer style_layers[i].\n","      \n","    Returns:\n","    - style_loss   : A PyTorch Tensor holding a scalar giving the style loss.\n","    \"\"\"\n","\n","    \n","    loss = torch.tensor(0.).type(dtype)\n","    \n","    for i in range(len(style_layers)):\n","        \n","        # select current feature layer\n","        feature = features[style_layers[i]]\n","        \n","        # calculate gram matrix layer-i by calling gram_matrix() function with input current feature\n","        gram_layer = ??\n","        \n","        # calculate loss of layer-i = style weights[i] * sum((gram_layer - gram_targets[i])^2)\n","        loss_layer = ??\n","        \n","        # add and accumulate the loss_layer into loss\n","        loss = loss + loss_layer\n","        \n","    return loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XVgvstc8ymP7"},"source":["---\n","## 2 - Test Style Loss\n","\n","Now, let's check your implementation\n","\n","You should see errors less than 0.0001."]},{"cell_type":"code","metadata":{"id":"0NaQ3FZrktOM"},"source":["content_image   = 'tubingen.jpg'\n","image_size      =  192\n","\n","style_image     = 'starry_night.jpg'\n","style_size      = 192\n","style_layers    = [1, 4, 6, 7]\n","style_weights   = [300000, 1000, 15, 3]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_sxBH-x08Fs"},"source":["---\n","### a. Extract Content and Style Image\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ap-Zx1sD08F3"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","Extract Feature from content image and style image"]},{"cell_type":"code","metadata":{"id":"tSVdAtR908F-"},"source":["# call features_from_img() function with input content_image, image_size, and cnn_model\n","c_feats, _ = ??\n","\n","# call features_from_img() function with input style_image, style_size, and cnn_model\n","s_feats, _ = ??\n","\n","style_targets = []\n","for idx in style_layers:\n","    style_targets.append(gram_matrix(s_feats[idx].clone()))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4eeDojjc1gYP"},"source":["---\n","### b. Calculate Style Loss"]},{"cell_type":"markdown","metadata":{"id":"iS9KEUoC1gYV"},"source":["#### <font color='red'>**EXERCISE:** </font>\n"]},{"cell_type":"code","metadata":{"id":"j0RhsKGo1gYa"},"source":["# call content_loss() function with input c_feats, style_layers, style_targets, and style_weights\n","student_output = ??\n","\n","# transfer tensor from GPU to CPU for reading\n","student_output = student_output.cpu().data.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PN-VighW2D-u"},"source":[" You should see errors less than 0.0001.\n"]},{"cell_type":"code","metadata":{"id":"cfLG_Q3u176S"},"source":["# calculate difference between expected values and current output\n","error = rel_error(answers['sl_out'], student_output)\n","\n","print('Maximum Error is {:.5f}'.format(error))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"39uT1NnEktOO"},"source":["**Expected Output**:\n","<pre>\n","Maximum error is 0.00002"]},{"cell_type":"markdown","metadata":{"id":"roZ8fBjfktOP"},"source":["---\n","---\n","# [Part 5] Total-Variation regularization\n","It turns out that it's helpful to also encourage smoothness in the image. We can do this by adding another term to our loss that penalizes wiggles or **\"total variation\"** in the pixel values. \n"]},{"cell_type":"markdown","metadata":{"id":"XaXshX1MyVg3"},"source":["---\n","## 1 - Total-Variation Math\n","\n","\n","You can compute the \"total variation\" as the sum of the squares of differences in the pixel values for all pairs of pixels that are next to each other (horizontally or vertically). \n","\n","Here we sum the total-variation regualarization for each of the 3 input channels (RGB), and weight the total summed loss by the total variation weight, $w_t$ with\n","\n","<br>\n","\n","$$\n","\\begin{align*}\n","H_v & =  \\sum_{c=1}^3\\sum_{i=1}^{H-1}\\sum_{j=1}^{W} (x_{i+1,j,c} - x_{i,j,c})^2\\\\\\\\\n","W_v & =  \\sum_{c=1}^3\\sum_{i=1}^{H}\\sum_{j=1}^{W - 1} (x_{i,j+1,c} - x_{i,j,c})^2\\\\\\\\\n","L_{tv} & = w_t \\times (H_v + W_v)\n","\\end{align*}\n","$$\n","\n","<br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"z5HYF1k1yf5s"},"source":["---\n","## 2 - Total-Variation Detailed\n","\n","In the next cell, fill in the definition for the TV loss term. \n","\n","To receive full credit, your implementation should not have any loops. So, let's break it down\n","\n","Note that the matrix shape is `N, C, H, W`, thus\n","\n","<table width=90%>\n","  <tr><th width=12%>Var</th><th width=35%>Math</th><th>Equals to</th><th>Explanation</th></tr>\n","  <tr>\n","    <td align=center><font size=3><pre>x_ipjc</pre></font></td>\n","    <td align=center><font size=4> \n","$$x_{(i+1,\\ j,\\ c)} \\ \\ \\substack{| \\ c:\\ 1..&3\\\\| \\ i:\\ 2..&H\\\\| \\ j:\\ 1..&W}$$\n","     </font></td>\n","    <td align=center><font size=3><pre>x[:, :,<font color='red'> 1:</font>, :]</pre></font></td>\n","    <td><pre>all N,<br>all channel C,<br>all width W,<br>height from 2 to H</pre></td>\n","  </tr>    \n","  <tr>\n","    <td align=center><font size=3><pre>xh</pre></font></td>\n","    <td align=center><font size=4>\n","$$x_{(i,\\ j,\\ c)}\\ \\ \\ \\ \\ \\substack{| \\ c:\\ 1..&3\\\\| \\ i:\\ 1..&H-1\\\\| \\ j:\\ 1..&W}$$ \n","    </font></td>\n","    <td align=center><font size=3><pre>x[:, :, <font color='red'>:-1</font>, :]</pre></font></td>\n","    <td><pre>all N,<br>all channel C,<br>all width W,<br>height from 1 to H-1</pre></td>\n","  </tr>    \n","  <tr>\n","    <td align=center><font size=3><pre>x_ijpc</pre></font></td>\n","    <td align=center><font size=4>\n","$$ x_{(i,\\ j+1,\\ c)} \\ \\ \\substack{| \\ c:\\ 1..&3\\\\| \\ i:\\ 1..&H\\\\| \\ j:\\ 2..&W}$$ \n","    </font></td>\n","    <td align=center><font size=3><pre>x[:, :, :, <font color='red'>1:</font>] </pre></font></td>\n","    <td><pre>all N,<br>all channel C,<br>all height H,<br>width from 2 to W</pre></td>\n","  </tr>    \n","  <tr>\n","    <td align=center><font size=3><pre>xw</pre></font></td>\n","    <td align=center><font size=4>\n","$$x_{(i,\\ j,\\ c)}\\ \\ \\ \\ \\ \\substack{| \\ c:\\ 1..&3\\\\| \\ i:\\ 1..&H\\\\| \\ j:\\ 1..&W-1}\n","    $$ \n","    </font></td>\n","    <td align=center><font size=3><pre>x[:, :, :, <font color='red'>:-1</font>]</pre></font></td>\n","    <td><pre>all N,<br>all channel C,<br>all height H,<br>width from 1 to W-1</pre></td>\n","  </tr>    \n","</table>\n","\n","<br>\n"]},{"cell_type":"markdown","metadata":{"id":"FuzvwBOA6q-F"},"source":["---\n","## 3 - Total-Variation Loss Function\n","\n","\n","Then the variation will be:\n","\n","    H_v = sum( (x_ipjc - xh)^2 )\n","\n","    W_v = sum( (x_ijpc - xw)^2 )\n","\n","    loss = weights * (H_v + W_v)`"]},{"cell_type":"markdown","metadata":{"id":"P5y1vTsQ6uIE"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Complete the Total-Variation Loss `tv_loss()` function\n","\n","\n","\n","note: use `torch.sum()`"]},{"cell_type":"code","metadata":{"id":"Aw4J96voktOQ"},"source":["def tv_loss(img, tv_weight):\n","    \"\"\"\n","    Compute total variation loss.\n","    \n","    Inputs:\n","    - img       : PyTorch Variable of shape (1, 3, H, W)\n","                  holding an input image.\n","    - tv_weight : Scalar giving the weight w_t to use for the TV loss.\n","    \n","    Returns:\n","    - loss      : PyTorch Variable holding a scalar \n","                  giving the total variation loss for img weighted by tv_weight.\n","    \"\"\"\n","    \n","    ##grab image img in range 1..N, 1..C, 2..H, 1..W\n","    x_ipjc = img[??, ??, ??, ??]\n","\n","    ##grab image img in range 1..N, 1..C, 1..H-1, 1..W\n","    xh = img[??, ??, ??, ??]\n","\n","    ##grab image img in range 1..N, 1..C, 1..H, 2..W\n","    x_ijpc = ??\n","\n","    ##grab image img in range  1..N, 1..C, 1..H, 1..W-1\n","    xw = ??\n","\n","  \n","    # call torch.sum() function with input as the math defined above\n","    H_v = ??\n","    \n","    # call torch.sum() function with input as the math defined above\n","    W_v = ??\n","    \n","    # calculate total loss as the math defined above\n","    loss = ??\n","    \n","    return loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UYJbbfK5ktOa"},"source":["---\n","## 4 - Test Total-Variation Loss\n","Test your TV loss implementation. Error should be less  than 0.0001."]},{"cell_type":"code","metadata":{"id":"L1qcFhzUktOe"},"source":["content_image = 'tubingen.jpg'\n","image_size    =  192\n","\n","tv_weight     = 2e-2\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HZ9ZGqAu8wKt"},"source":["---\n","### a. Extract Content Image\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x7lbMsjL8wKw"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","Extract Feature from content image"]},{"cell_type":"code","metadata":{"id":"UHuSP55k8yGx"},"source":["img = Image.open(content_image)\n","\n","# call preprocess() function with input img and size=image_size\n","content_img = ??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FylCwZGN9Fi-"},"source":["---\n","### b. Calculate Total-Variation Loss"]},{"cell_type":"markdown","metadata":{"id":"O-Wit7Ia9FjH"},"source":["#### <font color='red'>**EXERCISE:** </font>\n"]},{"cell_type":"code","metadata":{"id":"ZjgsBMvB9FjN"},"source":["# call tv_loss() function with input content_img and tv_weight\n","student_output = ??\n","\n","\n","# transfer tensor from GPU to CPU for reading\n","student_output = student_output.cpu().data.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"19hyYcWU8V90"},"source":["Error should be less  than 0.0001."]},{"cell_type":"code","metadata":{"id":"l3YEspA48VAB"},"source":["# calculate difference between expected values and current output\n","error = rel_error(answers['tv_out'], student_output)\n","\n","print('Error is {:.5f}'.format(error))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dhyu0uV2ktOg"},"source":["**Expected Output**:\n","<pre>\n","Maximum error is 0.00000"]},{"cell_type":"markdown","metadata":{"id":"k9nFLt-19cvp"},"source":["---\n","---\n","# [Part 6] Neural Style Transfer\n","\n","Now we're ready to string it all together \n"]},{"cell_type":"markdown","metadata":{"id":"1YM1eMguktOh"},"source":["---\n","## 1 - Style Transfer Function\n"]},{"cell_type":"markdown","metadata":{"id":"_wbLgNYUp151"},"source":["#### <font color='red'>**EXERCISE:** </font>\n","\n","Complete the `style_transfer()` function\n"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"4K9756-BktOj"},"source":["def style_transfer(model,\n","                   content_path, style_path, \n","                   content_size, style_size, \n","                   content_layer, content_weight,\n","                   style_layers, style_weights, \n","                   tv_weight, init_random = False, iterations=200):\n","    \"\"\"\n","    Run style transfer!\n","    \n","    Inputs:\n","    - content_path   : model used for style transfer\n","    - content_path   : filename of content image\n","    - style_path     : filename of style image\n","    - content_size   : size of smallest image dimension \n","                       (used for content loss and generated image)\n","    - style_size     : size of smallest style image dimension\n","    - content_layer  : layer to use for content loss\n","    - content_weight : weighting on content loss\n","    - style_layers   : list of layers to use for style loss\n","    - style_weights  : list of weights to use for each layer in style_layers\n","    - tv_weight      : weight of total variation regularization term\n","    - init_random    : initialize the starting image to uniform random noise\n","    \"\"\"\n","    \n","    # Extract features for the content image    \n","    # call features_from_img() function with input content_path, content_size, and model\n","    feats, content_img = ??\n","\n","    # initialize content target\n","    content_target     = feats[content_layer].clone()\n","\n","    # Extract features for the style image\n","    # call features_from_img() function with input style_path, style_size, and model\n","    feats, style_img = ??  \n","\n","    # initialize style targets\n","    style_targets  = []\n","    for idx in style_layers:\n","        style_targets.append(gram_matrix(feats[idx].clone()))\n","\n","    # Initialize output image to content image or noise\n","    if init_random:\n","        img = torch.Tensor(content_img.size()).uniform_(0, 1).type(dtype)\n","    else:\n","        img = content_img.clone().type(dtype)\n","\n","    # We do want the gradient computed on our image!\n","    img.requires_grad_()\n","    \n","    # Set up optimization hyperparameters\n","    initial_lr  = 3.0\n","    decayed_lr  = 0.1\n","    decay_lr_at = 180\n","\n","    # Note that we are optimizing the pixel values of the image by passing\n","    # in the img Torch tensor, whose requires_grad flag is set to True\n","    optimizer = torch.optim.Adam([img], lr=initial_lr)\n","    \n","    f, axarr = plt.subplots(1,2)\n","    axarr[0].axis('off')\n","    axarr[1].axis('off')\n","    axarr[0].set_title('Content Source Img.')\n","    axarr[1].set_title('Style Source Img.')\n","    axarr[0].imshow(deprocess(content_img.cpu()))\n","    axarr[1].imshow(deprocess(style_img.cpu()))\n","    plt.show()\n","    plt.figure()\n","    \n","    for t in range(iterations):\n","        if t < 190:\n","            img.data.clamp_(-1.5, 1.5)\n","        optimizer.zero_grad()\n","\n","        # extract feature from img\n","        # call extract_features() function with input img and model\n","        feats = ??\n","        current_content = feats[content_layer]\n","        \n","        # Compute loss\n","        # call content_loss() function with input content_weight, current_content, and content_target\n","        c_loss = ??\n","\n","        # call style_loss() function with input feats, style_layers, style_targets, and style_weights\n","        s_loss = ??\n","\n","        # call tv_loss() function with input img and tv_weight\n","        t_loss = ??\n","\n","        # sum all three losses\n","        loss = ??\n","        \n","        # perform backward pass\n","        loss.backward()\n","\n","        # Perform gradient descents on our image values\n","        if t == decay_lr_at:\n","            optimizer = torch.optim.Adam([img], lr=decayed_lr)\n","        optimizer.step()\n","\n","        if t % 100 == 0:\n","            print('Iteration {}'.format(t))\n","            plt.axis('off')\n","            plt.imshow(deprocess(img.data.cpu()))\n","            plt.show()\n","            \n","    print('Iteration {}'.format(t))\n","    plt.axis('off')\n","    plt.imshow(deprocess(img.data.cpu()))\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vkmU0SWlktOm"},"source":["---\n","## 2 - Generate Style Transfer\n","\n","Try out `style_transfer` on the three different parameter sets below. Make sure to run all three cells. Feel free to add your own, but make sure to include the results of style transfer on the third parameter set (starry night) in your submitted notebook.\n","\n","<br>\n","\n","<table width=90%>\n","\t<tr><th width=25%>Params</th><th>Notes</th></tr><tr>\n","\t\t<td align='center'><font size=3><pre>content_path</pre></font></td>\n","\t\t<td><font size=3>filename of content image.</font></td>\n","\t</tr><tr>\n","\t\t<td align='center'><font size=3><pre>style_path</pre></font></td>\n","\t\t<td><font size=3>filename of style image.</font></td>\n","\t</tr><tr>\n","\t\t<td align='center'><font size=3><pre>image_size</pre></font></td>\n","\t\t<td><font size=3>size of smallest image dimension of the content image  <br>\n","\t\t(used for content loss and generated image).</font></td>\n","\t</tr><tr>\n","\t\t<td align='center'><font size=3><pre>style_size</pre></font></td>\n","\t\t<td><font size=3>size of smallest style image dimension.</font></td>\n","\t</tr><tr>\n","\t\t<td align='center'><font size=3><pre>content_layer</pre></font></td>\n","\t\t<td><font size=3>specifies which layer to use for content loss.</font></td>\n","\t</tr><tr>\n","\t\t<td align='center'><font size=3><pre>content_weight</pre></font></td>\n","\t\t<td><font size=3>gives weighting on content loss in the overall loss function. <br>\n","\t\tIncreasing the value of this parameter will make the final image  <br>\n","\t\tlook more realistic (closer to the original content).</font></td>\n","\t</tr><tr>\n","\t\t<td align='center'><font size=3><pre>style_layers</pre></font></td>\n","\t\t<td><font size=3>specifies a list of which layers to use for style loss. </font></td>\n","\t</tr><tr>\n","\t\t<td align='center'><font size=3><pre>style_weights</pre></font></td>\n","\t\t<td><font size=3>specifies a list of weights to use for each layer in style_layers <br>\n","\t\t(each of which will contribute a term to the overall style loss).  <br><br>\n","\t\tWe generally use higher weights for the earlier style layers  <br>\n","\t\tbecause they describe more local/smaller scale features,  <br>\n","\t\twhich are more important to texture than features over larger receptive fields.  <br><br>\n","\t\tIn general, increasing these weights will make the resulting image  <br>\n","\t\tlook less like the original content and more distorted  <br>\n","\t\ttowards the appearance of the style image.</font></td>\n","\t</tr><tr>\n","\t\t<td align='center'><font size=3><pre>tv_weight</pre></font></td>\n","\t\t<td><font size=3>specifies the weighting of total variation regularization  <br>\n","\t\tin the overall loss function. Increasing this value makes  <br>\n","\t\tthe resulting image look smoother and less jagged,  <br>\n","\t\tat the cost of lower fidelity to style and content. </font></td>\n","\t</tr>\n","</table>\n","\n","<br>\n","\n","Below the next three cells of code, feel free to copy and paste the parameters to play around them and see how the resulting image changes. "]},{"cell_type":"markdown","metadata":{"id":"DcmaMXX5vfH-"},"source":["---\n","### a. Composition VII + Tubingen"]},{"cell_type":"code","metadata":{"id":"uQdNAjqbktOm"},"source":["plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","params1 = {\n","    'model'         : cnn_model,\n","    \n","    'content_path'  : 'tubingen.jpg',\n","    'style_path'    : 'composition_vii.jpg',\n","    \n","    'content_size'  : 192,\n","    'style_size'    : 512,\n","    \n","    'content_layer' : 3,\n","    'content_weight': 5e-2, \n","    \n","    'style_layers'  : (1, 4, 6, 7),\n","    'style_weights' : (20000, 500, 12, 1),\n","    \n","    'tv_weight'     : 5e-2\n","}\n","\n","style_transfer(**params1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3Af11x8vlCc"},"source":["---\n","### b. Scream + Tubingen"]},{"cell_type":"code","metadata":{"id":"RkeaqR31ktOo"},"source":["params2 = {\n","    'model'         : cnn_model,\n","    \n","    'content_path'  :'tubingen.jpg',\n","    'style_path'    :'the_scream.jpg',\n","    \n","    'content_size'  :192,\n","    'style_size'    :224,\n","    \n","    'content_layer' :3,\n","    'content_weight':3e-2,\n","    \n","    'style_layers'  :[1, 4, 6, 7],\n","    'style_weights' :[200000, 800, 12, 1],\n","    \n","    'tv_weight'     :2e-2\n","}\n","\n","style_transfer(**params2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eQ2MXMiJvnil"},"source":["---\n","### c. Starry Night + Tubingen"]},{"cell_type":"code","metadata":{"id":"dgqqBgSAktOt"},"source":["params3 = {\n","    'model'         : cnn_model,\n","    \n","    'content_path'  : 'tubingen.jpg',\n","    'style_path'    : 'starry_night.jpg',\n","    \n","    'content_size'  : 192,\n","    'style_size'    : 192,\n","    \n","    'content_layer' : 3,\n","    'content_weight': 6e-2,\n","    \n","    'style_layers'  : [1, 4, 6, 7],\n","    'style_weights' : [300000, 1000, 15, 3],\n","    \n","    'tv_weight'     : 2e-2\n","}\n","\n","style_transfer(**params3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"scrolled":true,"id":"LRcW3I2ektOu"},"source":["---\n","---\n","# [Part 7] Feature Inversion + Texture Synthesis\n","\n","You know that the Style Transfer is actually a combination of both **Feature Inversion** of the content and **Texture Synthesis** of the style\n","\n"]},{"cell_type":"markdown","metadata":{"id":"A2ACgk-nxmOo"},"source":["---\n","## 1 - Feature Inversion\n","\n","Feature Inversion as explained in [1] attempts to reconstruct an image from its extracted feature representation.\n","\n","Now, if you set the style weights to all be $0$ and initialize the starting image to random noise instead of the content source image, you'll reconstruct an image from the feature representation of the content source image. \n","\n","You're starting with total noise, but you should end up with something that looks quite a bit like your original image.\n","\n","\n","[1] Aravindh Mahendran, Andrea Vedaldi, \"Understanding Deep Image Representations by Inverting them\", CVPR 2015"]},{"cell_type":"markdown","metadata":{"id":"AlUE7mAOxwgB"},"source":["\n","Run the following cell to try out feature inversion from Starry Night + Tubingen."]},{"cell_type":"code","metadata":{"id":"xDCTq0mzktOv"},"source":["params_inv = {\n","    'model'         : cnn_model,\n","    \n","    'content_path'  : 'tubingen.jpg',\n","    'style_path'    : 'starry_night.jpg',\n","    \n","    'content_size'  : 192,\n","    'style_size'    : 192,\n","    \n","    'content_layer' : 3,\n","    'content_weight': 6e-2,\n","    \n","    'style_layers'  : [1, 4, 6, 7],\n","    'style_weights' : [0, 0, 0, 0],  # we discard any contributions from style to the loss\n","    \n","    'tv_weight'     : 2e-2,\n","    'init_random'   : True,          # we want to initialize our image to be random\n","    'iterations'    : 300\n","}\n","\n","style_transfer(**params_inv)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ecT-qzFdxzU_"},"source":["---\n","## 2 - Texture Synthesis\n","\n","Similarly, you could do \"texture synthesis\" from scratch if you set the content weight to $0$ and initialize the starting image to random noise"]},{"cell_type":"markdown","metadata":{"id":"rLYKBuq_yO6-"},"source":["\n","Run the following cell to try out texture synthesis from Starry Night + Tubingen."]},{"cell_type":"code","metadata":{"id":"vfhr4_3LBBI1"},"source":["params_synt = {\n","    'model'         : cnn_model,\n","    \n","    'content_path'  : 'tubingen.jpg',\n","    'style_path'    : 'starry_night.jpg',\n","    \n","    'content_size'  : 192,\n","    'style_size'    : 192,\n","    \n","    'content_layer' : 3,\n","    'content_weight': 0,             # we discard any contributions from content to the loss\n","    \n","    'style_layers'  : [1, 4, 6, 7],\n","    'style_weights' : [300000, 1000, 15, 3],\n","    \n","    'tv_weight'     : 2e-2,\n","    'init_random'   : True,          # we want to initialize our image to be random\n","    'iterations'    : 300\n","}\n","\n","style_transfer(**params_synt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xuujlTQYhhVW"},"source":["---\n","---\n","\n","# Congratulation, You've Completed Exercise 13\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2020 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"id":"7_Hkr9kg9Do-"},"source":["![footer](https://i.ibb.co/yX0jfMS/footer2020.png)"]}]}