{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CV2020 - 02 - Image Features.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"281px"},"toc_section_display":true,"toc_window_display":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"Nde_vGtp9Dlr"},"source":["![title](https://i.ibb.co/f2W87Fg/logo2020.png)\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"oUkeuLWzoHoN"},"source":["<table  class=\"tfo-notebook-buttons\" align=\"left\"><tr><td>\n","    \n","<a href=\"https://colab.research.google.com/github/adf-telkomuniv/CV2020_Exercises/blob/master/CV2020 - 02 - Image Features.ipynb\" source=\"blank\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n","</td><td>\n","<a href=\"https://github.com/adf-telkomuniv/CV2020_Exercises/blob/master/CV2020 - 02 - Image Features.ipynb\" source=\"blank\" ><img src=\"https://i.ibb.co/6NxqGSF/pinpng-com-github-logo-png-small.png\"></a>\n","    \n","</td></tr></table>"]},{"cell_type":"markdown","metadata":{"id":"pqfEjzA09Dlx"},"source":["# Task 2 - Image Classification via Feature Space\n","\n","We have seen that we can achieve reasonable performance on an image classification task by training a linear classifier on the pixels of the input image. In this exercise we will show that we can improve our classification performance by training linear classifiers not on raw pixels but on features that are computed from the raw pixels.\n"]},{"cell_type":"markdown","metadata":{"id":"pMlC1Kwa9Dl0"},"source":["---\n","---\n","## 0 - Import necessary libraries and informations"]},{"cell_type":"code","metadata":{"id":"SWJMf_Kx9Dl3"},"source":["import numpy as np\n","import sklearn\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","from scipy.ndimage import uniform_filter\n","from sklearn.metrics import accuracy_score\n","%matplotlib inline\n","\n","np.set_printoptions(precision=8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M8molz1J9Dl-"},"source":["Write down your Name and Student ID"]},{"cell_type":"code","metadata":{"id":"CkzQo8MY9DmB"},"source":["## --- start your code here ----\n","\n","NIM  = 123456\n","Nama = \"\"\n","\n","## --- end your code here ----"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FNXQVKWr9DmH"},"source":["---\n","---\n","## 1 - Load CIFAR-10 Dataset\n","\n","* First, Obtain Cifar-10 dataset.\n","  There are from various source in Internet like [Keras](https://keras.io/datasets/), [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/datasets), or any other source\n","* Next you will prepare the dataset by first:\n"," * visualizing data\n"," * split into training, validation, and testing set\n"," * normalize data"]},{"cell_type":"markdown","metadata":{"id":"FJ4WkMiR9DmI"},"source":["---\n","### a. Import Data ***CIFAR-10***"]},{"cell_type":"code","metadata":{"id":"tsjp0Rou9DmL"},"source":["import tensorflow as tf\n","\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'forse', 'ship', 'truck']\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a_drJAPL9DmR"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"0xkyOKsX9DmV"},"source":["print('X_train.shape =',X_train.shape)\n","print('y_train.shape =',y_train.shape)\n","print('X_test.shape  =',X_test.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SFmi0aMw9Dmb"},"source":["**Expected Output**: \n","<pre>\n","X_train.shape = (50000, 32, 32, 3)\n","y_train.shape = (50000, 1)\n","X_test.shape  = (10000, 32, 32, 3)\n","y_test.shape  = (10000, 1)\n"]},{"cell_type":"markdown","metadata":{"id":"6YNXtE039Dmd"},"source":["---\n","### b. Visualizing Data\n","\n","\n","Show the first 20 images from X_train"]},{"cell_type":"code","metadata":{"id":"oh3fg51s9Dmf"},"source":["fig, ax = plt.subplots(2,10,figsize=(15,4.5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for j in range(0,2):\n","    for i in range(0, 10):\n","        ax[j,i].imshow(X_train[i+j*10])\n","        ax[j,i].set_title(classes[y_train[i+j*10,0]])\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HAKLJdJK9Dml"},"source":["---\n","### c. Split Training Data\n","\n","Cut the `last 10000 data` from `Training Set`, and save it as `Validation Set`"]},{"cell_type":"code","metadata":{"id":"lQ8nhpld9Dmm"},"source":["X_train = X_train.astype('float32')\n","X_test  = X_test.astype('float32')\n","\n","X_val = X_train[-10000:,:]\n","y_val = y_train[-10000:]\n","\n","X_train = X_train[:-10000, :]\n","y_train = y_train[:-10000]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JYUmaL9WG73T"},"source":["---\n","---\n","## 2 - Feature Extraction Functions\n","\n","For each image we will compute a Histogram of Oriented Gradients (HOG) as well as a color histogram using the hue channel in HSV color space. We form our final feature vector for each image by concatenating the HOG and color histogram feature vectors.\n","\n","Here we already defined for you the implementation of HOG and Color Histogram. Read it carefully to understand it.\n","\n","The `hog_feature` and `color_histogram_hsv` functions both operate on a single\n","image and return a feature vector for that image. The `extract_features`\n","function takes a set of mages and a list of feature functions and evaluates\n","each feature function on each image, storing the results in a matrix where\n","each column is the concatenation of all feature vectors for a single image."]},{"cell_type":"markdown","metadata":{"id":"TUVf8eiIHEha"},"source":["---\n","### a. Histogram of Oriented Gradients\n","\n","Roughly speaking, HOG should capture the texture of the image while ignoring color information. The code below is a modified from [skimage.feature.hog](http://pydoc.net/Python/scikits-image/0.4.2/skimage.feature.hog)\n","\n","Read further: Histograms of Oriented Gradients for Human Detection, Navneet Dalal and Bill Triggs, CVPR 2005"]},{"cell_type":"code","metadata":{"id":"apqhhGCAIhVy"},"source":["def rgb2gray(rgb):\n","    return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xD5nPp0nHc7L"},"source":["def hog_feature(im):\n","\n","    # convert rgb to grayscale if needed\n","    if im.ndim == 3:\n","        image = rgb2gray(im)\n","    else:\n","        image = np.at_least_2d(im)\n","\n","    sx, sy = image.shape # image size\n","    orientations = 9 # number of gradient bins\n","    cx, cy = (8, 8) # pixels per cell\n","\n","    gx = np.zeros(image.shape)\n","    gy = np.zeros(image.shape)\n","\n","    # compute gradient on x-direction\n","    gx[:, :-1] = np.diff(image, n=1, axis=1) \n","    # compute gradient on y-direction\n","    gy[:-1, :] = np.diff(image, n=1, axis=0) \n","\n","    # gradient magnitude\n","    grad_mag = np.sqrt(gx ** 2 + gy ** 2) \n","\n","    # gradient orientation\n","    grad_ori = np.arctan2(gy, (gx + 1e-15)) * (180 / np.pi) + 90 \n","\n","    n_cellsx = int(np.floor(sx / cx))  # number of cells in x\n","    n_cellsy = int(np.floor(sy / cy))  # number of cells in y\n","\n","    # compute orientations integral images\n","    orientation_histogram = np.zeros((n_cellsx, n_cellsy, orientations))\n","\n","    for i in range(orientations):\n","        # create new integral image for this orientation\n","        # isolate orientations in this range\n","        temp_ori = np.where(grad_ori < 180 / orientations * (i + 1),\n","                            grad_ori, 0)\n","        temp_ori = np.where(grad_ori >= 180 / orientations * i,\n","                            temp_ori, 0)\n","        \n","        # select magnitudes for those orientations\n","        cond2 = temp_ori > 0\n","        temp_mag = np.where(cond2, grad_mag, 0)\n","        orientation_histogram[:,:,i] = uniform_filter(temp_mag, size=(cx, cy))[round(cx/2)::cx, round(cy/2)::cy].T\n","\n","    return orientation_histogram.ravel()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7M-eaRVjIOKT"},"source":["---\n","### b. Color Histogram\n","\n","Compared to HOG, the color histogram represents the color of the input image while ignoring texture. As a result, we expect that using both together ought to work better than using either alone. Verifying this assumption would be a good thing to try for your own interest."]},{"cell_type":"code","metadata":{"id":"1TqWdFMWIXKv"},"source":["def color_histogram_hsv(im, nbin=10, xmin=0, xmax=255, normalized=True):\n","\n","    ndim = im.ndim\n","    bins = np.linspace(xmin, xmax, nbin+1)\n","\n","    hsv = matplotlib.colors.rgb_to_hsv(im/xmax) * xmax\n","    imhist, bin_edges = np.histogram(hsv[:,:,0], bins=bins, density=normalized)\n","\n","    imhist = imhist * np.diff(bin_edges)\n","\n","    return imhist\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"olhYWNjdIlOD"},"source":["---\n","### c. Extract Feature Helper Function\n","\n","Given pixel data for images and several feature functions that can operate on single images, apply all feature functions to all images, concatenating the feature vectors for each image and storing the features for all images in a single matrix. This function returns an array of shape (N, F_1 + ... + F_k) where each column is the concatenation of all features for a single image."]},{"cell_type":"code","metadata":{"id":"b7VF6IIUIoaq"},"source":["def extract_features(imgs, feature_fns, verbose=False):\n"," \n","    num_images = imgs.shape[0]\n","    if num_images == 0:\n","        return np.array([])\n","\n","    # Use the first image to determine feature dimensions\n","    feature_dims = []\n","    first_image_features = []\n","    for feature_fn in feature_fns:\n","        feats = feature_fn(imgs[0].squeeze())\n","        assert len(feats.shape) == 1, 'Feature functions must be one-dimensional'\n","        feature_dims.append(feats.size)\n","        first_image_features.append(feats)\n","\n","    # Now that we know the dimensions of the features, we can allocate a single\n","    # big array to store all features as columns.\n","    total_feature_dim = sum(feature_dims)\n","    imgs_features = np.zeros((num_images, total_feature_dim))\n","    imgs_features[0] = np.hstack(first_image_features).T\n","\n","    # Extract features for the rest of the images.\n","    for i in range(1, num_images):\n","        idx = 0\n","        for feature_fn, feature_dim in zip(feature_fns, feature_dims):\n","            next_idx = idx + feature_dim\n","            imgs_features[i, idx:next_idx] = feature_fn(imgs[i].squeeze())\n","            idx = next_idx\n","        if verbose and i % 1000 == 999:\n","            print('Done extracting features for %d / %d images' % (i+1, num_images))\n","\n","    return imgs_features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ua5-gt3KI8GO"},"source":["---\n","---\n","## 3 - Extract and Normalize Feature"]},{"cell_type":"markdown","metadata":{"id":"g0ui8xcKJBjk"},"source":["---\n","### a. Extract Features"]},{"cell_type":"code","metadata":{"id":"sm7JqSmMJEB0"},"source":["num_color_bins = 10 # Number of bins in the color histogram\n","\n","feature_fns   = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\n","\n","X_train_feats = extract_features(X_train, feature_fns, verbose=True)\n","X_val_feats   = extract_features(X_val, feature_fns)\n","X_test_feats  = extract_features(X_test, feature_fns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9fyDx053JUVb"},"source":["---\n","### b. Subtract Mean"]},{"cell_type":"markdown","metadata":{"id":"AuFKuseSa6Ws"},"source":["##### <font color='red'>**EXERCISE**: </font>\n","* Normalize `X_train_feats`, `X_val_feats`, and `X_test_feats` by *zero-centering* them:\n","    * calculate the `mean` of training data `X_train_feats`\n","    * subtract `X_train_feats`, `X_val_feats`, and `X_test_feats` using mean of `X_train_feats`\n","    * use `np.mean()` with `axis=0` and `keepdims=True` to calculate the standard deviation"]},{"cell_type":"code","metadata":{"id":"FcpIfZc89Dm3"},"source":["mean_feat      = ??\n","\n","X_train_feats -= ??\n","X_val_feats   -= ??\n","X_test_feats  -= ??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MOMpLNHT9Dm6"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"BWC1p_HN9Dm7"},"source":["print('np.mean(X_train_feats) =',np.mean(X_train_feats))\n","print('np.mean(X_val_feats)   =',np.mean(X_val_feats))\n","print('np.mean(X_test_feats)  =',np.mean(X_test_feats))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GnveLwD19Dm-"},"source":["**Expected Output**: \n","<pre>np.mean(X_train_feats) = -1.8848760935890657e-16\n","np.mean(X_val_feats)   = -0.007372060535654032\n","np.mean(X_test_feats)  = -0.002616707164937622"]},{"cell_type":"markdown","metadata":{"id":"_fXQbAIwJlQ8"},"source":["---\n","### c. Divide by STD\n","\n","Divide by standard deviation. This ensures that each feature has roughly the same scale.\n"]},{"cell_type":"markdown","metadata":{"id":"CALCCCR0bDF0"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* Divide `X_train_feats`, `X_val_feats`, and `X_test_feats` by the standard deviation of `X_train_feats`\n","* use `np.std()` with `axis=0` and `keepdims=True` to calculate the standard deviation"]},{"cell_type":"code","metadata":{"id":"4uZxB58qJlQ_"},"source":["std_feat       = ??\n","\n","X_train_feats  = ??\n","X_val_feats    = ??\n","X_test_feats   = ??\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xTO4D1V8JlRJ"},"source":["Check your implementation"]},{"cell_type":"code","metadata":{"id":"EHbZu6U0JlRK"},"source":["print('np.mean(X_train_feats) =',np.mean(X_train_feats))\n","print('np.mean(X_val_feats)   =',np.mean(X_val_feats))\n","print('np.mean(X_test_feats)  =',np.mean(X_test_feats))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PIDtm7IjJlRR"},"source":["**Expected Output**: \n","<pre>np.mean(X_train_feats) = -2.6402660721516865e-16\n","np.mean(X_val_feats)   = -0.004088128603644509\n","np.mean(X_test_feats)  = -0.0009327328073588954"]},{"cell_type":"markdown","metadata":{"id":"rhc2VSMs9Dm_"},"source":["---\n","### d. Stack feature data"]},{"cell_type":"code","metadata":{"id":"w-H-e8_c9Dm_"},"source":["X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])\n","X_val_feats   = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])\n","X_test_feats  = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aEzfB_UGPY8t"},"source":["---\n","---\n","##4 - Normalize RAW Pixel Data\n","\n","Let's also normalize the raw pixel data as comparison"]},{"cell_type":"code","metadata":{"id":"J-5dk8QdPkTt"},"source":["mean_image = np.mean(X_train, axis = 0)\n","X_train -= mean_image\n","X_val   -= mean_image\n","X_test  -= mean_image\n","\n","X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]*X_train.shape[3]))\n","X_val   = X_val.reshape((X_val.shape[0],X_val.shape[1]*X_val.shape[2]*X_val.shape[3]))\n","X_test  = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]*X_test.shape[3]))\n","\n","y_train = y_train.ravel()\n","y_val   = y_val.ravel()\n","y_test  = y_test.ravel()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YWdxD4yY9DnS"},"source":["---\n","---\n","## 5 - Linear Function\n","These parts are exactly the same as previous exercise,\n","so we'll provide you the result"]},{"cell_type":"markdown","metadata":{"id":"BCWkIzxS9DnU"},"source":["---\n","### a. Forward Function\n"]},{"cell_type":"code","metadata":{"id":"r5NBU3R09DnU"},"source":["def forward(x, W, b):  \n","    v = np.dot(x, W) + b\n","    return v"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vVopw_gk9Dnd"},"source":["---\n","### b. Backward Function"]},{"cell_type":"code","metadata":{"id":"zbgHRRHX9Dne"},"source":["def backward(dout, x, W, b):\n","    \n","    dW = np.dot(x.T,dout)\n","    db = np.sum(dout, axis=0, keepdims=True)\n","    dx = dout.dot(W.T)\n","    \n","    return dW, db, dx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dsBraIv69Dnu"},"source":["---\n","---\n","## 6 - Loss Functions"]},{"cell_type":"markdown","metadata":{"id":"ZlkgZgQp9Dn4"},"source":["---\n","### a. Softmax Loss"]},{"cell_type":"code","metadata":{"id":"mi1S0uZp9Dn5"},"source":["def softmax_loss(score, y):   \n","    \n","    score -= np.max(score)\n","    score_exp = np.exp(score)\n","    score_sum = np.sum(score_exp, axis = 1, keepdims = True)\n","    score = score_exp / score_sum\n","\n","    num_examples = score.shape[0]\n","        \n","    number_list = range(num_examples)\n","    corect_logprobs = -np.log(score[number_list,y])\n","    loss = np.sum(corect_logprobs)/num_examples\n","\n","    dscores = score\n","    dscores[range(num_examples),y] -= 1\n","    dscores /= num_examples\n","\n","    return loss, dscores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0NwIDkX09Doz"},"source":["---\n","### b. SVM Loss"]},{"cell_type":"code","metadata":{"id":"mp767_w69Do0"},"source":["def svm_loss(score, y):\n","    num_examples = score.shape[0]\n","    \n","    correct_scores = np.choose(y,score.T)\n","    margin = score.T - correct_scores + 1\n","    margin = np.maximum(0,margin)\n","    loss_i = np.sum(margin) - num_examples\n","    loss = loss_i/num_examples\n","    dscores = (margin.T > 0).astype(float)\n","    marginsSum = dscores.sum(1) - 1\n","    dscores[range(dscores.shape[0]), y] = -marginsSum\n","        \n","    return loss, dscores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vl2_OZBK9DoE"},"source":["---\n","---\n","## 7 - Linear Classifier\n"]},{"cell_type":"markdown","metadata":{"id":"_KO3lGVv9DoF"},"source":["---\n","### a. Weight Initialization"]},{"cell_type":"code","metadata":{"id":"HsDrps-i9DoF"},"source":["def initialize_weights(n, d, std):\n","    \n","    W = std * np.random.randn(n, d)\n","    b = np.zeros((1,d))\n","    \n","    return W, b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ITTqUp4c9DoJ"},"source":["---\n","### b. Training Function"]},{"cell_type":"code","metadata":{"id":"ISpmp2Oj9DoJ"},"source":["def train(X, y, W=None, b=None, learning_rate=1e-6, reg=1e4, num_iters=100, batch_size=200, verbose=False, loss_fn=softmax_loss):\n","    \n","    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n","    num_train, dim = X.shape\n","    \n","    if W is None:\n","        W, b = initialize_weights(dim, num_classes, 0.001)\n","\n","    loss_history = []\n","                     \n","    for it in range(num_iters):\n","        X_batch = None\n","        y_batch = None\n","\n","        train_rows = np.arange(num_train)\n","        idxs = np.random.choice(train_rows, batch_size, replace=False)\n","  \n","        X_batch = X[idxs]\n","        y_batch = y[idxs]\n","\n","        scores = forward(X_batch, W, b)        \n","        loss, dout = loss_fn(scores, y_batch)\n","        loss_history.append(loss)\n","\n","        dW, db, _ = backward(dout, X_batch, W, b)\n","        dW += reg*W\n","        \n","        W -= learning_rate * dW\n","        b -= learning_rate * db\n","        \n","        if verbose and it % 100 == 0:\n","            print ('iteration', it,'/',num_iters, ': loss =', loss)\n","    return loss_history, W, b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z6iAfJWB9DoU"},"source":["---\n","### c. Predict Function"]},{"cell_type":"code","metadata":{"id":"DqxCYfon9DoU"},"source":["def predict(X, W, b):    \n","\n","    y_pred = np.zeros(X.shape[1])\n","    y_pred = forward(X, W, b)\n","    y_pred = y_pred.argmax(axis=-1)\n","    \n","    return y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p2aCpTqkQOS5"},"source":["---\n","---\n","##8 - Train Softmax "]},{"cell_type":"markdown","metadata":{"id":"eSC5w5Bw9Dof"},"source":["---\n","###a. Train on Raw Pixel\n","* Find the best Learning Rate and Regularization Strength\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Bv71C-pAbVbl"},"source":["##### <font color='red'>**EXERCISE**: </font>     \n","* experiment with different ranges for the learning rates and regularization strengths; "]},{"cell_type":"code","metadata":{"id":"IZSdjo7V9Dof"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","np.random.seed(None)\n","\n","results = {}\n","best_val = -1\n","best_W = None\n","best_b = None\n","learning_rates = [1e-7, 5e-7]\n","regularization_strengths = [9e4, 1e5]\n","\n","iterations = 2000\n","\n","# Greedily loop over learning_rates and regularization_strengths\n","for rate in learning_rates:\n","    for reg in regularization_strengths:\n","        print('Running {} iterations, rate = {}, reg = {}'.format(iterations, rate, reg))\n","        # call train function using the learning rate and regularization selected\n","        loss, W, b = train(X_train, y_train, \n","                           learning_rate=rate, reg=reg,\n","                           num_iters=iterations, verbose=False,\n","                           loss_fn=softmax_loss)\n","        \n","        # call predict function using pretrained W and b on X_train and X_val to evaluate\n","        y_train_pred = predict(??, ??, ??)\n","        y_val_pred   = predict(??, ??, ??)\n","        \n","        # calculate the accuracy\n","        train_accuracy = np.mean(y_train == y_train_pred)\n","        val_accuracy = np.mean(y_val == y_val_pred)\n","        \n","        print ('rate = {}, reg = {}, test accuracy = {}, validation accuracy = {}'.format(\n","            rate, reg, train_accuracy, val_accuracy))\n","        \n","        # store the result accuracy combination\n","        results[(rate, reg)] = (train_accuracy, val_accuracy)\n","        \n","        # store the best Weight and Bias\n","        if val_accuracy > best_val:\n","            best_W = W\n","            best_b = b\n","            best_val = val_accuracy\n","\n","\n","    \n","# Print out results.\n","for lr, reg in sorted(results):\n","    train_accuracy, val_accuracy = results[(lr, reg)]\n","    print('lr',lr,',reg',reg,', train accuracy: ',train_accuracy*100,'%, val accuracy: ', val_accuracy ,'%')\n","    \n","print()\n","print('best validation accuracy achieved during cross-validation:',best_val*100,'%')\n","print('best Weights and bias are stored in `best_W` and `best_b`')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nvoX3R9b9Doi"},"source":["**Expected Output**:\n","\n","if you are careful you should be able to get a classification accuracy of **over 35%** on the validation set\n"]},{"cell_type":"markdown","metadata":{"id":"7mAms-8F9Doi"},"source":["---\n","###b. Test the Trained Weights"]},{"cell_type":"code","metadata":{"id":"2rxRWuP59Doj","scrolled":true},"source":["y_pred   = predict(X_test, best_W, best_b)\n","accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n","\n","print('Testing Accuracy =', accuracy*100,'%')\n","print('Test label      =',y_test[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UMEZgROgQp2P"},"source":["---\n","###c. Train on Features\n","* Find the best Learning Rate and Regularization Strength\n","* * Train the Classifier with Softmax Loss on top of the features extracted above; this should achieve better results than training Softmax directly on top of raw pixels.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8zz2HG0PbZgj"},"source":["##### <font color='red'>**EXERCISE**: </font>     \n","* experiment with different ranges for the learning rates and regularization strengths; \n","* You might also want to play with different numbers of bins in the color histogram."]},{"cell_type":"code","metadata":{"id":"GFEf_Y2rQp2R"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","np.random.seed(None)\n","\n","results = {}\n","best_val = -1\n","best_W_feats = None\n","best_b_feats = None\n","learning_rates = [1e-7, 5e-7]\n","regularization_strengths = [9e4, 1e5]\n","\n","iterations = 2000\n","\n","# Greedily loop over learning_rates and regularization_strengths\n","for rate in learning_rates:\n","    for reg in regularization_strengths:\n","        print('Running {} iterations, rate = {}, reg = {}'.format(iterations, rate, reg))\n","        # call train function using the learning rate and regularization selected\n","        loss, W, b = train(X_train_feats, y_train, \n","                           learning_rate=rate, reg=reg,\n","                           num_iters=iterations, verbose=False,\n","                           loss_fn=softmax_loss)\n","        \n","        # call predict function using pretrained W and b on X_train_feats and X_val_feats to evaluate\n","        y_train_pred = predict(??, ??, ??)\n","        y_val_pred   = predict(??, ??, ??)\n","        \n","        # calculate the accuracy\n","        train_accuracy = np.mean(y_train == y_train_pred)\n","        val_accuracy = np.mean(y_val == y_val_pred)\n","        \n","        print ('rate = {}, reg = {}, test accuracy = {}, validation accuracy = {}'.format(\n","            rate, reg, train_accuracy, val_accuracy))\n","        \n","        # store the result accuracy combination\n","        results[(rate, reg)] = (train_accuracy, val_accuracy)\n","        \n","        # store the best Weight and Bias\n","        if val_accuracy > best_val:\n","            best_W_feats = W\n","            best_b_feats = b\n","            best_val = val_accuracy\n","\n","\n","    \n","# Print out results.\n","for lr, reg in sorted(results):\n","    train_accuracy, val_accuracy = results[(lr, reg)]\n","    print('lr',lr,',reg',reg,', train accuracy: ',train_accuracy*100,'%, val accuracy: ', val_accuracy ,'%')\n","    \n","print()\n","print('best validation accuracy achieved during cross-validation:',best_val*100,'%')\n","print('best Weights and bias are stored in `best_W_feats` and `best_b_feats`')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hdw_stSKQp2c"},"source":["**Expected Output**:\n","\n","if you are careful you should be able to get a classification accuracy of **near 44%** on the validation set\n"]},{"cell_type":"markdown","metadata":{"id":"3Ja2zR_yQp2d"},"source":["---\n","###d. Test the Trained Weights\n"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"8G7CcAx-Qp2e"},"source":["y_pred   = predict(X_test_feats, best_W_feats, best_b_feats)\n","accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n","\n","print('Testing Accuracy =', accuracy*100,'%')\n","print('Test label      =',y_test[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lM85_AmY9Dol"},"source":["**Expected Output**:\n","\n","You should reach at least  **42%** accuracy on test set\n"]},{"cell_type":"markdown","metadata":{"id":"Ai-VS3zpVSn0"},"source":["---\n","---\n","##9 - Train Multiclass SVM "]},{"cell_type":"markdown","metadata":{"id":"yTabZHwzVSn6"},"source":["---\n","###a. Train on Raw Pixel\n","* Find the best Learning Rate and Regularization Strength\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"d_9BHi5SbdN_"},"source":["##### <font color='red'>**EXERCISE**: </font>     \n","* experiment with different ranges for the learning rates and regularization strengths; "]},{"cell_type":"code","metadata":{"id":"-D2T1E2iVSn9"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","np.random.seed(None)\n","\n","results = {}\n","best_val = -1\n","best_W = None\n","best_b = None\n","learning_rates = [1e-7, 5e-7]\n","regularization_strengths = [9e4, 1e5]\n","\n","iterations = 2000\n","\n","# Greedily loop over learning_rates and regularization_strengths\n","for rate in learning_rates:\n","    for reg in regularization_strengths:\n","        print('Running {} iterations, rate = {}, reg = {}'.format(iterations, rate, reg))\n","        # call train function using the learning rate and regularization selected\n","        loss, W, b = train(X_train, y_train, \n","                           learning_rate=rate, reg=reg,\n","                           num_iters=iterations, verbose=False,\n","                           loss_fn=svm_loss)\n","        \n","        # call predict function using pretrained W and b on X_train and X_val to evaluate\n","        y_train_pred = predict(??, ??, ??)\n","        y_val_pred   = predict(??, ??, ??)\n","        \n","        # calculate the accuracy\n","        train_accuracy = np.mean(y_train == y_train_pred)\n","        val_accuracy = np.mean(y_val == y_val_pred)\n","        \n","        print ('rate = {}, reg = {}, test accuracy = {}, validation accuracy = {}'.format(\n","            rate, reg, train_accuracy, val_accuracy))\n","        \n","        # store the result accuracy combination\n","        results[(rate, reg)] = (train_accuracy, val_accuracy)\n","        \n","        # store the best Weight and Bias\n","        if val_accuracy > best_val:\n","            best_W = W\n","            best_b = b\n","            best_val = val_accuracy\n","\n","\n","    \n","# Print out results.\n","for lr, reg in sorted(results):\n","    train_accuracy, val_accuracy = results[(lr, reg)]\n","    print('lr',lr,',reg',reg,', train accuracy: ',train_accuracy*100,'%, val accuracy: ', val_accuracy ,'%')\n","    \n","print()\n","print('best validation accuracy achieved during cross-validation:',best_val*100,'%')\n","print('best Weights and bias are stored in `best_W` and `best_b`')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HACKUtEn9Do7"},"source":["**Expected Output**:\n","\n","You should reach `at least`  **`30%`** accuracy on cross-validation"]},{"cell_type":"markdown","metadata":{"id":"6mAVPbKjVSoJ"},"source":["---\n","###b. Test the Trained Weights"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"F2XoQZu8VSoK"},"source":["y_pred = predict(X_test, best_W, best_b)\n","accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n","\n","print('Testing Accuracy =', accuracy*100,'%')\n","print('Test label      =',y_test[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p3l3_S8HVSoT"},"source":["---\n","###c. Train on Features\n","* Find the best Learning Rate and Regularization Strength\n","* Train the Classifier with SVM Loss on top of the features extracted above; this should achieve better results than training SVMs directly on top of raw pixels.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hDGGhIdSbNC8"},"source":["#####<font color='red'>**EXERCISE**: </font>\n","* Experiment with different ranges for the learning rates and regularization strengths; \n","* You might also want to play with different numbers of bins in the color histogram."]},{"cell_type":"code","metadata":{"id":"IbIu30pkVSoU"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","np.random.seed(None)\n","\n","results = {}\n","best_val = -1\n","best_W_feats = None\n","best_b_feats = None\n","learning_rates = [1e-7, 5e-7]\n","regularization_strengths = [9e4, 1e5]\n","\n","iterations = 2000\n","\n","# Greedily loop over learning_rates and regularization_strengths\n","for rate in learning_rates:\n","    for reg in regularization_strengths:\n","        print('Running {} iterations, rate = {}, reg = {}'.format(iterations, rate, reg))\n","        # call train function using the learning rate and regularization selected\n","        loss, W, b = train(X_train_feats, y_train, \n","                           learning_rate=rate, reg=reg,\n","                           num_iters=iterations, verbose=False,\n","                           loss_fn=svm_loss)\n","        \n","        # call predict function using pretrained W and b on X_train_feats and X_val_feats to evaluate\n","        y_train_pred = predict(??, ??, ??)\n","        y_val_pred   = predict(??, ??, ??)\n","        \n","        # calculate the accuracy\n","        train_accuracy = np.mean(y_train == y_train_pred)\n","        val_accuracy = np.mean(y_val == y_val_pred)\n","        \n","        print ('rate = {}, reg = {}, test accuracy = {}, validation accuracy = {}'.format(\n","            rate, reg, train_accuracy, val_accuracy))\n","        \n","        # store the result accuracy combination\n","        results[(rate, reg)] = (train_accuracy, val_accuracy)\n","        \n","        # store the best Weight and Bias\n","        if val_accuracy > best_val:\n","            best_W_feats = W\n","            best_b_feats = b\n","            best_val = val_accuracy\n","\n","\n","    \n","# Print out results.\n","for lr, reg in sorted(results):\n","    train_accuracy, val_accuracy = results[(lr, reg)]\n","    print('lr',lr,',reg',reg,', train accuracy: ',train_accuracy*100,'%, val accuracy: ', val_accuracy ,'%')\n","    \n","print()\n","print('best validation accuracy achieved during cross-validation:',best_val*100,'%')\n","print('best Weights and bias are stored in `best_W_feats` and `best_b_feats`')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"awPyXqTTVSoZ"},"source":["**Expected Output**:\n","\n","if you are careful you should be able to get a classification accuracy of **near 42%** on the validation set\n"]},{"cell_type":"markdown","metadata":{"id":"fcC_E6bkVSoa"},"source":["---\n","###d. Test the Trained Weights\n"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"TpvWjK0cVSob"},"source":["y_pred = predict(X_test_feats, best_W_feats, best_b_feats)\n","accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n","\n","print('Testing Accuracy =', accuracy*100,'%')\n","print('Test label      =',y_test[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ioubboYqVSog"},"source":["**Expected Output**:\n","\n","You should reach at least  **40%** accuracy on test set\n"]},{"cell_type":"markdown","metadata":{"id":"quMA17JKDnpI"},"source":["---\n","\n","# Congratulation, You've Completed Exercise 2\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2020 - ADF</a> </p>"]},{"cell_type":"markdown","metadata":{"id":"7_Hkr9kg9Do-"},"source":["![footer](https://i.ibb.co/yX0jfMS/footer2020.png)"]}]}