{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "CV2020 - 12.2 - Network Visualization (TensorFlow).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_ChrQTZyAZU"
      },
      "source": [
        "![title](https://i.ibb.co/f2W87Fg/logo2020.png)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQjL2XtX7uKJ"
      },
      "source": [
        "<table  class=\"tfo-notebook-buttons\" align=\"left\"><tr><td>\n",
        "    \n",
        "<a href=\"https://colab.research.google.com/github/adf-telkomuniv/CV2020_Exercises/blob/main/CV2020 - 12.2 - Network Visualization (TensorFlow).ipynb\" source=\"blank\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n",
        "</td><td>\n",
        "<a href=\"https://github.com/adf-telkomuniv/CV2020_Exercises/blob/main/CV2020 - 12.2 - Network Visualization (TensorFlow).ipynb\" source=\"blank\" ><img src=\"https://i.ibb.co/6NxqGSF/pinpng-com-github-logo-png-small.png\"></a>\n",
        "    \n",
        "</td></tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnrUNY5Er7UE"
      },
      "source": [
        "# Task 12 part 2 - Network Visualization (TensorFlow)\n",
        "\n",
        "A convolutional neural network typically has multiple convolutional layers (hence, the name).\n",
        "\n",
        "Conceptually, we understand each convolutional layer extracts spatial features from their inputs.  Earlier layer detects low-level features like color, texture, lines, curves, etc.  Later layers detect higher abstraction like eyes, tail, etc.\n",
        "\n",
        "\n",
        "In this notebook, we use a pre-trained convolutional neural network to see what kind of input images strongly activate filters in convolutional layers.\n",
        "\n",
        "This notebook code is largely based on the blog article **How convolutional neural networks see the world** by **Francois Chollet** [2]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF71bN55cZzi"
      },
      "source": [
        "Write down your Name and Student ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5yg44U8cZzk"
      },
      "source": [
        "## --- start your code here ----\n",
        "\n",
        "NIM  = ??\n",
        "Nama = ??\n",
        "\n",
        "## --- end your code here ----"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VAE_sE-Bgk1"
      },
      "source": [
        "---\n",
        "---\n",
        "#[Part 0] Import Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z389HkeEvQ6J"
      },
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grVDHPIghjT9"
      },
      "source": [
        "---\n",
        "---\n",
        "# [Part 1] Load Pretrained Model\n",
        "\n",
        "We could use any other convolutional neural network, but here we use the pre-trained VGG16 available in Keras.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ph4CMkhhjUI"
      },
      "source": [
        "---\n",
        "## 1 - Load VGG16 Model\n",
        "The following shows the available convolutional layers in VGG16."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b44bj6kahjUA"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "vgg_model = VGG16(weights='imagenet', include_top=False)\n",
        "vgg_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMt-NcuehjUX"
      },
      "source": [
        "---\n",
        "## 2 - Helper Function\n",
        "\n",
        "\n",
        "Typically, the input to the VGG16 model is an image to classify such as cats and dogs. \n",
        "\n",
        "Here, however, we use a randomly generated noise image and feed it to VGG16 model to calculate filter activations and their gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQAMWpgChjUZ"
      },
      "source": [
        "---\n",
        "### a. Random Image Generator\n",
        "\n",
        "We start the visualization from a random image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGIUI-A8hjUa"
      },
      "source": [
        "def make_random_image(img_height=128, img_width=128, shift=True):\n",
        "    img = tf.random.uniform((1, img_width, img_height, 3))\n",
        "    if shift:\n",
        "        return (img - 0.5) * 0.25\n",
        "    return img * 0.25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2ZMgLPNhjUp"
      },
      "source": [
        "---\n",
        "### b. Convert to Image\n",
        "\n",
        "Function to convert result data into 0-255 image data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7QbeskbhjUq"
      },
      "source": [
        "def convert_to_image(x):\n",
        "    # normalize data\n",
        "    # set the std to 0.1 and the mean to 0.5\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.15\n",
        "    \n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "    \n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShCCJ079L2f2"
      },
      "source": [
        "let's try to visualize a random image patch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKOKaNrChjUf"
      },
      "source": [
        "input_img = make_random_image()\n",
        "\n",
        "img = convert_to_image(input_img[0].numpy())\n",
        "plt.imshow(img)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um0vdsxQhjUv"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# [Part 2] Filter Visualization\n",
        "\n",
        "Filter Visualization is basically performing a Gradient Ascent into an input image to maximize the filter activation. Gradient ascent function simply computes the gradients of the loss above with regard to the input image, and update the update image so as to move it towards a state that will activate the target filter more strongly.\n",
        "\n",
        "In TensorFlow 2, we use GradientTape to perform backward pass. GradientTape is an API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs. TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using reverse mode differentiation.\n",
        "\n",
        "And with Eager execution, it is faster to compute the gradient by constructing a callable function that executes a TensorFlow graph by defining a `@tf.function`.\n",
        "\n",
        "So let's do that\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPHODRBFxbB6"
      },
      "source": [
        "---\n",
        "## 1 - Gradient Ascent Function\n",
        "\n",
        "Here we define a `@tf.function` to perform Gradient Ascent\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc2RyhhXUYMU"
      },
      "source": [
        "---\n",
        "#### <font color='red'>**EXERCISE:** </font>\n",
        "\n",
        "Implement this function. \n",
        "* Perform gradient ascent to generate filter visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdqvKGvi6zAt"
      },
      "source": [
        "@tf.function\n",
        "def gradient_ascent_filter(img, filter_index, step_size, model):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(img)\n",
        "\n",
        "        # get activation from model\n",
        "        # call model() with input img\n",
        "        activation = ??\n",
        "\n",
        "        # We avoid border artifacts by only involving non-border pixels in the loss.\n",
        "        filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
        "\n",
        "        # calculate the loss from activation\n",
        "        # call tf.reduce_mean() function with input filter_activation\n",
        "        loss = ??\n",
        "\n",
        "    # Compute gradients.\n",
        "    # call tape.gradient() with input loss and img\n",
        "    grads = ??\n",
        "\n",
        "    # Normalize gradients.\n",
        "    grads = tf.math.l2_normalize(grads)\n",
        "    img += step_size * grads\n",
        "\n",
        "    return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYcSqa0bN_pq"
      },
      "source": [
        "---\n",
        "## 2 - Visualization Function\n",
        "Now define a function to perform the filter visualization loop end-to-end. \n",
        "We start from a random image that is close to \"all gray\" (i.e. visually netural), then repeatedly apply the gradient ascent step function defined above. \n",
        "\n",
        "Lastly, we convert the resulting input image back to a displayable form, by normalizing it and restricting it to the [0, 255] range.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_TlZuuHN_pt"
      },
      "source": [
        "---\n",
        "#### <font color='red'>**EXERCISE:** </font>\n",
        "\n",
        "Implement this function. \n",
        "* Perform gradient ascent to generate filter visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "-QLVUxfchjUw"
      },
      "source": [
        "def visualize_filter(model, input_img, layer_name, filter_index, steps=50, step_size=10.0 ):\n",
        "\n",
        "    img = input_img\n",
        "\n",
        "    # get layer from model based on layer_name \n",
        "    # by calling model.get_layer() function with input layer_name\n",
        "    layer = ??\n",
        "\n",
        "    # Set up a model that returns the activation values for our target layer\n",
        "    feature_extractor = Model(inputs=model.inputs, outputs=layer.output)\n",
        "\n",
        "    # perform gradient ascent\n",
        "    for iteration in range(steps):\n",
        "        # call gradient_ascent_filter() function with input img, filter_index, \n",
        "        # step_size, and feature_extractor\n",
        "        img = ??\n",
        "\n",
        "    # Decode the resulting input image\n",
        "    img = convert_to_image(img[0].numpy())\n",
        "    \n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhG9f4t7xv4V"
      },
      "source": [
        "---\n",
        "## 3 - Visualize a Filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebm_OtCHhjU1"
      },
      "source": [
        "The following is an example of an input image that strongly activates the first filter in the layer `block4_conv1`.\n",
        "\n",
        "You can change the `layer_name` and `filter_index`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9Ux-m9OhjU2"
      },
      "source": [
        "layer_name   = 'block4_conv1'         # change this to see filter from another layer\n",
        "filter_index = 0\n",
        "input_img    = make_random_image()\n",
        "\n",
        "result = visualize_filter(vgg_model, input_img, layer_name, filter_index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5q-CQsDyAOV"
      },
      "source": [
        "Visualize the filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rtcTIPNyC2V"
      },
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "plt.imshow(result)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40YOWy4MhjU6"
      },
      "source": [
        "---\n",
        "## 4 - Visualize 20 Filters\n",
        "Let's examine some filters in a layer. In the function below, we're going to select 20 filters randomly, and view it's visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7zoKDAhhjU7"
      },
      "source": [
        "def visualize_20_filters(model, layer_name, steps=80, step_size=10.0, shift=True):\n",
        "  \n",
        "    input_img = make_random_image(shift=shift)\n",
        "    print('Showing 20 random filters for layer', layer_name)\n",
        "    plt.figure(figsize=(16,16))\n",
        "    num_filter = model.get_layer(layer_name).output.shape[3]\n",
        "\n",
        "    for i in range(20):\n",
        "        idx = np.random.randint(0,num_filter)\n",
        "        result = visualize_filter(model, input_img, layer_name, filter_index=idx, steps=steps, step_size=step_size)    \n",
        "        plt.subplot(4, 5, i+1)\n",
        "        plt.imshow(result)\n",
        "        plt.gca().set_title(idx)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjlwFWru02OJ"
      },
      "source": [
        "Visualize the filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05gT7jB1hjU-"
      },
      "source": [
        "layer_name   = 'block3_conv3'        # change this to see filter from another layer\n",
        "\n",
        "visualize_20_filters(vgg_model, layer_name, steps=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPouEDLz5cRF"
      },
      "source": [
        "---\n",
        "---\n",
        "# [Part 3] VGG Face Visualization\n",
        "\n",
        "Now for comparison, Here you are provided a vgg model that has been trained on face dataset.\n",
        "\n",
        "The model was trained to perform various task from face detection to face recognition\n",
        "\n",
        "So it made sense that the filters will form facial attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMmGdflshjUR"
      },
      "source": [
        "---\n",
        "## 1 - Load VGG Face\n",
        "\n",
        "Download and load the model provided\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRAsF8EP581G"
      },
      "source": [
        "!wget -O 'vgg_face_notop.h5' 'https://github.com/CNN-ADF/Task2019/raw/master/resources/vgg_face_notop.h5' -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zht-4eDhjUT"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "face_model = load_model('vgg_face_notop.h5')\n",
        "\n",
        "face_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBScmVmV9tfi"
      },
      "source": [
        "---\n",
        "## 2 - Visualize 20 Filters\n",
        "Let's examine some filters in a layer. In the function below, we're going to select 20 filters randomly, and view it's visualization\n",
        "\n",
        "WARNING,<br>\n",
        "you might see some disturbing features \t(￣ヘ￣)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7_kXM9L9wvW"
      },
      "source": [
        "layer_name   = 'block5_conv2'        # change this to see filter from another layer\n",
        "\n",
        "visualize_20_filters(face_model, layer_name, steps=100, step_size=10.0, shift=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXPoylwyhjVD"
      },
      "source": [
        "---\n",
        "---\n",
        "# [Part 4] Little Deep Dreams\n",
        "\n",
        "If we throw a Sky image into this experiment and let a filter to nudge the image, what do we get?\n",
        "\n",
        "This is a similar idea used in **Inceptionism: Going Deeper into Neural Networks** [3]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR_RBkXr1q2h"
      },
      "source": [
        "---\n",
        "## 1 - Load Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffCMlzDmkdby"
      },
      "source": [
        "!wget -O 'sky1024px.jpg' 'https://images.squarespace-cdn.com/content/57c8515459cc68fba78d9781/1494175003092-5PQRBFQHQ31HXEWW9O07/sky1024px.jpg' -q\n",
        "\n",
        "sky_img = plt.imread('sky1024px.jpg') \n",
        "plt.imshow(sky_img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GotX4AW21tzh"
      },
      "source": [
        "---\n",
        "## 2 - Hallucinate the Image\n",
        "\n",
        "Now we perform the filter visualization to the image.\n",
        "\n",
        "It will, in sense, hallucinate the appearance of the selected filters inside the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKnmGhGNhjVE"
      },
      "source": [
        "layer_name   = 'block5_conv2'      # change this to hallucinate image using filter from another layer\n",
        "filter_index = 15                  # change this to hallucinate image using another filter index\n",
        "img = np.expand_dims(sky_img/127,0).astype('float')\n",
        "\n",
        "result = visualize_filter(vgg_model, \n",
        "                         layer_name=layer_name,\n",
        "                         filter_index=filter_index, \n",
        "                         input_img=img, \n",
        "                         steps=100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIZ5T02L1nKb"
      },
      "source": [
        "Visualize the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl04BWvH1lCB"
      },
      "source": [
        "plt.figure(figsize=(18,8))\n",
        "plt.subplot(121)\n",
        "plt.imshow(sky_img)\n",
        "plt.axis('off')\n",
        "plt.subplot(122)\n",
        "plt.imshow(result)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ1YoiXFPYuq"
      },
      "source": [
        "And this is the process that was called Deep Dream"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eWsPrN1_A9j"
      },
      "source": [
        "---\n",
        "## 3 - Hallucinate using VGG Face\n",
        "\n",
        "Now let's try to hallucinate the image using VGG Face Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1Ru4feM_A9q"
      },
      "source": [
        "layer_name   = 'block5_conv2'      # change this to hallucinate image using filter from another layer\n",
        "filter_index = 153                 # change this to hallucinate image using another filter index\n",
        "\n",
        "result = visualize_filter(face_model, \n",
        "                         layer_name=layer_name,\n",
        "                         filter_index=filter_index, \n",
        "                         input_img=img, \n",
        "                         steps=100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcLLGUAA_A90"
      },
      "source": [
        "Visualize the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjcEZVvS_A92"
      },
      "source": [
        "plt.figure(figsize=(18,8))\n",
        "plt.subplot(121)\n",
        "plt.imshow(sky_img)\n",
        "plt.axis('off')\n",
        "plt.subplot(122)\n",
        "plt.imshow(result)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvfrJTGDPdm-"
      },
      "source": [
        "You can see that now there're face feature (the one that you selected) everywhere on the image.\n",
        "\n",
        "Now let's try to build a better Deep Dream function that produce better image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WUaDGqlhjVH"
      },
      "source": [
        "---\n",
        "---\n",
        "# [Part 4] Deep Dreams using Inception V3\n",
        "\n",
        "\"Deep dream\" is an image-filtering technique which consists of taking an image classification model, and running gradient ascent over an input image to try to maximize the activations of specific layers (and sometimes, specific units in specific layers) for this input. It produces hallucination-like visuals.\n",
        "\n",
        "It was first introduced by Alexander Mordvintsev from Google in July 2015.\n",
        "\n",
        "To build a better Deep Dream Image, let's use `InceptionV3` model for this exercise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9K2DE0Y_o3Y"
      },
      "source": [
        "---\n",
        "## 1 - Load Inception Model\n",
        "\n",
        "Here we load InceptionV3 model that was trained on Imagenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R9nciFJhjVI"
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, save_img, img_to_array\n",
        "\n",
        "inception_model = InceptionV3(weights='imagenet',include_top=False)\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n0blthUhjVL"
      },
      "source": [
        "---\n",
        "## 2 - Helper Function\n",
        "Notice that using previous helper function, the image result became darker\n",
        "\n",
        "To clear the image, we prepare another helper function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3Z7l3ojhjVM"
      },
      "source": [
        "def preprocess_image(image_path):\n",
        "    # Util function to open, resize and format pictures into appropriate tensors.\n",
        "    img = load_img(image_path)\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def deprocess_image(x):\n",
        "    # reshape array into a valid image\n",
        "    x = x.reshape((x.shape[1], x.shape[2], 3))\n",
        "\n",
        "    # Undo inception v3 preprocessing\n",
        "    x /= 2.\n",
        "    x += 0.5\n",
        "    x *= 255.\n",
        "\n",
        "    # Convert to uint8 and clip to the valid range [0, 255]\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmV58zrxQFEo"
      },
      "source": [
        "---\n",
        "## 3 - Gradient Ascent Function\n",
        "\n",
        "Similar to before, here we define a `@tf.function` to perform Gradient Ascent. However, slightly different from before, in Gradient Ascent for Deep Dream, we calculate the loss from activations of several layers that we choose, scaled by its coefficient that we also defined.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQVfTr45QFEw"
      },
      "source": [
        "---\n",
        "#### <font color='red'>**EXERCISE:** </font>\n",
        "\n",
        "Complete the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctrs65Wc19Tk"
      },
      "source": [
        "@tf.function\n",
        "def gradient_ascent_deep_dream(img, step_size, model):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(img)\n",
        "\n",
        "        # get features from model\n",
        "        # call model() with input img\n",
        "        features = ??\n",
        "\n",
        "        # Initialize the loss\n",
        "        loss = tf.zeros(shape=())\n",
        "\n",
        "        # extract activations from the selected features\n",
        "        for name in features.keys():\n",
        "            # get coefficient for features in each layer\n",
        "            coeff = dream_filters[name]\n",
        "            \n",
        "            # get the activation\n",
        "            activation = features[name]\n",
        "            \n",
        "            # We avoid border artifacts by only involving non-border pixels in the loss.\n",
        "            scaling = tf.reduce_prod(tf.cast(tf.shape(activation), \"float32\"))\n",
        "\n",
        "            # define loss function\n",
        "            loss += coeff * tf.reduce_sum(tf.square(activation[:, 2:-2, 2:-2, :])) / scaling\n",
        "\n",
        "    # Compute gradients.\n",
        "    # call tape.gradient() with input loss and img\n",
        "    grads = ??\n",
        "\n",
        "    # Normalize gradients.\n",
        "    grads /= tf.maximum(tf.reduce_mean(tf.abs(grads)), 1e-6)\n",
        "    img += step_size * grads\n",
        "\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebN2peXLhjVU"
      },
      "source": [
        "---\n",
        "## 3 - Deep Dream Function\n",
        "\n",
        "The complete Deep Dream Function perform the filter hallucination in several image scales to generate better and more \"smooth\" image. \n",
        "\n",
        "The process is by defining the number of processing scales (\"octaves\"), then start hallucinating image from the smallest scale. \n",
        "\n",
        "For every scale, we resize the image to that scale, run gradient ascent, upscale image to the next scale, and reinject the detail that was lost at upscaling time.\n",
        "\n",
        "We stop the process when we are back to the original size. To obtain the detail lost during upscaling, we simply take the original image, shrink it down, upscale it, and compare the result to the (resized) original image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmFRj6xwAJH3"
      },
      "source": [
        "---\n",
        "#### <font color='red'>**EXERCISE:** </font>\n",
        "\n",
        "Complete this function. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-ZUKNo60c-b"
      },
      "source": [
        "def deep_dream(model, input_img_path, dream_filters, num_octave, octave_scale, steps=20, step_size=0.01):\n",
        "\n",
        "    # Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "    outputs_dict = dict([\n",
        "        (layer.name, layer.output)\n",
        "        for layer in [model.get_layer(name) for name in dream_filters.keys()]\n",
        "    ])\n",
        "\n",
        "    # Set up a model that returns the activation values for every target layer\n",
        "    feature_extractor = Model(inputs=model.inputs, outputs=outputs_dict)\n",
        "\n",
        "    # prepare original image\n",
        "    original_img      = preprocess_image(input_img_path)\n",
        "    original_shape    = original_img.shape[1:3]\n",
        "    successive_shapes = [original_shape]\n",
        "\n",
        "    for i in range(1, num_octave):\n",
        "        shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n",
        "        successive_shapes.append(shape)\n",
        "    successive_shapes = successive_shapes[::-1]\n",
        "    shrunk_original_img = tf.image.resize(original_img, successive_shapes[0])\n",
        "\n",
        "    # Make a copy of original image\n",
        "    img = tf.identity(original_img)  \n",
        "\n",
        "    # perform Deep Dream in multiple image scale\n",
        "    for shape in successive_shapes:  \n",
        "        print('Processing image shape', shape)\n",
        "\n",
        "        # resize image by calling tf.image.resize() function \n",
        "        # with input img and shape\n",
        "        img = ??\n",
        "\n",
        "        # perform gradient ascent\n",
        "        for i in range(steps):\n",
        "            # call gradient_ascent_deep_dream() function  \n",
        "            # with input img, step_size, and feature_extractor\n",
        "            img = ??\n",
        "\n",
        "        # resize back image\n",
        "        upscaled_shrunk_original_img = tf.image.resize(shrunk_original_img, shape)\n",
        "        same_size_original           = tf.image.resize(original_img, shape)\n",
        "        lost_detail                  = same_size_original - upscaled_shrunk_original_img\n",
        "\n",
        "        # hallucinate the image by adding the lost_detail to img\n",
        "        img = ??\n",
        "        \n",
        "        # resize original image by calling tf.image.resize() function \n",
        "        # with input original_img and shape\n",
        "        shrunk_original_img = ??\n",
        "\n",
        "    # convert the result into image by calling deprocess_image() function with input img.numpy()\n",
        "    image_result = ??\n",
        "    \n",
        "    return image_result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnQ0OtLNhjVQ"
      },
      "source": [
        "---\n",
        "## 4 - Feature List\n",
        "\n",
        "Now, define list of filter that will be used to hallucinate the image\n",
        "\n",
        "You can change or add the feature and coefficient to use. \n",
        "\n",
        "See the list of layer name in `summary()` method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U33jRFeChjVR"
      },
      "source": [
        "dream_filters = {\n",
        "    # 'mixed2' : 0.2,\n",
        "    'mixed4' : 1.1,\n",
        "    'mixed5' : 1.4,\n",
        "    'mixed7' : 2.5,\n",
        "    'mixed8' : 1.0,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1TqYLv0Bjo4"
      },
      "source": [
        "---\n",
        "## 5 - Perform Deep Dream"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUiTrb392GJK"
      },
      "source": [
        "# prepare setting for scaling process\n",
        "num_octave   = 5         # Number of scales at which to run gradient ascent\n",
        "octave_scale = 1.5       # Size ratio between scales\n",
        "\n",
        "result = deep_dream(inception_model, 'sky1024px.jpg', dream_filters, num_octave, octave_scale, steps=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLpN40F8Lps4"
      },
      "source": [
        "Visualize the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyWVqWr52Iao"
      },
      "source": [
        "plt.figure(figsize=(18,8))\n",
        "plt.subplot(121)\n",
        "plt.imshow(sky_img)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.imshow(result)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgImbfG8Wn3y"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# Congratulation, You've Completed Exercise 12 part 2\n",
        "\n",
        "<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2020 - ADF</a> </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Hkr9kg9Do-"
      },
      "source": [
        "![footer](https://i.ibb.co/yX0jfMS/footer2020.png)"
      ]
    }
  ]
}